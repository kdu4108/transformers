{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: XLA_PYTHON_CLIENT_PREALLOCATE=false\n",
      "env: JAX_PLATFORM_NAME=cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/miniconda3/envs/jax-hf/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc832be1750>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%env XLA_PYTHON_CLIENT_PREALLOCATE=false\n",
    "%env JAX_PLATFORM_NAME=cpu\n",
    "\n",
    "from jax.config import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "from dataclasses import is_dataclass\n",
    "from jax import numpy as jnp\n",
    "import transformers\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForMaskedLM, \n",
    "    BertTokenizer, \n",
    "    BertTokenizerFast, \n",
    "    BertEmbeddings,\n",
    "    BfBertEmbeddings,\n",
    "    BertConfig,\n",
    "    BertSelfAttention,\n",
    "    BfBertSelfAttention,\n",
    "    BertSelfOutput,\n",
    "    BfBertSelfOutput,\n",
    "    BertAttention,\n",
    "    BfBertAttention,\n",
    "    BertLayer,\n",
    "    BfBertLayer,\n",
    "    BertEncoder,\n",
    "    BfBertEncoder,\n",
    "    BaseModelOutputWithPastAndCrossAttentions,\n",
    "    BfBaseModelOutputWithPastAndCrossAttentions,\n",
    "    BertForMaskedLM,\n",
    "    BfBertForMaskedLM,\n",
    "    BertForSequenceClassification,\n",
    "    BfBertForSequenceClassification,\n",
    ")\n",
    "from brunoflow.ad.utils import check_node_equals_tensor, check_node_allclose_tensor\n",
    "from utils import check_bf_param_weights_match_torch, check_equivalent_class, check_dataclass_keys_match, check_model_outputs_allclose, check_bf_model_outputs_match_torch_outputs, check_bf_param_grads_allclose_torch\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9378"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = BertConfig.from_pretrained(pretrained_model_name_or_path=\"../../brunoflow/models/bert/config-toy.json\")\n",
    "m = BertForSequenceClassification(config=config)\n",
    "sum(p.numel() for p in m.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: TODO(KD) - when this is used, write some tests for this!\n"
     ]
    }
   ],
   "source": [
    "m = BfBertForSequenceClassification(config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bert_uncased_L-2_H-128_A-2 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: TODO(KD) - when this is used, write some tests for this!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4386178"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Init torch and bf models\n",
    "BF_FROM_MODEL_ID = False ### NOTE: BECAUSE THIS IS SUPER HACKY THIS SOMEWHAT DOES NOT WORK WHEN SET TO TRUE. FROM_PRETRAINED FOR BRUNOFLOW IS PROBABLY SOMEWHAT BROKEN, BUT AT LEAST THIS IS A WORKAROUND. Also it looks like the errors are only bounded by 0.01 :/.\n",
    "TORCH_FROM_MODEL_ID = True\n",
    "# model_id = \"bert-base-uncased\"\n",
    "model_id = \"google/bert_uncased_L-2_H-128_A-2\"\n",
    "config = BertConfig.from_pretrained(pretrained_model_name_or_path=\"../../brunoflow/models/bert/config-tiny.json\")\n",
    "\n",
    "if TORCH_FROM_MODEL_ID:\n",
    "    torch_model = BertForSequenceClassification.from_pretrained(model_id)\n",
    "else:\n",
    "    torch_model = BertForSequenceClassification(config)\n",
    "if BF_FROM_MODEL_ID:\n",
    "    bf_model = BfBertForSequenceClassification.from_pretrained(model_id)\n",
    "else:\n",
    "    bf_model = BfBertForSequenceClassification(config)\n",
    "\n",
    "sum(p.numel() for p in torch_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish data\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_id)\n",
    "text = [\"hello I want to eat some [MASK] meat today. It's thanksgiving [MASK] all!\", \"yo yo what's up\"]\n",
    "tokens = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# Create torch and bf inputs to model\n",
    "input_ids_torch = tokens[\"input_ids\"]\n",
    "labels_torch = torch.tensor([0, 1], dtype=torch.long)\n",
    "input_ids_bf = jnp.array(input_ids_torch.numpy())\n",
    "labels_bf = jnp.array(labels_torch.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.modeling_outputs.SequenceClassifierOutput'>\n",
      "CPU times: user 189 ms, sys: 109 Âµs, total: 189 ms\n",
      "Wall time: 36.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "outputs_torch = torch_model(input_ids_torch)\n",
    "print(type(outputs_torch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.modeling_bf_outputs.BfSequenceClassifierOutput'>\n",
      "CPU times: user 1.12 s, sys: 34.1 ms, total: 1.16 s\n",
      "Wall time: 1.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "outputs_bf = bf_model(input_ids_bf)\n",
    "print(type(outputs_bf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that forward pass for bf works and matches output shape with torch\n",
    "if isinstance(outputs_bf, (list, tuple)):\n",
    "    # Handle case where outputs is a tuple/list and not just a single item\n",
    "    assert len(outputs_bf) == len(outputs_torch)\n",
    "    for i in range(len(outputs_bf)):\n",
    "        out_bf, out_torch = outputs_bf[i], outputs_torch[i] \n",
    "        assert(out_torch.shape == out_bf.shape)\n",
    "elif is_dataclass(outputs_bf):\n",
    "    check_equivalent_class(outputs_bf, outputs_torch)\n",
    "    check_dataclass_keys_match(outputs_bf, outputs_torch)\n",
    "else:\n",
    "    assert(outputs_torch.shape == outputs_bf.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save torch BertForMLM to file\n",
    "save_path = \"bertforseqclass_torch.pt\"\n",
    "torch.save(torch_model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load state dict for BertForMLM into BF and check weights, outputs, and backprop\n",
    "if not BF_FROM_MODEL_ID:\n",
    "    bf_model.load_state_dict(torch.load(save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check weights of BF model and Torch model match exactly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of param weight bert.embeddings.word_embeddings.weight for bf and torch are equal? True\n",
      "Value of param weight bert.embeddings.position_embeddings.weight for bf and torch are equal? True\n",
      "Value of param weight bert.embeddings.token_type_embeddings.weight for bf and torch are equal? True\n",
      "Value of param weight bert.embeddings.LayerNorm.weight for bf and torch are equal? True\n",
      "Value of param weight bert.embeddings.LayerNorm.bias for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.0.attention.self.query.weight for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.0.attention.self.query.bias for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.0.attention.self.key.weight for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.0.attention.self.key.bias for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.0.attention.self.value.weight for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.0.attention.self.value.bias for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.0.attention.output.dense.weight for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.0.attention.output.dense.bias for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.0.attention.output.LayerNorm.weight for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.0.attention.output.LayerNorm.bias for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.0.intermediate.dense.weight for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.0.intermediate.dense.bias for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.0.output.dense.weight for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.0.output.dense.bias for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.0.output.LayerNorm.weight for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.0.output.LayerNorm.bias for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.1.attention.self.query.weight for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.1.attention.self.query.bias for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.1.attention.self.key.weight for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.1.attention.self.key.bias for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.1.attention.self.value.weight for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.1.attention.self.value.bias for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.1.attention.output.dense.weight for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.1.attention.output.dense.bias for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.1.attention.output.LayerNorm.weight for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.1.attention.output.LayerNorm.bias for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.1.intermediate.dense.weight for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.1.intermediate.dense.bias for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.1.output.dense.weight for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.1.output.dense.bias for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.1.output.LayerNorm.weight for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.1.output.LayerNorm.bias for bf and torch are equal? True\n",
      "Value of param weight bert.pooler.dense.weight for bf and torch are equal? True\n",
      "Value of param weight bert.pooler.dense.bias for bf and torch are equal? True\n",
      "Value of param weight classifier.weight for bf and torch are equal? True\n",
      "Value of param weight classifier.bias for bf and torch are equal? True\n"
     ]
    }
   ],
   "source": [
    "# Check weights match\n",
    "check_bf_param_weights_match_torch(bf_model, torch_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check model output after forward pass matches for BF and Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.dropout 0.1\n",
      "bert.embeddings.dropout 0\n",
      "bert.encoder.layer.0.attention.self.dropout 0.1\n",
      "bert.encoder.layer.0.attention.self.dropout 0\n",
      "bert.encoder.layer.0.attention.output.dropout 0.1\n",
      "bert.encoder.layer.0.attention.output.dropout 0\n",
      "bert.encoder.layer.0.output.dropout 0.1\n",
      "bert.encoder.layer.0.output.dropout 0\n",
      "bert.encoder.layer.1.attention.self.dropout 0.1\n",
      "bert.encoder.layer.1.attention.self.dropout 0\n",
      "bert.encoder.layer.1.attention.output.dropout 0.1\n",
      "bert.encoder.layer.1.attention.output.dropout 0\n",
      "bert.encoder.layer.1.output.dropout 0.1\n",
      "bert.encoder.layer.1.output.dropout 0\n",
      "dropout 0.1\n",
      "dropout 0\n"
     ]
    }
   ],
   "source": [
    "# Set all dropouts to 0\n",
    "for name, module in torch_model.named_modules():\n",
    "    if module._get_name() == \"Dropout\":\n",
    "        print(name, module.p)\n",
    "        module.p = 0\n",
    "        print(name, module.p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking diff between BF and torch for logits:\n",
      "Output of bf and torch are within 0.01? True\n",
      "\tStats on diff in outputs between bf and torch:                   0\n",
      "count  4.000000e+00\n",
      "mean   5.000582e-08\n",
      "std    1.432968e-08\n",
      "min    2.938199e-08\n",
      "25%    4.707900e-08\n",
      "50%    5.410022e-08\n",
      "75%    5.702704e-08\n",
      "max    6.244085e-08\n"
     ]
    }
   ],
   "source": [
    "# Check output from forward passes match for bf and torch\n",
    "torch_model.train(False)\n",
    "bf_model.train(False)\n",
    "\n",
    "outputs_bf = bf_model(input_ids_bf)\n",
    "outputs_torch = torch_model(input_ids_torch)\n",
    "\n",
    "if isinstance(outputs_bf, (list, tuple)):\n",
    "    assert len(outputs_bf) == len(outputs_torch)\n",
    "    for i in range(len(outputs_bf)):\n",
    "        out_bf, out_torch = outputs_bf[i], outputs_torch[i]\n",
    "        check_bf_model_outputs_match_torch_outputs(out_bf, out_torch, atol=1e-6)\n",
    "elif is_dataclass(outputs_bf):\n",
    "    check_model_outputs_allclose(outputs_bf, outputs_torch, print_stats=True, atol=1e-2)\n",
    "else:\n",
    "    check_bf_model_outputs_match_torch_outputs(outputs_bf, outputs_torch, atol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check grad after backward pass matches for BF and torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.2 ms, sys: 981 Âµs, total: 21.2 ms\n",
      "Wall time: 9.69 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Torch backward pass\n",
    "torch_model.train(True)\n",
    "\n",
    "if isinstance(outputs_torch, (list, tuple)):\n",
    "    assert len(outputs_bf) == len(outputs_torch)\n",
    "    backprop_node_torch = outputs_torch[0]\n",
    "elif is_dataclass(outputs_torch):\n",
    "    backprop_node_torch = outputs_torch.logits\n",
    "else:\n",
    "    backprop_node_torch = outputs_torch\n",
    "    \n",
    "backprop_node_torch.backward(gradient=torch.ones_like(backprop_node_torch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/miniconda3/envs/jax-hf/lib/python3.9/site-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int64. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\"scatter inputs have incompatible types: cannot safely cast \"\n",
      "/home/kevin/miniconda3/envs/jax-hf/lib/python3.9/site-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int64. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\"scatter inputs have incompatible types: cannot safely cast \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.3 s, sys: 5.16 s, total: 15.4 s\n",
      "Wall time: 7.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# BF backward pass\n",
    "\n",
    "if isinstance(outputs_bf, (list, tuple)):\n",
    "    assert len(outputs_bf) == len(outputs_torch)\n",
    "    backprop_node = outputs_bf[0]\n",
    "elif is_dataclass(outputs_torch):\n",
    "    backprop_node = outputs_bf.logits\n",
    "else:\n",
    "    backprop_node = outputs_bf\n",
    "    \n",
    "backprop_node.backprop(values_to_compute=(\"grad\",))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad of param bert.embeddings.word_embeddings.weight for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.embeddings.position_embeddings.weight for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.embeddings.token_type_embeddings.weight for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.embeddings.LayerNorm.weight for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.embeddings.LayerNorm.bias for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.0.attention.self.query.weight for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.0.attention.self.query.bias for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.0.attention.self.key.weight for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.0.attention.self.key.bias for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.0.attention.self.value.weight for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.0.attention.self.value.bias for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.0.attention.output.dense.weight for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.0.attention.output.dense.bias for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.0.attention.output.LayerNorm.weight for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.0.attention.output.LayerNorm.bias for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.0.intermediate.dense.weight for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.0.intermediate.dense.bias for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.0.output.dense.weight for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.0.output.dense.bias for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.0.output.LayerNorm.weight for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.0.output.LayerNorm.bias for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.1.attention.self.query.weight for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.1.attention.self.query.bias for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.1.attention.self.key.weight for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.1.attention.self.key.bias for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.1.attention.self.value.weight for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.1.attention.self.value.bias for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.1.attention.output.dense.weight for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.1.attention.output.dense.bias for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.1.attention.output.LayerNorm.weight for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.1.attention.output.LayerNorm.bias for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.1.intermediate.dense.weight for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.1.intermediate.dense.bias for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.1.output.dense.weight for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.1.output.dense.bias for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.1.output.LayerNorm.weight for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.1.output.LayerNorm.bias for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.pooler.dense.weight for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.pooler.dense.bias for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param classifier.weight for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param classifier.bias for bf and torch are within rtol=0.06, atol=0.01? True\n"
     ]
    }
   ],
   "source": [
    "# Run the actual check\n",
    "check_bf_param_grads_allclose_torch(bf_model, torch_model, rtol=6e-2, atol=1e-2, print_output=True, print_stats=True, use_assert=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "b4cc9cc217af6b7e12b7da5c82d5884fde07a0e0f6b7f76767c2fbf53f076f9a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
