{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: XLA_PYTHON_CLIENT_PREALLOCATE=false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/miniconda3/envs/jax-hf/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe5ae9d94b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%env XLA_PYTHON_CLIENT_PREALLOCATE=false\n",
    "import brunoflow as bf\n",
    "from brunoflow.ad.utils import check_node_equals_tensor, check_node_allclose_tensor\n",
    "from jax import numpy as jnp\n",
    "import numpy as np\n",
    "import transformers\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForMaskedLM, \n",
    "    BertForMaskedLM, \n",
    "    BertTokenizer, \n",
    "    BertTokenizerFast, \n",
    "    BertEmbeddings,\n",
    "    BfBertEmbeddings,\n",
    "    BfBertEncoder,\n",
    "    BertConfig,\n",
    "    BfBertSelfAttention,\n",
    "    BfBertForMaskedLM\n",
    ")\n",
    "from collections import Counter, OrderedDict\n",
    "from typing import List\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sentence_to_tokens_and_target_idx(sent: str, tokenizer):\n",
    "    pre, target, post = sent.split(\"***\")\n",
    "    if \"mask\" in target.lower():\n",
    "        target = [\"[MASK]\"]\n",
    "    else:\n",
    "        target = tokenizer.tokenize(target)\n",
    "    tokens = [\"[CLS]\"] + tokenizer.tokenize(pre)\n",
    "    target_idx = len(tokens)\n",
    "    # print(target_idx)\n",
    "    tokens += target + tokenizer.tokenize(post) + [\"[SEP]\"]\n",
    "    return tokens, target_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-11 11:42:18.562355: E external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  101  1037 17711  2015  4949  1997  4407  1005  1055  3987 13345   103\n",
      "   1037  3481  2006  1996  2479  1010  2021  2009  2003 20293  1012   102]] ['[CLS]', 'a', '1770', '##s', 'map', 'of', 'philadelphia', \"'\", 's', 'naval', 'defenses', '[MASK]', 'a', 'fort', 'on', 'the', 'island', ',', 'but', 'it', 'is', 'unidentified', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Establish data\n",
    "# model_id = \"google/bert_uncased_L-2_H-128_A-2\"\n",
    "model_id = \"google/bert_uncased_L-6_H-128_A-2\"\n",
    "config = BertConfig.from_pretrained(pretrained_model_name_or_path=\"../../brunoflow/models/bert/config-tiny.json\")\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_id)\n",
    "text = \"a 1770s map of philadelphia 's naval defenses ***mask*** a fort on the island , but it is unidentified .\"\n",
    "good_word = \"shows\"\n",
    "bad_word = \"show\"\n",
    "word_ids = tokenizer.convert_tokens_to_ids([good_word, bad_word])\n",
    "\n",
    "# tokenize text and pass into model\n",
    "tokens, target_idx = convert_sentence_to_tokens_and_target_idx(text, tokenizer)\n",
    "input_ids = np.expand_dims(tokenizer.convert_tokens_to_ids(tokens), axis=0)\n",
    "jax_input_ids = bf.Node(jnp.array(input_ids, dtype=int), name=\"inputs\")\n",
    "\n",
    "print(input_ids, tokens)\n",
    "# tokens = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "# input_ids = tokens[\"input_ids\"]\n",
    "# jax_input_ids = bf.Node(jnp.array(input_ids.numpy(), dtype=int), name=\"inputs\")\n",
    "# print(input_ids, input_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/code/rycolab/transformers/src/transformers/models/bert/modeling_bf_bert.py:178: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in zeros is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  bf.Parameter(jnp.zeros(self.position_ids.shape, dtype=jnp.int64), name=\"position_ids\"),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: not detaching params for module BfBertForMaskedLM when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertModel when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertEmbeddings when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Embedding(word_embeddings) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Embedding(position_embeddings) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Embedding(token_type_embeddings) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module LayerNorm(in BfBertEmbeddings) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Dropout when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertEncoder when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module ModuleList when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertLayer when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertAttention when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertSelfAttention when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Linear(query) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Linear(key) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Linear(value) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Dropout when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertSelfOutput when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Linear(in BfBertSelfOutput) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module LayerNorm(in BfBertSelfOutput) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Dropout(in BfBertSelfOutput) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertIntermediate when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Linear(in BfBertIntermediate) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertOutput when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Linear(in BfBertOutput) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module LayerNorm(in BfBertOutput) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Dropout(in BfBertOutput) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertLayer when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertAttention when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertSelfAttention when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Linear(query) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Linear(key) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Linear(value) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Dropout when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertSelfOutput when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Linear(in BfBertSelfOutput) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module LayerNorm(in BfBertSelfOutput) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Dropout(in BfBertSelfOutput) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertIntermediate when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Linear(in BfBertIntermediate) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertOutput when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Linear(in BfBertOutput) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module LayerNorm(in BfBertOutput) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Dropout(in BfBertOutput) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertLayer when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertAttention when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertSelfAttention when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Linear(query) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Linear(key) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Linear(value) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Dropout when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertSelfOutput when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Linear(in BfBertSelfOutput) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module LayerNorm(in BfBertSelfOutput) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Dropout(in BfBertSelfOutput) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertIntermediate when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Linear(in BfBertIntermediate) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertOutput when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Linear(in BfBertOutput) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module LayerNorm(in BfBertOutput) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Dropout(in BfBertOutput) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertLayer when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertAttention when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertSelfAttention when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Linear(query) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Linear(key) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Linear(value) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Dropout when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertSelfOutput when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Linear(in BfBertSelfOutput) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module LayerNorm(in BfBertSelfOutput) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Dropout(in BfBertSelfOutput) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertIntermediate when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Linear(in BfBertIntermediate) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertOutput when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Linear(in BfBertOutput) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module LayerNorm(in BfBertOutput) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Dropout(in BfBertOutput) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertLayer when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertAttention when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertSelfAttention when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Linear(query) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Linear(key) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Linear(value) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Dropout when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertSelfOutput when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Linear(in BfBertSelfOutput) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module LayerNorm(in BfBertSelfOutput) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Dropout(in BfBertSelfOutput) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertIntermediate when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Linear(in BfBertIntermediate) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertOutput when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Linear(in BfBertOutput) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module LayerNorm(in BfBertOutput) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Dropout(in BfBertOutput) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertLayer when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertAttention when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertSelfAttention when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Linear(query) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Linear(key) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Linear(value) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Dropout when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertSelfOutput when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Linear(in BfBertSelfOutput) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module LayerNorm(in BfBertSelfOutput) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Dropout(in BfBertSelfOutput) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertIntermediate when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Linear(in BfBertIntermediate) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertOutput when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Linear(in BfBertOutput) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module LayerNorm(in BfBertOutput) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Dropout(in BfBertOutput) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertOnlyMLMHead when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertLMPredictionHead when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertPredictionHeadTransform when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Linear(in BfBertPredictionHeadTransform) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module LayerNorm when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Linear(in BfBertLMPredictionHead) when saving state dict bc BF doesn't support that.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bert_uncased_L-6_H-128_A-2 were not used when initializing BfBertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BfBertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BfBertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Node of name + has inputs from different modules, {Linear(in BfBertLMPredictionHead)(), Embedding(token_type_embeddings)(2, 128)}. Picking its module to be the FIRST one, Linear(in BfBertLMPredictionHead)().\n",
      "WARNING: Node of name + has inputs from different modules, {Linear(in BfBertLMPredictionHead)(), Embedding(position_embeddings)(512, 128)}. Picking its module to be the FIRST one, Linear(in BfBertLMPredictionHead)().\n",
      "WARNING: Node of name * has inputs from different modules, {Linear(in BfBertLMPredictionHead)(), LayerNorm(in BfBertEmbeddings)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertEmbeddings)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name matmul has inputs from different modules, {LayerNorm(in BfBertEmbeddings)((128,), eps=1e-12, elementwise_affine=True), Linear(query)()}. Picking its module to be the FIRST one, LayerNorm(in BfBertEmbeddings)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name + has inputs from different modules, {LayerNorm(in BfBertEmbeddings)((128,), eps=1e-12, elementwise_affine=True), Linear(query)()}. Picking its module to be the FIRST one, LayerNorm(in BfBertEmbeddings)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name matmul has inputs from different modules, {LayerNorm(in BfBertEmbeddings)((128,), eps=1e-12, elementwise_affine=True), Linear(key)()}. Picking its module to be the FIRST one, LayerNorm(in BfBertEmbeddings)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name + has inputs from different modules, {LayerNorm(in BfBertEmbeddings)((128,), eps=1e-12, elementwise_affine=True), Linear(key)()}. Picking its module to be the FIRST one, LayerNorm(in BfBertEmbeddings)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name matmul has inputs from different modules, {LayerNorm(in BfBertEmbeddings)((128,), eps=1e-12, elementwise_affine=True), Linear(value)()}. Picking its module to be the FIRST one, LayerNorm(in BfBertEmbeddings)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name + has inputs from different modules, {LayerNorm(in BfBertEmbeddings)((128,), eps=1e-12, elementwise_affine=True), Linear(value)()}. Picking its module to be the FIRST one, LayerNorm(in BfBertEmbeddings)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name matmul has inputs from different modules, {LayerNorm(in BfBertEmbeddings)((128,), eps=1e-12, elementwise_affine=True), Linear(in BfBertSelfOutput)()}. Picking its module to be the FIRST one, LayerNorm(in BfBertEmbeddings)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name + has inputs from different modules, {LayerNorm(in BfBertEmbeddings)((128,), eps=1e-12, elementwise_affine=True), Linear(in BfBertSelfOutput)()}. Picking its module to be the FIRST one, LayerNorm(in BfBertEmbeddings)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name * has inputs from different modules, {LayerNorm(in BfBertEmbeddings)((128,), eps=1e-12, elementwise_affine=True), LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name matmul has inputs from different modules, {LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True), Linear(in BfBertIntermediate)()}. Picking its module to be the FIRST one, LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name + has inputs from different modules, {LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True), Linear(in BfBertIntermediate)()}. Picking its module to be the FIRST one, LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name matmul has inputs from different modules, {LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True), Linear(in BfBertOutput)()}. Picking its module to be the FIRST one, LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name + has inputs from different modules, {LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True), Linear(in BfBertOutput)()}. Picking its module to be the FIRST one, LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name * has inputs from different modules, {LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True), LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name matmul has inputs from different modules, {Linear(query)(), LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name + has inputs from different modules, {Linear(query)(), LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name matmul has inputs from different modules, {Linear(key)(), LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name + has inputs from different modules, {Linear(key)(), LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name matmul has inputs from different modules, {Linear(value)(), LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name + has inputs from different modules, {Linear(value)(), LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name matmul has inputs from different modules, {LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True), Linear(in BfBertSelfOutput)()}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name + has inputs from different modules, {LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True), Linear(in BfBertSelfOutput)()}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name * has inputs from different modules, {LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True), LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name matmul has inputs from different modules, {Linear(in BfBertIntermediate)(), LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name + has inputs from different modules, {Linear(in BfBertIntermediate)(), LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name matmul has inputs from different modules, {Linear(in BfBertOutput)(), LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name + has inputs from different modules, {Linear(in BfBertOutput)(), LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name * has inputs from different modules, {LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True), LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name matmul has inputs from different modules, {LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True), Linear(query)()}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name + has inputs from different modules, {LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True), Linear(query)()}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name matmul has inputs from different modules, {LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True), Linear(key)()}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name + has inputs from different modules, {LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True), Linear(key)()}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name matmul has inputs from different modules, {LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True), Linear(value)()}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name + has inputs from different modules, {LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True), Linear(value)()}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name matmul has inputs from different modules, {LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True), Linear(in BfBertSelfOutput)()}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name + has inputs from different modules, {LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True), Linear(in BfBertSelfOutput)()}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name * has inputs from different modules, {LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True), LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name matmul has inputs from different modules, {Linear(in BfBertIntermediate)(), LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name + has inputs from different modules, {Linear(in BfBertIntermediate)(), LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name matmul has inputs from different modules, {LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True), Linear(in BfBertOutput)()}. Picking its module to be the FIRST one, LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name + has inputs from different modules, {LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True), Linear(in BfBertOutput)()}. Picking its module to be the FIRST one, LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name * has inputs from different modules, {LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True), LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name matmul has inputs from different modules, {LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True), Linear(query)()}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name + has inputs from different modules, {LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True), Linear(query)()}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name matmul has inputs from different modules, {Linear(key)(), LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name + has inputs from different modules, {Linear(key)(), LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name matmul has inputs from different modules, {LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True), Linear(value)()}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name + has inputs from different modules, {LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True), Linear(value)()}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name matmul has inputs from different modules, {LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True), Linear(in BfBertSelfOutput)()}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name + has inputs from different modules, {LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True), Linear(in BfBertSelfOutput)()}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name * has inputs from different modules, {LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True), LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name matmul has inputs from different modules, {LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True), Linear(in BfBertIntermediate)()}. Picking its module to be the FIRST one, LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name + has inputs from different modules, {LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True), Linear(in BfBertIntermediate)()}. Picking its module to be the FIRST one, LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name matmul has inputs from different modules, {Linear(in BfBertOutput)(), LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name + has inputs from different modules, {Linear(in BfBertOutput)(), LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name * has inputs from different modules, {LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True), LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name matmul has inputs from different modules, {LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True), Linear(query)()}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name + has inputs from different modules, {LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True), Linear(query)()}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name matmul has inputs from different modules, {Linear(key)(), LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name + has inputs from different modules, {Linear(key)(), LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name matmul has inputs from different modules, {Linear(value)(), LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name + has inputs from different modules, {Linear(value)(), LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name matmul has inputs from different modules, {LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True), Linear(in BfBertSelfOutput)()}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name + has inputs from different modules, {LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True), Linear(in BfBertSelfOutput)()}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name * has inputs from different modules, {LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True), LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name matmul has inputs from different modules, {Linear(in BfBertIntermediate)(), LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name + has inputs from different modules, {Linear(in BfBertIntermediate)(), LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name matmul has inputs from different modules, {Linear(in BfBertOutput)(), LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name + has inputs from different modules, {Linear(in BfBertOutput)(), LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name * has inputs from different modules, {LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True), LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name matmul has inputs from different modules, {LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True), Linear(query)()}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name + has inputs from different modules, {LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True), Linear(query)()}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name matmul has inputs from different modules, {LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True), Linear(key)()}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name + has inputs from different modules, {LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True), Linear(key)()}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name matmul has inputs from different modules, {LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True), Linear(value)()}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name + has inputs from different modules, {LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True), Linear(value)()}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name matmul has inputs from different modules, {LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True), Linear(in BfBertSelfOutput)()}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name + has inputs from different modules, {LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True), Linear(in BfBertSelfOutput)()}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name * has inputs from different modules, {LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True), LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name matmul has inputs from different modules, {Linear(in BfBertIntermediate)(), LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name + has inputs from different modules, {Linear(in BfBertIntermediate)(), LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name matmul has inputs from different modules, {Linear(in BfBertOutput)(), LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name + has inputs from different modules, {Linear(in BfBertOutput)(), LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name * has inputs from different modules, {LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True), LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name matmul has inputs from different modules, {LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True), Linear(in BfBertPredictionHeadTransform)()}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name + has inputs from different modules, {LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True), Linear(in BfBertPredictionHeadTransform)()}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name * has inputs from different modules, {LayerNorm((128,), eps=1e-12, elementwise_affine=True), LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name matmul has inputs from different modules, {LayerNorm((128,), eps=1e-12, elementwise_affine=True), Linear(in BfBertLMPredictionHead)()}. Picking its module to be the FIRST one, LayerNorm((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name + has inputs from different modules, {LayerNorm((128,), eps=1e-12, elementwise_affine=True), Linear(in BfBertLMPredictionHead)()}. Picking its module to be the FIRST one, LayerNorm((128,), eps=1e-12, elementwise_affine=True).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/miniconda3/envs/jax-hf/lib/python3.9/site-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\"scatter inputs have incompatible types: cannot safely cast \"\n",
      "/home/kevin/miniconda3/envs/jax-hf/lib/python3.9/site-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\"scatter inputs have incompatible types: cannot safely cast \"\n"
     ]
    }
   ],
   "source": [
    "# Create BfBertForMaskedLM model\n",
    "config = BertConfig.from_pretrained(pretrained_model_name_or_path=\"../../brunoflow/models/bert/config-tiny.json\")\n",
    "bf_model = BfBertForMaskedLM.from_pretrained(model_id)\n",
    "\n",
    "# Visualize output of forward pass of BfBertEmbeddings\n",
    "bf_model.train(False)\n",
    "out_bf = bf_model(input_ids=jax_input_ids).logits # shape = (vs, seq_len)\n",
    "qoi = out_bf[:, target_idx] # shape = (1, vs)\n",
    "qoi = qoi[:, word_ids[0]] - qoi[:, word_ids[1]] # shape = (1,)\n",
    "# out_bf.visualize(collapse_to_modules=True)\n",
    "# print(bf_embs)\n",
    "bf_model.train(True)\n",
    "\n",
    "qoi.backprop(values_to_compute=(\"max_grad\",))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/miniconda3/envs/jax-hf/lib/python3.9/site-packages/pygraphviz/agraph.py:1405: RuntimeWarning: neato: graph is too large for cairo-renderer bitmaps. Scaling by 0.856609 to fit\n",
      "\n",
      "  warnings.warn(b\"\".join(errors).decode(self.encoding), RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "qoi.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matching_nodes(root: bf.Node, name: str):\n",
    "    def _find_matching_nodes(root: bf.Node, name: str, visited=set()):\n",
    "        assert isinstance(root, bf.Node), f\"root input must be a Node, instead received {root}\"\n",
    "        if root in visited:\n",
    "            return []\n",
    "            \n",
    "        matching_nodes = []\n",
    "        if root.name is not None and name in root.name:\n",
    "            matching_nodes.append(root)\n",
    "            # return [root]\n",
    "        for inp in root.inputs:\n",
    "            if isinstance(inp, bf.Node):\n",
    "                matching_nodes_in_subtree = _find_matching_nodes(inp, name, visited=visited)\n",
    "                visited.add(inp)\n",
    "                if matching_nodes_in_subtree:\n",
    "                    matching_nodes = matching_nodes_in_subtree + matching_nodes\n",
    "        \n",
    "        return matching_nodes\n",
    "    return _find_matching_nodes(root, name, visited=set())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does BERT use the skip or self-attention mechanism more?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_to_bert_attn_nodes = find_matching_nodes(out_bf[0], \"input to bertattention\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "(1, 24, 128)\n",
      "names of parent nodes of input to bert attn: 4 ['matmul', 'matmul', 'matmul', 'combine self_attention_output and bert attention input 8788907714527']\n"
     ]
    }
   ],
   "source": [
    "print(len(input_to_bert_attn_nodes)) # should match number of bert layers\n",
    "print(input_to_bert_attn_nodes[-1].shape) # (1, seq_len, hidden_sz)\n",
    "layer0_bert_attn_input = input_to_bert_attn_nodes[0]\n",
    "print(\"names of parent nodes of input to bert attn:\", len(layer0_bert_attn_input.get_parents()), [p.name for p in layer0_bert_attn_input.get_parents()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_max_grad_parents_bert_attn_input(input_to_bert_attn_node: bf.Node, tokens: List[str]):\n",
    "    # Number of hidden units corresponding to each max grad parent option (for the input to bert attention Node)\n",
    "    count_hidden_unit_max_grad_parents: OrderedDict = OrderedDict({\n",
    "        tokens[i]: [\n",
    "            (k.name, v) for k,v in Counter(input_to_bert_attn_node.get_max_grad_parent()[0, i]).items()\n",
    "        ] for i in range(len(input_to_bert_attn_node.get_max_grad_parent()[0]))\n",
    "    }) # keys are tokens, values are a list of the counts of # emb units for that token which have each of the possible max grad parents\n",
    "\n",
    "    skip_and_attn_max_grads_per_word = []\n",
    "    for i in range(len(input_to_bert_attn_node.get_max_grad_parent()[0])): # each word\n",
    "        max_grad_parent_for_emb = input_to_bert_attn_node.get_max_grad_parent()[0, i] # shape = (emb_sz,)\n",
    "        skip_max_grad = 0\n",
    "        attn_max_grad = 0\n",
    "        for j in range(len(max_grad_parent_for_emb)):\n",
    "            emb_unit_max_grad_val = input_to_bert_attn_node.max_grad_of_output_wrt_node[0][0][i][j]\n",
    "            emb_unit_max_grad_parent = input_to_bert_attn_node.max_grad_of_output_wrt_node[1][0][i][j]\n",
    "            if emb_unit_max_grad_parent.name == \"matmul\":\n",
    "                attn_max_grad += emb_unit_max_grad_val\n",
    "            elif \"combine self_attention_output and bert attention input\" in emb_unit_max_grad_parent.name:\n",
    "                skip_max_grad += emb_unit_max_grad_val\n",
    "            else:\n",
    "                raise ValueError(f\"uhoh! received an unexpected parent, {emb_unit_max_grad_parent}\")\n",
    "        skip_and_attn_max_grads_per_word.append((skip_max_grad, attn_max_grad))\n",
    "\n",
    "    grad_diff_between_skip_and_attention = OrderedDict({tokens[i]: skip_and_attn_max_grads_per_word[i][0] - skip_and_attn_max_grads_per_word[i][1] for i in range(len(skip_and_attn_max_grads_per_word))})\n",
    "\n",
    "    return count_hidden_unit_max_grad_parents, grad_diff_between_skip_and_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('[CLS]',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907714527',\n",
      "                128)]),\n",
      "             ('a',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907714527',\n",
      "                128)]),\n",
      "             ('1770',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907714527',\n",
      "                110),\n",
      "               ('matmul', 17),\n",
      "               ('matmul', 1)]),\n",
      "             ('##s',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907714527',\n",
      "                127),\n",
      "               ('matmul', 1)]),\n",
      "             ('map',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907714527',\n",
      "                128)]),\n",
      "             ('of',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907714527',\n",
      "                128)]),\n",
      "             ('philadelphia',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907714527',\n",
      "                66),\n",
      "               ('matmul', 38),\n",
      "               ('matmul', 24)]),\n",
      "             (\"'\",\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907714527',\n",
      "                127),\n",
      "               ('matmul', 1)]),\n",
      "             ('s',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907714527',\n",
      "                128)]),\n",
      "             ('naval',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907714527',\n",
      "                121),\n",
      "               ('matmul', 7)]),\n",
      "             ('defenses',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907714527',\n",
      "                128)]),\n",
      "             ('[MASK]',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907714527',\n",
      "                128)]),\n",
      "             ('fort',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907714527',\n",
      "                128)]),\n",
      "             ('on',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907714527',\n",
      "                128)]),\n",
      "             ('the',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907714527',\n",
      "                127),\n",
      "               ('matmul', 1)]),\n",
      "             ('island',\n",
      "              [('matmul', 55),\n",
      "               ('matmul', 28),\n",
      "               ('combine self_attention_output and bert attention input '\n",
      "                '8788907714527',\n",
      "                45)]),\n",
      "             (',',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907714527',\n",
      "                128)]),\n",
      "             ('but',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907714527',\n",
      "                128)]),\n",
      "             ('it',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907714527',\n",
      "                128)]),\n",
      "             ('is',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907714527',\n",
      "                127),\n",
      "               ('matmul', 1)]),\n",
      "             ('unidentified',\n",
      "              [('matmul', 42),\n",
      "               ('combine self_attention_output and bert attention input '\n",
      "                '8788907714527',\n",
      "                76),\n",
      "               ('matmul', 10)]),\n",
      "             ('.',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907714527',\n",
      "                128)]),\n",
      "             ('[SEP]',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907714527',\n",
      "                128)])])\n",
      "OrderedDict([('[CLS]', DeviceArray(15.883381, dtype=float32)),\n",
      "             ('a', DeviceArray(48.850807, dtype=float32)),\n",
      "             ('1770', DeviceArray(17.473118, dtype=float32)),\n",
      "             ('##s', DeviceArray(5.533828, dtype=float32)),\n",
      "             ('map', DeviceArray(28.55528, dtype=float32)),\n",
      "             ('of', DeviceArray(11.91217, dtype=float32)),\n",
      "             ('philadelphia', DeviceArray(-2.27673, dtype=float32)),\n",
      "             (\"'\", DeviceArray(7.091193, dtype=float32)),\n",
      "             ('s', DeviceArray(9.242327, dtype=float32)),\n",
      "             ('naval', DeviceArray(40.828945, dtype=float32)),\n",
      "             ('defenses', DeviceArray(37.195797, dtype=float32)),\n",
      "             ('[MASK]', DeviceArray(275.05634, dtype=float32)),\n",
      "             ('fort', DeviceArray(50.79479, dtype=float32)),\n",
      "             ('on', DeviceArray(23.78974, dtype=float32)),\n",
      "             ('the', DeviceArray(5.295932, dtype=float32)),\n",
      "             ('island', DeviceArray(-4.2009964, dtype=float32)),\n",
      "             (',', DeviceArray(9.807892, dtype=float32)),\n",
      "             ('but', DeviceArray(9.7999325, dtype=float32)),\n",
      "             ('it', DeviceArray(8.244433, dtype=float32)),\n",
      "             ('is', DeviceArray(10.521877, dtype=float32)),\n",
      "             ('unidentified', DeviceArray(0.35490894, dtype=float32)),\n",
      "             ('.', DeviceArray(12.900608, dtype=float32)),\n",
      "             ('[SEP]', DeviceArray(15.419886, dtype=float32))])\n",
      "\n",
      "OrderedDict([('[CLS]',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907477989',\n",
      "                126),\n",
      "               ('matmul', 2)]),\n",
      "             ('a',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907477989',\n",
      "                128)]),\n",
      "             ('1770',\n",
      "              [('matmul', 75),\n",
      "               ('combine self_attention_output and bert attention input '\n",
      "                '8788907477989',\n",
      "                38),\n",
      "               ('matmul', 15)]),\n",
      "             ('##s',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907477989',\n",
      "                128)]),\n",
      "             ('map',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907477989',\n",
      "                128)]),\n",
      "             ('of',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907477989',\n",
      "                128)]),\n",
      "             ('philadelphia',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907477989',\n",
      "                122),\n",
      "               ('matmul', 6)]),\n",
      "             (\"'\",\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907477989',\n",
      "                124),\n",
      "               ('matmul', 4)]),\n",
      "             ('s',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907477989',\n",
      "                128)]),\n",
      "             ('naval',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907477989',\n",
      "                128)]),\n",
      "             ('defenses',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907477989',\n",
      "                128)]),\n",
      "             ('[MASK]',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907477989',\n",
      "                128)]),\n",
      "             ('fort',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907477989',\n",
      "                128)]),\n",
      "             ('on',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907477989',\n",
      "                128)]),\n",
      "             ('the',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907477989',\n",
      "                118),\n",
      "               ('matmul', 10)]),\n",
      "             ('island',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907477989',\n",
      "                124),\n",
      "               ('matmul', 4)]),\n",
      "             (',',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907477989',\n",
      "                128)]),\n",
      "             ('but',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907477989',\n",
      "                125),\n",
      "               ('matmul', 3)]),\n",
      "             ('it',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907477989',\n",
      "                101),\n",
      "               ('matmul', 20),\n",
      "               ('matmul', 7)]),\n",
      "             ('is',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907477989',\n",
      "                111),\n",
      "               ('matmul', 15),\n",
      "               ('matmul', 2)]),\n",
      "             ('unidentified',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907477989',\n",
      "                82),\n",
      "               ('matmul', 40),\n",
      "               ('matmul', 6)]),\n",
      "             ('.',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907477989',\n",
      "                126),\n",
      "               ('matmul', 2)]),\n",
      "             ('[SEP]',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907477989',\n",
      "                128)])])\n",
      "OrderedDict([('[CLS]', DeviceArray(26.549803, dtype=float32)),\n",
      "             ('a', DeviceArray(49.049706, dtype=float32)),\n",
      "             ('1770', DeviceArray(-12.663247, dtype=float32)),\n",
      "             ('##s', DeviceArray(5.038073, dtype=float32)),\n",
      "             ('map', DeviceArray(28.509693, dtype=float32)),\n",
      "             ('of', DeviceArray(11.759639, dtype=float32)),\n",
      "             ('philadelphia', DeviceArray(8.714518, dtype=float32)),\n",
      "             (\"'\", DeviceArray(6.45392, dtype=float32)),\n",
      "             ('s', DeviceArray(9.006098, dtype=float32)),\n",
      "             ('naval', DeviceArray(41.163933, dtype=float32)),\n",
      "             ('defenses', DeviceArray(33.98246, dtype=float32)),\n",
      "             ('[MASK]', DeviceArray(325.3573, dtype=float32)),\n",
      "             ('fort', DeviceArray(49.63008, dtype=float32)),\n",
      "             ('on', DeviceArray(22.224924, dtype=float32)),\n",
      "             ('the', DeviceArray(4.6348424, dtype=float32)),\n",
      "             ('island', DeviceArray(4.73368, dtype=float32)),\n",
      "             (',', DeviceArray(10.393808, dtype=float32)),\n",
      "             ('but', DeviceArray(8.609225, dtype=float32)),\n",
      "             ('it', DeviceArray(3.7909904, dtype=float32)),\n",
      "             ('is', DeviceArray(6.7477264, dtype=float32)),\n",
      "             ('unidentified', DeviceArray(0.16800737, dtype=float32)),\n",
      "             ('.', DeviceArray(13.00039, dtype=float32)),\n",
      "             ('[SEP]', DeviceArray(21.357883, dtype=float32))])\n",
      "\n",
      "OrderedDict([('[CLS]',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907700971',\n",
      "                116),\n",
      "               ('matmul', 12)]),\n",
      "             ('a',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907700971',\n",
      "                110),\n",
      "               ('matmul', 18)]),\n",
      "             ('1770',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907700971',\n",
      "                109),\n",
      "               ('matmul', 14),\n",
      "               ('matmul', 2),\n",
      "               ('matmul', 3)]),\n",
      "             ('##s',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907700971',\n",
      "                42),\n",
      "               ('matmul', 53),\n",
      "               ('matmul', 33)]),\n",
      "             ('map',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907700971',\n",
      "                128)]),\n",
      "             ('of',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907700971',\n",
      "                127),\n",
      "               ('matmul', 1)]),\n",
      "             ('philadelphia',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907700971',\n",
      "                107),\n",
      "               ('matmul', 17),\n",
      "               ('matmul', 3),\n",
      "               ('matmul', 1)]),\n",
      "             (\"'\",\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907700971',\n",
      "                86),\n",
      "               ('matmul', 34),\n",
      "               ('matmul', 7),\n",
      "               ('matmul', 1)]),\n",
      "             ('s',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907700971',\n",
      "                116),\n",
      "               ('matmul', 3),\n",
      "               ('matmul', 5),\n",
      "               ('matmul', 4)]),\n",
      "             ('naval',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907700971',\n",
      "                5),\n",
      "               ('matmul', 101),\n",
      "               ('matmul', 22)]),\n",
      "             ('defenses',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907700971',\n",
      "                66),\n",
      "               ('matmul', 38),\n",
      "               ('matmul', 23),\n",
      "               ('matmul', 1)]),\n",
      "             ('[MASK]',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907700971',\n",
      "                128)]),\n",
      "             ('fort',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907700971',\n",
      "                104),\n",
      "               ('matmul', 24)]),\n",
      "             ('on',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907700971',\n",
      "                128)]),\n",
      "             ('the',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907700971',\n",
      "                123),\n",
      "               ('matmul', 2),\n",
      "               ('matmul', 3)]),\n",
      "             ('island',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907700971',\n",
      "                71),\n",
      "               ('matmul', 35),\n",
      "               ('matmul', 20),\n",
      "               ('matmul', 2)]),\n",
      "             (',',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907700971',\n",
      "                122),\n",
      "               ('matmul', 6)]),\n",
      "             ('but',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907700971',\n",
      "                117),\n",
      "               ('matmul', 11)]),\n",
      "             ('it',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907700971',\n",
      "                83),\n",
      "               ('matmul', 31),\n",
      "               ('matmul', 14)]),\n",
      "             ('is',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907700971',\n",
      "                115),\n",
      "               ('matmul', 10),\n",
      "               ('matmul', 3)]),\n",
      "             ('unidentified',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907700971',\n",
      "                77),\n",
      "               ('matmul', 9),\n",
      "               ('matmul', 36),\n",
      "               ('matmul', 6)]),\n",
      "             ('.',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907700971',\n",
      "                128)]),\n",
      "             ('[SEP]',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907700971',\n",
      "                128)])])\n",
      "OrderedDict([('[CLS]', DeviceArray(25.628782, dtype=float32)),\n",
      "             ('a', DeviceArray(30.153728, dtype=float32)),\n",
      "             ('1770', DeviceArray(7.1030817, dtype=float32)),\n",
      "             ('##s', DeviceArray(-2.7714975, dtype=float32)),\n",
      "             ('map', DeviceArray(32.72601, dtype=float32)),\n",
      "             ('of', DeviceArray(10.464437, dtype=float32)),\n",
      "             ('philadelphia', DeviceArray(6.4857044, dtype=float32)),\n",
      "             (\"'\", DeviceArray(1.3266187, dtype=float32)),\n",
      "             ('s', DeviceArray(6.674217, dtype=float32)),\n",
      "             ('naval', DeviceArray(-33.560013, dtype=float32)),\n",
      "             ('defenses', DeviceArray(-7.3904448, dtype=float32)),\n",
      "             ('[MASK]', DeviceArray(299.5921, dtype=float32)),\n",
      "             ('fort', DeviceArray(26.909538, dtype=float32)),\n",
      "             ('on', DeviceArray(20.815477, dtype=float32)),\n",
      "             ('the', DeviceArray(4.09614, dtype=float32)),\n",
      "             ('island', DeviceArray(-0.2552731, dtype=float32)),\n",
      "             (',', DeviceArray(7.9947877, dtype=float32)),\n",
      "             ('but', DeviceArray(6.7561746, dtype=float32)),\n",
      "             ('it', DeviceArray(0.6428256, dtype=float32)),\n",
      "             ('is', DeviceArray(7.0533876, dtype=float32)),\n",
      "             ('unidentified', DeviceArray(0.3644737, dtype=float32)),\n",
      "             ('.', DeviceArray(11.996691, dtype=float32)),\n",
      "             ('[SEP]', DeviceArray(19.619078, dtype=float32))])\n",
      "\n",
      "OrderedDict([('[CLS]',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907682070',\n",
      "                128)]),\n",
      "             ('a',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907682070',\n",
      "                80),\n",
      "               ('matmul', 25),\n",
      "               ('matmul', 23)]),\n",
      "             ('1770',\n",
      "              [('matmul', 101),\n",
      "               ('combine self_attention_output and bert attention input '\n",
      "                '8788907682070',\n",
      "                25),\n",
      "               ('matmul', 2)]),\n",
      "             ('##s',\n",
      "              [('matmul', 23),\n",
      "               ('combine self_attention_output and bert attention input '\n",
      "                '8788907682070',\n",
      "                101),\n",
      "               ('matmul', 3),\n",
      "               ('matmul', 1)]),\n",
      "             ('map',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907682070',\n",
      "                123),\n",
      "               ('matmul', 5)]),\n",
      "             ('of',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907682070',\n",
      "                98),\n",
      "               ('matmul', 6),\n",
      "               ('matmul', 20),\n",
      "               ('matmul', 4)]),\n",
      "             ('philadelphia',\n",
      "              [('matmul', 27),\n",
      "               ('matmul', 98),\n",
      "               ('combine self_attention_output and bert attention input '\n",
      "                '8788907682070',\n",
      "                3)]),\n",
      "             (\"'\",\n",
      "              [('matmul', 31),\n",
      "               ('matmul', 79),\n",
      "               ('combine self_attention_output and bert attention input '\n",
      "                '8788907682070',\n",
      "                18)]),\n",
      "             ('s',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907682070',\n",
      "                128)]),\n",
      "             ('naval',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907682070',\n",
      "                112),\n",
      "               ('matmul', 14),\n",
      "               ('matmul', 2)]),\n",
      "             ('defenses',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907682070',\n",
      "                127),\n",
      "               ('matmul', 1)]),\n",
      "             ('[MASK]',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907682070',\n",
      "                128)]),\n",
      "             ('fort',\n",
      "              [('matmul', 20),\n",
      "               ('matmul', 91),\n",
      "               ('combine self_attention_output and bert attention input '\n",
      "                '8788907682070',\n",
      "                17)]),\n",
      "             ('on',\n",
      "              [('matmul', 71),\n",
      "               ('combine self_attention_output and bert attention input '\n",
      "                '8788907682070',\n",
      "                7),\n",
      "               ('matmul', 48),\n",
      "               ('matmul', 2)]),\n",
      "             ('the',\n",
      "              [('matmul', 41),\n",
      "               ('matmul', 84),\n",
      "               ('combine self_attention_output and bert attention input '\n",
      "                '8788907682070',\n",
      "                2),\n",
      "               ('matmul', 1)]),\n",
      "             ('island',\n",
      "              [('matmul', 59),\n",
      "               ('combine self_attention_output and bert attention input '\n",
      "                '8788907682070',\n",
      "                35),\n",
      "               ('matmul', 34)]),\n",
      "             (',',\n",
      "              [('matmul', 72),\n",
      "               ('combine self_attention_output and bert attention input '\n",
      "                '8788907682070',\n",
      "                22),\n",
      "               ('matmul', 28),\n",
      "               ('matmul', 6)]),\n",
      "             ('but',\n",
      "              [('matmul', 39),\n",
      "               ('combine self_attention_output and bert attention input '\n",
      "                '8788907682070',\n",
      "                57),\n",
      "               ('matmul', 22),\n",
      "               ('matmul', 10)]),\n",
      "             ('it',\n",
      "              [('matmul', 40),\n",
      "               ('matmul', 14),\n",
      "               ('combine self_attention_output and bert attention input '\n",
      "                '8788907682070',\n",
      "                73),\n",
      "               ('matmul', 1)]),\n",
      "             ('is',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907682070',\n",
      "                122),\n",
      "               ('matmul', 4),\n",
      "               ('matmul', 2)]),\n",
      "             ('unidentified',\n",
      "              [('matmul', 41),\n",
      "               ('matmul', 37),\n",
      "               ('combine self_attention_output and bert attention input '\n",
      "                '8788907682070',\n",
      "                50)]),\n",
      "             ('.',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907682070',\n",
      "                76),\n",
      "               ('matmul', 42),\n",
      "               ('matmul', 6),\n",
      "               ('matmul', 4)]),\n",
      "             ('[SEP]',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907682070',\n",
      "                83),\n",
      "               ('matmul', 40),\n",
      "               ('matmul', 5)])])\n",
      "OrderedDict([('[CLS]', DeviceArray(36.807777, dtype=float32)),\n",
      "             ('a', DeviceArray(3.7462292, dtype=float32)),\n",
      "             ('1770', DeviceArray(-8.159486, dtype=float32)),\n",
      "             ('##s', DeviceArray(1.2356138, dtype=float32)),\n",
      "             ('map', DeviceArray(33.458275, dtype=float32)),\n",
      "             ('of', DeviceArray(4.2481823, dtype=float32)),\n",
      "             ('philadelphia', DeviceArray(-6.9537373, dtype=float32)),\n",
      "             (\"'\", DeviceArray(-3.291143, dtype=float32)),\n",
      "             ('s', DeviceArray(8.027609, dtype=float32)),\n",
      "             ('naval', DeviceArray(3.9168134, dtype=float32)),\n",
      "             ('defenses', DeviceArray(28.690084, dtype=float32)),\n",
      "             ('[MASK]', DeviceArray(299.45526, dtype=float32)),\n",
      "             ('fort', DeviceArray(-33.87364, dtype=float32)),\n",
      "             ('on', DeviceArray(-16.132261, dtype=float32)),\n",
      "             ('the', DeviceArray(-3.1333811, dtype=float32)),\n",
      "             ('island', DeviceArray(-2.1820683, dtype=float32)),\n",
      "             (',', DeviceArray(-5.3010488, dtype=float32)),\n",
      "             ('but', DeviceArray(-2.324108, dtype=float32)),\n",
      "             ('it', DeviceArray(-0.07147765, dtype=float32)),\n",
      "             ('is', DeviceArray(8.377839, dtype=float32)),\n",
      "             ('unidentified', DeviceArray(-1.4177665, dtype=float32)),\n",
      "             ('.', DeviceArray(0.33071232, dtype=float32)),\n",
      "             ('[SEP]', DeviceArray(3.282774, dtype=float32))])\n",
      "\n",
      "OrderedDict([('[CLS]',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907478812',\n",
      "                128)]),\n",
      "             ('a',\n",
      "              [('matmul', 93),\n",
      "               ('matmul', 33),\n",
      "               ('combine self_attention_output and bert attention input '\n",
      "                '8788907478812',\n",
      "                2)]),\n",
      "             ('1770',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907478812',\n",
      "                107),\n",
      "               ('matmul', 21)]),\n",
      "             ('##s',\n",
      "              [('matmul', 45),\n",
      "               ('combine self_attention_output and bert attention input '\n",
      "                '8788907478812',\n",
      "                36),\n",
      "               ('matmul', 46),\n",
      "               ('matmul', 1)]),\n",
      "             ('map',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907478812',\n",
      "                126),\n",
      "               ('matmul', 2)]),\n",
      "             ('of',\n",
      "              [('matmul', 49),\n",
      "               ('combine self_attention_output and bert attention input '\n",
      "                '8788907478812',\n",
      "                79)]),\n",
      "             ('philadelphia',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907478812',\n",
      "                89),\n",
      "               ('matmul', 13),\n",
      "               ('matmul', 26)]),\n",
      "             (\"'\",\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907478812',\n",
      "                41),\n",
      "               ('matmul', 32),\n",
      "               ('matmul', 53),\n",
      "               ('matmul', 2)]),\n",
      "             ('s',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907478812',\n",
      "                126),\n",
      "               ('matmul', 2)]),\n",
      "             ('naval',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907478812',\n",
      "                49),\n",
      "               ('matmul', 58),\n",
      "               ('matmul', 21)]),\n",
      "             ('defenses',\n",
      "              [('matmul', 88),\n",
      "               ('combine self_attention_output and bert attention input '\n",
      "                '8788907478812',\n",
      "                19),\n",
      "               ('matmul', 21)]),\n",
      "             ('[MASK]',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907478812',\n",
      "                127),\n",
      "               ('matmul', 1)]),\n",
      "             ('fort',\n",
      "              [('matmul', 55),\n",
      "               ('matmul', 63),\n",
      "               ('combine self_attention_output and bert attention input '\n",
      "                '8788907478812',\n",
      "                10)]),\n",
      "             ('on',\n",
      "              [('matmul', 68),\n",
      "               ('combine self_attention_output and bert attention input '\n",
      "                '8788907478812',\n",
      "                37),\n",
      "               ('matmul', 23)]),\n",
      "             ('the',\n",
      "              [('matmul', 83),\n",
      "               ('combine self_attention_output and bert attention input '\n",
      "                '8788907478812',\n",
      "                25),\n",
      "               ('matmul', 20)]),\n",
      "             ('island',\n",
      "              [('matmul', 74),\n",
      "               ('matmul', 44),\n",
      "               ('combine self_attention_output and bert attention input '\n",
      "                '8788907478812',\n",
      "                10)]),\n",
      "             (',',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907478812',\n",
      "                110),\n",
      "               ('matmul', 18)]),\n",
      "             ('but',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907478812',\n",
      "                128)]),\n",
      "             ('it',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907478812',\n",
      "                120),\n",
      "               ('matmul', 7),\n",
      "               ('matmul', 1)]),\n",
      "             ('is',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907478812',\n",
      "                128)]),\n",
      "             ('unidentified',\n",
      "              [('matmul', 46),\n",
      "               ('matmul', 33),\n",
      "               ('combine self_attention_output and bert attention input '\n",
      "                '8788907478812',\n",
      "                49)]),\n",
      "             ('.',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907478812',\n",
      "                128)]),\n",
      "             ('[SEP]',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907478812',\n",
      "                122),\n",
      "               ('matmul', 6)])])\n",
      "OrderedDict([('[CLS]', DeviceArray(35.06779, dtype=float32)),\n",
      "             ('a', DeviceArray(-24.904957, dtype=float32)),\n",
      "             ('1770', DeviceArray(2.023059, dtype=float32)),\n",
      "             ('##s', DeviceArray(-1.1965811, dtype=float32)),\n",
      "             ('map', DeviceArray(32.900524, dtype=float32)),\n",
      "             ('of', DeviceArray(0.20028639, dtype=float32)),\n",
      "             ('philadelphia', DeviceArray(0.39465457, dtype=float32)),\n",
      "             (\"'\", DeviceArray(-0.5631724, dtype=float32)),\n",
      "             ('s', DeviceArray(7.418116, dtype=float32)),\n",
      "             ('naval', DeviceArray(-2.187242, dtype=float32)),\n",
      "             ('defenses', DeviceArray(-20.00929, dtype=float32)),\n",
      "             ('[MASK]', DeviceArray(254.50336, dtype=float32)),\n",
      "             ('fort', DeviceArray(-12.029471, dtype=float32)),\n",
      "             ('on', DeviceArray(-2.097718, dtype=float32)),\n",
      "             ('the', DeviceArray(-0.39496398, dtype=float32)),\n",
      "             ('island', DeviceArray(-1.5966768, dtype=float32)),\n",
      "             (',', DeviceArray(2.0949275, dtype=float32)),\n",
      "             ('but', DeviceArray(5.691873, dtype=float32)),\n",
      "             ('it', DeviceArray(3.1243358, dtype=float32)),\n",
      "             ('is', DeviceArray(8.888982, dtype=float32)),\n",
      "             ('unidentified', DeviceArray(-0.89661866, dtype=float32)),\n",
      "             ('.', DeviceArray(10.728252, dtype=float32)),\n",
      "             ('[SEP]', DeviceArray(98.58209, dtype=float32))])\n",
      "\n",
      "OrderedDict([('[CLS]', [('matmul', 44), ('matmul', 84)]),\n",
      "             ('a', [('matmul', 75), ('matmul', 53)]),\n",
      "             ('1770',\n",
      "              [('matmul', 89),\n",
      "               ('matmul', 38),\n",
      "               ('combine self_attention_output and bert attention input '\n",
      "                '8788907479330',\n",
      "                1)]),\n",
      "             ('##s',\n",
      "              [('matmul', 89),\n",
      "               ('matmul', 38),\n",
      "               ('combine self_attention_output and bert attention input '\n",
      "                '8788907479330',\n",
      "                1)]),\n",
      "             ('map', [('matmul', 106), ('matmul', 22)]),\n",
      "             ('of',\n",
      "              [('matmul', 89),\n",
      "               ('matmul', 38),\n",
      "               ('combine self_attention_output and bert attention input '\n",
      "                '8788907479330',\n",
      "                1)]),\n",
      "             ('philadelphia', [('matmul', 38), ('matmul', 90)]),\n",
      "             (\"'\", [('matmul', 81), ('matmul', 47)]),\n",
      "             ('s', [('matmul', 93), ('matmul', 35)]),\n",
      "             ('naval', [('matmul', 74), ('matmul', 54)]),\n",
      "             ('defenses',\n",
      "              [('matmul', 88),\n",
      "               ('matmul', 39),\n",
      "               ('combine self_attention_output and bert attention input '\n",
      "                '8788907479330',\n",
      "                1)]),\n",
      "             ('[MASK]',\n",
      "              [('combine self_attention_output and bert attention input '\n",
      "                '8788907479330',\n",
      "                115),\n",
      "               ('matmul', 13)]),\n",
      "             ('fort', [('matmul', 85), ('matmul', 43)]),\n",
      "             ('on', [('matmul', 39), ('matmul', 89)]),\n",
      "             ('the', [('matmul', 77), ('matmul', 51)]),\n",
      "             ('island',\n",
      "              [('matmul', 81),\n",
      "               ('matmul', 46),\n",
      "               ('combine self_attention_output and bert attention input '\n",
      "                '8788907479330',\n",
      "                1)]),\n",
      "             (',', [('matmul', 70), ('matmul', 58)]),\n",
      "             ('but', [('matmul', 65), ('matmul', 63)]),\n",
      "             ('it', [('matmul', 53), ('matmul', 75)]),\n",
      "             ('is', [('matmul', 40), ('matmul', 88)]),\n",
      "             ('unidentified', [('matmul', 88), ('matmul', 40)]),\n",
      "             ('.', [('matmul', 84), ('matmul', 44)]),\n",
      "             ('[SEP]', [('matmul', 82), ('matmul', 46)])])\n",
      "OrderedDict([('[CLS]', DeviceArray(-28.261866, dtype=float32)),\n",
      "             ('a', DeviceArray(-2.1865602, dtype=float32)),\n",
      "             ('1770', DeviceArray(-2.8452728, dtype=float32)),\n",
      "             ('##s', DeviceArray(-0.9991833, dtype=float32)),\n",
      "             ('map', DeviceArray(-27.468517, dtype=float32)),\n",
      "             ('of', DeviceArray(-4.225206, dtype=float32)),\n",
      "             ('philadelphia', DeviceArray(-1.0777594, dtype=float32)),\n",
      "             (\"'\", DeviceArray(-0.4918676, dtype=float32)),\n",
      "             ('s', DeviceArray(-5.598378, dtype=float32)),\n",
      "             ('naval', DeviceArray(-2.6458273, dtype=float32)),\n",
      "             ('defenses', DeviceArray(-5.5111003, dtype=float32)),\n",
      "             ('[MASK]', DeviceArray(177.5767, dtype=float32)),\n",
      "             ('fort', DeviceArray(-2.4840374, dtype=float32)),\n",
      "             ('on', DeviceArray(-1.5030341, dtype=float32)),\n",
      "             ('the', DeviceArray(-0.14263056, dtype=float32)),\n",
      "             ('island', DeviceArray(-0.38235858, dtype=float32)),\n",
      "             (',', DeviceArray(-2.1643183, dtype=float32)),\n",
      "             ('but', DeviceArray(-4.2975407, dtype=float32)),\n",
      "             ('it', DeviceArray(-2.5301564, dtype=float32)),\n",
      "             ('is', DeviceArray(-7.228382, dtype=float32)),\n",
      "             ('unidentified', DeviceArray(-1.0317044, dtype=float32)),\n",
      "             ('.', DeviceArray(-7.6729307, dtype=float32)),\n",
      "             ('[SEP]', DeviceArray(-85.707695, dtype=float32))])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pprint import PrettyPrinter\n",
    "p = PrettyPrinter()\n",
    "for layer in input_to_bert_attn_nodes:\n",
    "    counts, grads = summarize_max_grad_parents_bert_attn_input(layer, tokens)\n",
    "    p.pprint(counts)\n",
    "    p.pprint(grads)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does the gradient go more through the LM prediction head or the bert encoder?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embs_nodes = find_matching_nodes(qoi, \"emb weights (30522\")\n",
    "assert len(word_embs_nodes) == 1\n",
    "word_embs_node = word_embs_nodes[0]\n",
    "word_embs_per_token_node = word_embs_node[input_ids]\n",
    "word_embs_per_token_node.max_grad_of_output_wrt_node = (word_embs_node.max_grad_of_output_wrt_node[0][input_ids], word_embs_node.max_grad_of_output_wrt_node[1][input_ids])\n",
    "word_embs_per_token_node.max_neg_grad_of_output_wrt_node = (word_embs_node.max_neg_grad_of_output_wrt_node[0][input_ids], word_embs_node.max_neg_grad_of_output_wrt_node[1][input_ids])\n",
    "word_embs_per_token_node.parents = word_embs_node.parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 24, 128)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embs_per_token_node.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['transpose', 'get_embedding']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[p.name for p in word_embs_per_token_node.get_parents()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_max_grad_word_embs(word_embs_node: bf.Node, tokens: List[str]):\n",
    "    # Number of hidden units corresponding to each max grad parent option (for the input to bert attention Node)\n",
    "    count_hidden_unit_max_grad_parents: OrderedDict = OrderedDict({\n",
    "        tokens[i]: [\n",
    "            (k.name, v) for k,v in Counter(word_embs_node.get_max_grad_parent()[0, i]).items()\n",
    "        ] for i in range(len(word_embs_node.get_max_grad_parent()[0]))\n",
    "    }) # keys are tokens, values are a list of the counts of # emb units for that token which have each of the possible max grad parents\n",
    "\n",
    "    max_grad_buckets_for_all_words = []\n",
    "    parent_names = [p.name for p in word_embs_node.get_parents()]\n",
    "    for i in range(len(word_embs_node.get_max_grad_parent()[0])): # each word\n",
    "        max_grad_parent_for_emb = word_embs_node.get_max_grad_parent()[0, i] # shape = (emb_sz,)\n",
    "        max_grad_buckets = dict.fromkeys(parent_names, 0.)\n",
    "        for j in range(len(max_grad_parent_for_emb)):\n",
    "            emb_unit_max_grad_val = word_embs_node.max_grad_of_output_wrt_node[0][0][i][j]\n",
    "            emb_unit_max_grad_parent = word_embs_node.max_grad_of_output_wrt_node[1][0][i][j]\n",
    "            max_grad_buckets[emb_unit_max_grad_parent.name] += emb_unit_max_grad_val\n",
    "\n",
    "        max_grad_buckets_for_all_words.append((max_grad_buckets[parent_names[0]], max_grad_buckets[parent_names[1]]))\n",
    "\n",
    "    grad_diff_between_skip_and_attention = OrderedDict({**{\"parent_names\": parent_names}, **{tokens[i]: max_grad_buckets_for_all_words[i][0] - max_grad_buckets_for_all_words[i][1] for i in range(len(max_grad_buckets_for_all_words))}})\n",
    "\n",
    "    return count_hidden_unit_max_grad_parents, grad_diff_between_skip_and_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(OrderedDict([('[CLS]', [('get_embedding', 128)]),\n",
       "              ('a', [('get_embedding', 128)]),\n",
       "              ('1770', [('get_embedding', 128)]),\n",
       "              ('##s', [('get_embedding', 128)]),\n",
       "              ('map', [('get_embedding', 128)]),\n",
       "              ('of', [('get_embedding', 128)]),\n",
       "              ('philadelphia', [('get_embedding', 128)]),\n",
       "              (\"'\", [('get_embedding', 128)]),\n",
       "              ('s', [('get_embedding', 128)]),\n",
       "              ('naval', [('get_embedding', 128)]),\n",
       "              ('defenses', [('get_embedding', 128)]),\n",
       "              ('[MASK]', [('get_embedding', 128)]),\n",
       "              ('fort', [('get_embedding', 128)]),\n",
       "              ('on', [('get_embedding', 128)]),\n",
       "              ('the', [('get_embedding', 128)]),\n",
       "              ('island', [('get_embedding', 128)]),\n",
       "              (',', [('get_embedding', 128)]),\n",
       "              ('but', [('get_embedding', 128)]),\n",
       "              ('it', [('get_embedding', 128)]),\n",
       "              ('is', [('get_embedding', 128)]),\n",
       "              ('unidentified', [('get_embedding', 128)]),\n",
       "              ('.', [('get_embedding', 128)]),\n",
       "              ('[SEP]', [('get_embedding', 128)])]),\n",
       " OrderedDict([('parent_names', ['transpose', 'get_embedding']),\n",
       "              ('[CLS]', DeviceArray(-241.1845, dtype=float32)),\n",
       "              ('a', DeviceArray(-1256.6534, dtype=float32)),\n",
       "              ('1770', DeviceArray(-482.13672, dtype=float32)),\n",
       "              ('##s', DeviceArray(-119.86372, dtype=float32)),\n",
       "              ('map', DeviceArray(-593.2218, dtype=float32)),\n",
       "              ('of', DeviceArray(-229.58571, dtype=float32)),\n",
       "              ('philadelphia', DeviceArray(-259.50714, dtype=float32)),\n",
       "              (\"'\", DeviceArray(-145.83856, dtype=float32)),\n",
       "              ('s', DeviceArray(-180.49391, dtype=float32)),\n",
       "              ('naval', DeviceArray(-911.9706, dtype=float32)),\n",
       "              ('defenses', DeviceArray(-699.164, dtype=float32)),\n",
       "              ('[MASK]', DeviceArray(-4655.573, dtype=float32)),\n",
       "              ('fort', DeviceArray(-977.8213, dtype=float32)),\n",
       "              ('on', DeviceArray(-507.1824, dtype=float32)),\n",
       "              ('the', DeviceArray(-109.89747, dtype=float32)),\n",
       "              ('island', DeviceArray(-187.4435, dtype=float32)),\n",
       "              (',', DeviceArray(-189.19164, dtype=float32)),\n",
       "              ('but', DeviceArray(-222.62868, dtype=float32)),\n",
       "              ('it', DeviceArray(-175.826, dtype=float32)),\n",
       "              ('is', DeviceArray(-229.14064, dtype=float32)),\n",
       "              ('unidentified', DeviceArray(-133.59932, dtype=float32)),\n",
       "              ('.', DeviceArray(-250.39233, dtype=float32)),\n",
       "              ('[SEP]', DeviceArray(-283.62155, dtype=float32))]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize_max_grad_word_embs(word_embs_per_token_node, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: grad entirely goes through the bert encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does gradient travel through key/value/query structures per layer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distinguish the matmul parents\n",
    "for input_to_bert_attn_layer in input_to_bert_attn_nodes:\n",
    "    for parent in input_to_bert_attn_layer.get_parents():\n",
    "        curr_parent = parent\n",
    "        if curr_parent.name == \"matmul\":\n",
    "            while \"bertselfattention\" not in curr_parent.name:\n",
    "                assert len(curr_parent.get_parents()) == 1\n",
    "                curr_parent = curr_parent.get_parents()[0]\n",
    "            parent.name = f\"matmul ({curr_parent.name})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[node(name: matmul (bertselfattention key), val: [[[ 0.6101066  -1.758796   -0.9247101  ... -1.3825     -1.5679675\n",
       "     0.36370578]\n",
       "   [-0.11861765 -2.4003637  -0.5975313  ...  2.406166    0.49865592\n",
       "     0.96619177]\n",
       "   [ 0.15247516  1.4226599   1.0073868  ... -0.13554578  0.38859507\n",
       "     0.127669  ]\n",
       "   ...\n",
       "   [-0.64928347 -0.44245568  0.5023466  ... -0.23630762  1.103121\n",
       "     1.1080121 ]\n",
       "   [-1.3285136  -1.8599983  -0.3786667  ...  0.14449848  0.48642528\n",
       "    -0.21755914]\n",
       "   [-0.4534198  -2.4247804  -1.1860352  ...  0.74313396 -0.37907493\n",
       "     0.11684031]]], grad: [[[0. 0. 0. ... 0. 0. 0.]\n",
       "   [0. 0. 0. ... 0. 0. 0.]\n",
       "   [0. 0. 0. ... 0. 0. 0.]\n",
       "   ...\n",
       "   [0. 0. 0. ... 0. 0. 0.]\n",
       "   [0. 0. 0. ... 0. 0. 0.]\n",
       "   [0. 0. 0. ... 0. 0. 0.]]]),\n",
       " node(name: matmul (bertselfattention value), val: [[[-1.4332561  -1.8980492  -0.8303858  ...  0.08597035 -1.1728458\n",
       "    -1.1266255 ]\n",
       "   [-1.1197389  -0.8480332   0.42633006 ...  0.01381378 -0.81014633\n",
       "    -0.21485434]\n",
       "   [-0.6148334   0.27644622 -1.0346075  ...  0.5669905   0.18998016\n",
       "     0.8989487 ]\n",
       "   ...\n",
       "   [-0.5671722  -0.7026203   0.01299728 ... -0.24938297  0.56574947\n",
       "     0.6735406 ]\n",
       "   [ 0.11092103 -1.3568488  -0.0382809  ...  0.6727732  -0.6667858\n",
       "    -0.14039566]\n",
       "   [-0.30487448 -0.9550067   0.7016412  ...  1.4844625  -0.79268634\n",
       "     0.02493394]]], grad: [[[0. 0. 0. ... 0. 0. 0.]\n",
       "   [0. 0. 0. ... 0. 0. 0.]\n",
       "   [0. 0. 0. ... 0. 0. 0.]\n",
       "   ...\n",
       "   [0. 0. 0. ... 0. 0. 0.]\n",
       "   [0. 0. 0. ... 0. 0. 0.]\n",
       "   [0. 0. 0. ... 0. 0. 0.]]]),\n",
       " node(name: matmul (bertselfattention mixed_query_layer), val: [[[ 0.46789908  0.2780262   1.1105936  ... -0.9250348  -0.67958367\n",
       "    -0.02224174]\n",
       "   [-0.20141947  1.5229245   1.4245518  ...  0.19710077  1.51528\n",
       "     0.7291203 ]\n",
       "   [-0.36503038 -0.5461453   0.60622233 ...  0.7080284   2.194298\n",
       "     0.82709026]\n",
       "   ...\n",
       "   [-1.7116289   0.02722291  0.16597691 ...  1.0109922  -1.06297\n",
       "     0.5822234 ]\n",
       "   [-0.08793674  0.7470818   0.68259144 ...  0.97321224  1.1205964\n",
       "     0.41738394]\n",
       "   [ 0.09249742  0.81556946  1.1164112  ... -0.22062212 -0.06999272\n",
       "     1.3049568 ]]], grad: [[[0. 0. 0. ... 0. 0. 0.]\n",
       "   [0. 0. 0. ... 0. 0. 0.]\n",
       "   [0. 0. 0. ... 0. 0. 0.]\n",
       "   ...\n",
       "   [0. 0. 0. ... 0. 0. 0.]\n",
       "   [0. 0. 0. ... 0. 0. 0.]\n",
       "   [0. 0. 0. ... 0. 0. 0.]]]),\n",
       " node(name: combine self_attention_output and bert attention input 8788907714527, val: [[[ 9.2256129e-01 -4.4765824e-01  7.9845023e-01 ...  8.4029377e-01\n",
       "    -1.0461329e+00  6.0026798e+00]\n",
       "   [ 1.3694379e+00 -1.8483167e+00 -1.0970235e-02 ... -1.4656394e+00\n",
       "     1.4009339e-01  1.4570549e+00]\n",
       "   [-1.2092878e+00 -6.0745960e-01 -7.8931010e-01 ...  5.9910044e-03\n",
       "    -1.4854363e+00  4.6192244e-01]\n",
       "   ...\n",
       "   [-7.2907972e-01  5.5454975e-01  9.0197849e-01 ... -2.1358788e-02\n",
       "     3.2766065e-01  6.9995075e-01]\n",
       "   [-2.8115422e-01 -9.5105541e-01  3.7652713e-01 ...  1.3209920e+00\n",
       "     9.7809309e-01  2.8640740e+00]\n",
       "   [ 8.6872309e-01 -1.5370054e+00  1.4097973e+00 ...  1.5847508e+00\n",
       "     6.2750113e-01  3.5552385e+00]]], grad: [[[0. 0. 0. ... 0. 0. 0.]\n",
       "   [0. 0. 0. ... 0. 0. 0.]\n",
       "   [0. 0. 0. ... 0. 0. 0.]\n",
       "   ...\n",
       "   [0. 0. 0. ... 0. 0. 0.]\n",
       "   [0. 0. 0. ... 0. 0. 0.]\n",
       "   [0. 0. 0. ... 0. 0. 0.]]])]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_to_bert_attn_nodes[0].get_parents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_max_grad_kvq(input_to_bert_attn_node: bf.Node, tokens: List[str]):\n",
    "    # Number of hidden units corresponding to each max grad parent option (for the input to bert attention Node)\n",
    "    count_hidden_unit_max_grad_parents: OrderedDict = OrderedDict({\n",
    "        tokens[i]: [\n",
    "            (k.name, v) for k,v in Counter(input_to_bert_attn_node.get_max_grad_parent()[0, i]).items()\n",
    "        ] for i in range(len(input_to_bert_attn_node.get_max_grad_parent()[0]))\n",
    "    }) # keys are tokens, values are a list of the counts of # emb units for that token which have each of the possible max grad parents\n",
    "\n",
    "    max_grad_buckets_for_all_words = []\n",
    "    parent_names = [p.name for p in input_to_bert_attn_node.get_parents()]\n",
    "    for i in range(len(input_to_bert_attn_node.get_max_grad_parent()[0])): # each word\n",
    "        max_grad_parent_for_emb = input_to_bert_attn_node.get_max_grad_parent()[0, i] # shape = (emb_sz,)\n",
    "        max_grad_buckets = dict.fromkeys(parent_names, 0.)\n",
    "        for j in range(len(max_grad_parent_for_emb)):\n",
    "            emb_unit_max_grad_val = input_to_bert_attn_node.max_grad_of_output_wrt_node[0][0][i][j]\n",
    "            emb_unit_max_grad_parent = input_to_bert_attn_node.max_grad_of_output_wrt_node[1][0][i][j]\n",
    "            max_grad_buckets[emb_unit_max_grad_parent.name] += emb_unit_max_grad_val\n",
    "\n",
    "        max_grad_buckets_for_all_words.append(max_grad_buckets)\n",
    "\n",
    "    grad_diff_between_skip_and_attention = OrderedDict({tokens[i]: max_grad_buckets_for_all_words[i] for i in range(len(max_grad_buckets_for_all_words))})\n",
    "\n",
    "    return count_hidden_unit_max_grad_parents, grad_diff_between_skip_and_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(OrderedDict([('[CLS]',\n",
       "               [('combine self_attention_output and bert attention input 8788907682070',\n",
       "                 128)]),\n",
       "              ('a',\n",
       "               [('combine self_attention_output and bert attention input 8788907682070',\n",
       "                 80),\n",
       "                ('matmul (bertselfattention key)', 25),\n",
       "                ('matmul (bertselfattention value)', 23)]),\n",
       "              ('1770',\n",
       "               [('matmul (bertselfattention key)', 101),\n",
       "                ('combine self_attention_output and bert attention input 8788907682070',\n",
       "                 25),\n",
       "                ('matmul (bertselfattention value)', 2)]),\n",
       "              ('##s',\n",
       "               [('matmul (bertselfattention value)', 23),\n",
       "                ('combine self_attention_output and bert attention input 8788907682070',\n",
       "                 101),\n",
       "                ('matmul (bertselfattention key)', 3),\n",
       "                ('matmul (bertselfattention mixed_query_layer)', 1)]),\n",
       "              ('map',\n",
       "               [('combine self_attention_output and bert attention input 8788907682070',\n",
       "                 123),\n",
       "                ('matmul (bertselfattention mixed_query_layer)', 5)]),\n",
       "              ('of',\n",
       "               [('combine self_attention_output and bert attention input 8788907682070',\n",
       "                 98),\n",
       "                ('matmul (bertselfattention value)', 6),\n",
       "                ('matmul (bertselfattention key)', 20),\n",
       "                ('matmul (bertselfattention mixed_query_layer)', 4)]),\n",
       "              ('philadelphia',\n",
       "               [('matmul (bertselfattention value)', 27),\n",
       "                ('matmul (bertselfattention key)', 98),\n",
       "                ('combine self_attention_output and bert attention input 8788907682070',\n",
       "                 3)]),\n",
       "              (\"'\",\n",
       "               [('matmul (bertselfattention value)', 31),\n",
       "                ('matmul (bertselfattention key)', 79),\n",
       "                ('combine self_attention_output and bert attention input 8788907682070',\n",
       "                 18)]),\n",
       "              ('s',\n",
       "               [('combine self_attention_output and bert attention input 8788907682070',\n",
       "                 128)]),\n",
       "              ('naval',\n",
       "               [('combine self_attention_output and bert attention input 8788907682070',\n",
       "                 112),\n",
       "                ('matmul (bertselfattention value)', 14),\n",
       "                ('matmul (bertselfattention key)', 2)]),\n",
       "              ('defenses',\n",
       "               [('combine self_attention_output and bert attention input 8788907682070',\n",
       "                 127),\n",
       "                ('matmul (bertselfattention value)', 1)]),\n",
       "              ('[MASK]',\n",
       "               [('combine self_attention_output and bert attention input 8788907682070',\n",
       "                 128)]),\n",
       "              ('fort',\n",
       "               [('matmul (bertselfattention value)', 20),\n",
       "                ('matmul (bertselfattention key)', 91),\n",
       "                ('combine self_attention_output and bert attention input 8788907682070',\n",
       "                 17)]),\n",
       "              ('on',\n",
       "               [('matmul (bertselfattention value)', 71),\n",
       "                ('combine self_attention_output and bert attention input 8788907682070',\n",
       "                 7),\n",
       "                ('matmul (bertselfattention key)', 48),\n",
       "                ('matmul (bertselfattention mixed_query_layer)', 2)]),\n",
       "              ('the',\n",
       "               [('matmul (bertselfattention value)', 41),\n",
       "                ('matmul (bertselfattention key)', 84),\n",
       "                ('combine self_attention_output and bert attention input 8788907682070',\n",
       "                 2),\n",
       "                ('matmul (bertselfattention mixed_query_layer)', 1)]),\n",
       "              ('island',\n",
       "               [('matmul (bertselfattention value)', 59),\n",
       "                ('combine self_attention_output and bert attention input 8788907682070',\n",
       "                 35),\n",
       "                ('matmul (bertselfattention key)', 34)]),\n",
       "              (',',\n",
       "               [('matmul (bertselfattention value)', 72),\n",
       "                ('combine self_attention_output and bert attention input 8788907682070',\n",
       "                 22),\n",
       "                ('matmul (bertselfattention key)', 28),\n",
       "                ('matmul (bertselfattention mixed_query_layer)', 6)]),\n",
       "              ('but',\n",
       "               [('matmul (bertselfattention value)', 39),\n",
       "                ('combine self_attention_output and bert attention input 8788907682070',\n",
       "                 57),\n",
       "                ('matmul (bertselfattention key)', 22),\n",
       "                ('matmul (bertselfattention mixed_query_layer)', 10)]),\n",
       "              ('it',\n",
       "               [('matmul (bertselfattention mixed_query_layer)', 40),\n",
       "                ('matmul (bertselfattention key)', 14),\n",
       "                ('combine self_attention_output and bert attention input 8788907682070',\n",
       "                 73),\n",
       "                ('matmul (bertselfattention value)', 1)]),\n",
       "              ('is',\n",
       "               [('combine self_attention_output and bert attention input 8788907682070',\n",
       "                 122),\n",
       "                ('matmul (bertselfattention mixed_query_layer)', 4),\n",
       "                ('matmul (bertselfattention key)', 2)]),\n",
       "              ('unidentified',\n",
       "               [('matmul (bertselfattention value)', 41),\n",
       "                ('matmul (bertselfattention key)', 37),\n",
       "                ('combine self_attention_output and bert attention input 8788907682070',\n",
       "                 50)]),\n",
       "              ('.',\n",
       "               [('combine self_attention_output and bert attention input 8788907682070',\n",
       "                 76),\n",
       "                ('matmul (bertselfattention key)', 42),\n",
       "                ('matmul (bertselfattention mixed_query_layer)', 6),\n",
       "                ('matmul (bertselfattention value)', 4)]),\n",
       "              ('[SEP]',\n",
       "               [('combine self_attention_output and bert attention input 8788907682070',\n",
       "                 83),\n",
       "                ('matmul (bertselfattention key)', 40),\n",
       "                ('matmul (bertselfattention mixed_query_layer)', 5)])]),\n",
       " OrderedDict([('[CLS]',\n",
       "               {'matmul (bertselfattention mixed_query_layer)': 0.0,\n",
       "                'matmul (bertselfattention value)': 0.0,\n",
       "                'matmul (bertselfattention key)': 0.0,\n",
       "                'combine self_attention_output and bert attention input 8788907682070': DeviceArray(36.807777, dtype=float32)}),\n",
       "              ('a',\n",
       "               {'matmul (bertselfattention mixed_query_layer)': 0.0,\n",
       "                'matmul (bertselfattention value)': DeviceArray(9.597576, dtype=float32),\n",
       "                'matmul (bertselfattention key)': DeviceArray(9.353506, dtype=float32),\n",
       "                'combine self_attention_output and bert attention input 8788907682070': DeviceArray(22.69731, dtype=float32)}),\n",
       "              ('1770',\n",
       "               {'matmul (bertselfattention mixed_query_layer)': 0.0,\n",
       "                'matmul (bertselfattention value)': DeviceArray(0.08174217, dtype=float32),\n",
       "                'matmul (bertselfattention key)': DeviceArray(8.826442, dtype=float32),\n",
       "                'combine self_attention_output and bert attention input 8788907682070': DeviceArray(0.7486969, dtype=float32)}),\n",
       "              ('##s',\n",
       "               {'matmul (bertselfattention mixed_query_layer)': DeviceArray(0.0169543, dtype=float32),\n",
       "                'matmul (bertselfattention value)': DeviceArray(0.5906631, dtype=float32),\n",
       "                'matmul (bertselfattention key)': DeviceArray(0.05853069, dtype=float32),\n",
       "                'combine self_attention_output and bert attention input 8788907682070': DeviceArray(1.9017619, dtype=float32)}),\n",
       "              ('map',\n",
       "               {'matmul (bertselfattention mixed_query_layer)': DeviceArray(1.6517375, dtype=float32),\n",
       "                'matmul (bertselfattention value)': 0.0,\n",
       "                'matmul (bertselfattention key)': 0.0,\n",
       "                'combine self_attention_output and bert attention input 8788907682070': DeviceArray(35.110012, dtype=float32)}),\n",
       "              ('of',\n",
       "               {'matmul (bertselfattention mixed_query_layer)': DeviceArray(0.31144005, dtype=float32),\n",
       "                'matmul (bertselfattention value)': DeviceArray(0.44666228, dtype=float32),\n",
       "                'matmul (bertselfattention key)': DeviceArray(1.8550743, dtype=float32),\n",
       "                'combine self_attention_output and bert attention input 8788907682070': DeviceArray(6.8613586, dtype=float32)}),\n",
       "              ('philadelphia',\n",
       "               {'matmul (bertselfattention mixed_query_layer)': 0.0,\n",
       "                'matmul (bertselfattention value)': DeviceArray(0.7146638, dtype=float32),\n",
       "                'matmul (bertselfattention key)': DeviceArray(6.273955, dtype=float32),\n",
       "                'combine self_attention_output and bert attention input 8788907682070': DeviceArray(0.0348809, dtype=float32)}),\n",
       "              (\"'\",\n",
       "               {'matmul (bertselfattention mixed_query_layer)': 0.0,\n",
       "                'matmul (bertselfattention value)': DeviceArray(0.65782636, dtype=float32),\n",
       "                'matmul (bertselfattention key)': DeviceArray(2.842026, dtype=float32),\n",
       "                'combine self_attention_output and bert attention input 8788907682070': DeviceArray(0.20870933, dtype=float32)}),\n",
       "              ('s',\n",
       "               {'matmul (bertselfattention mixed_query_layer)': 0.0,\n",
       "                'matmul (bertselfattention value)': 0.0,\n",
       "                'matmul (bertselfattention key)': 0.0,\n",
       "                'combine self_attention_output and bert attention input 8788907682070': DeviceArray(8.027609, dtype=float32)}),\n",
       "              ('naval',\n",
       "               {'matmul (bertselfattention mixed_query_layer)': 0.0,\n",
       "                'matmul (bertselfattention value)': DeviceArray(0.6489055, dtype=float32),\n",
       "                'matmul (bertselfattention key)': DeviceArray(0.10115233, dtype=float32),\n",
       "                'combine self_attention_output and bert attention input 8788907682070': DeviceArray(4.666871, dtype=float32)}),\n",
       "              ('defenses',\n",
       "               {'matmul (bertselfattention mixed_query_layer)': 0.0,\n",
       "                'matmul (bertselfattention value)': DeviceArray(0.21084899, dtype=float32),\n",
       "                'matmul (bertselfattention key)': 0.0,\n",
       "                'combine self_attention_output and bert attention input 8788907682070': DeviceArray(28.900934, dtype=float32)}),\n",
       "              ('[MASK]',\n",
       "               {'matmul (bertselfattention mixed_query_layer)': 0.0,\n",
       "                'matmul (bertselfattention value)': 0.0,\n",
       "                'matmul (bertselfattention key)': 0.0,\n",
       "                'combine self_attention_output and bert attention input 8788907682070': DeviceArray(299.45526, dtype=float32)}),\n",
       "              ('fort',\n",
       "               {'matmul (bertselfattention mixed_query_layer)': 0.0,\n",
       "                'matmul (bertselfattention value)': DeviceArray(4.841671, dtype=float32),\n",
       "                'matmul (bertselfattention key)': DeviceArray(31.156456, dtype=float32),\n",
       "                'combine self_attention_output and bert attention input 8788907682070': DeviceArray(2.1244879, dtype=float32)}),\n",
       "              ('on',\n",
       "               {'matmul (bertselfattention mixed_query_layer)': DeviceArray(0.08068078, dtype=float32),\n",
       "                'matmul (bertselfattention value)': DeviceArray(10.43748, dtype=float32),\n",
       "                'matmul (bertselfattention key)': DeviceArray(5.8354735, dtype=float32),\n",
       "                'combine self_attention_output and bert attention input 8788907682070': DeviceArray(0.22137395, dtype=float32)}),\n",
       "              ('the',\n",
       "               {'matmul (bertselfattention mixed_query_layer)': DeviceArray(0.01157443, dtype=float32),\n",
       "                'matmul (bertselfattention value)': DeviceArray(0.8205565, dtype=float32),\n",
       "                'matmul (bertselfattention key)': DeviceArray(2.3159876, dtype=float32),\n",
       "                'combine self_attention_output and bert attention input 8788907682070': DeviceArray(0.01473826, dtype=float32)}),\n",
       "              ('island',\n",
       "               {'matmul (bertselfattention mixed_query_layer)': 0.0,\n",
       "                'matmul (bertselfattention value)': DeviceArray(1.847619, dtype=float32),\n",
       "                'matmul (bertselfattention key)': DeviceArray(0.9048146, dtype=float32),\n",
       "                'combine self_attention_output and bert attention input 8788907682070': DeviceArray(0.5703639, dtype=float32)}),\n",
       "              (',',\n",
       "               {'matmul (bertselfattention mixed_query_layer)': DeviceArray(0.24482009, dtype=float32),\n",
       "                'matmul (bertselfattention value)': DeviceArray(4.38648, dtype=float32),\n",
       "                'matmul (bertselfattention key)': DeviceArray(1.2835188, dtype=float32),\n",
       "                'combine self_attention_output and bert attention input 8788907682070': DeviceArray(0.61377054, dtype=float32)}),\n",
       "              ('but',\n",
       "               {'matmul (bertselfattention mixed_query_layer)': DeviceArray(0.65095216, dtype=float32),\n",
       "                'matmul (bertselfattention value)': DeviceArray(3.0280871, dtype=float32),\n",
       "                'matmul (bertselfattention key)': DeviceArray(1.5118197, dtype=float32),\n",
       "                'combine self_attention_output and bert attention input 8788907682070': DeviceArray(2.866751, dtype=float32)}),\n",
       "              ('it',\n",
       "               {'matmul (bertselfattention mixed_query_layer)': DeviceArray(1.8277957, dtype=float32),\n",
       "                'matmul (bertselfattention value)': DeviceArray(0.03217724, dtype=float32),\n",
       "                'matmul (bertselfattention key)': DeviceArray(0.5264892, dtype=float32),\n",
       "                'combine self_attention_output and bert attention input 8788907682070': DeviceArray(2.3149848, dtype=float32)}),\n",
       "              ('is',\n",
       "               {'matmul (bertselfattention mixed_query_layer)': DeviceArray(0.30340746, dtype=float32),\n",
       "                'matmul (bertselfattention value)': 0.0,\n",
       "                'matmul (bertselfattention key)': DeviceArray(0.15265507, dtype=float32),\n",
       "                'combine self_attention_output and bert attention input 8788907682070': DeviceArray(8.833901, dtype=float32)}),\n",
       "              ('unidentified',\n",
       "               {'matmul (bertselfattention mixed_query_layer)': 0.0,\n",
       "                'matmul (bertselfattention value)': DeviceArray(1.3101138, dtype=float32),\n",
       "                'matmul (bertselfattention key)': DeviceArray(1.0388291, dtype=float32),\n",
       "                'combine self_attention_output and bert attention input 8788907682070': DeviceArray(0.93117607, dtype=float32)}),\n",
       "              ('.',\n",
       "               {'matmul (bertselfattention mixed_query_layer)': DeviceArray(0.49465382, dtype=float32),\n",
       "                'matmul (bertselfattention value)': DeviceArray(0.34710154, dtype=float32),\n",
       "                'matmul (bertselfattention key)': DeviceArray(4.371755, dtype=float32),\n",
       "                'combine self_attention_output and bert attention input 8788907682070': DeviceArray(5.544224, dtype=float32)}),\n",
       "              ('[SEP]',\n",
       "               {'matmul (bertselfattention mixed_query_layer)': DeviceArray(0.6772306, dtype=float32),\n",
       "                'matmul (bertselfattention value)': 0.0,\n",
       "                'matmul (bertselfattention key)': DeviceArray(7.5418787, dtype=float32),\n",
       "                'combine self_attention_output and bert attention input 8788907682070': DeviceArray(11.501883, dtype=float32)})]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize_max_grad_kvq(input_to_bert_attn_nodes[-3], tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'[CLS]': [('matmul', 53),\n",
       "   ('matmul', 74),\n",
       "   ('combine self_attention_output and bert attention input 8758245107119',\n",
       "    1)],\n",
       "  'a': [('combine self_attention_output and bert attention input 8758245107119',\n",
       "    128)],\n",
       "  '1770': [('matmul', 31), ('matmul', 97)],\n",
       "  '##s': [('matmul', 31), ('matmul', 97)],\n",
       "  'map': [('matmul', 68), ('matmul', 60)],\n",
       "  'of': [('matmul', 29),\n",
       "   ('matmul', 87),\n",
       "   ('combine self_attention_output and bert attention input 8758245107119',\n",
       "    12)],\n",
       "  'philadelphia': [('matmul', 85), ('matmul', 43)],\n",
       "  \"'\": [('matmul', 25),\n",
       "   ('matmul', 84),\n",
       "   ('combine self_attention_output and bert attention input 8758245107119',\n",
       "    19)],\n",
       "  's': [('matmul', 55),\n",
       "   ('matmul', 56),\n",
       "   ('combine self_attention_output and bert attention input 8758245107119',\n",
       "    17)],\n",
       "  'naval': [('combine self_attention_output and bert attention input 8758245107119',\n",
       "    104),\n",
       "   ('matmul', 22),\n",
       "   ('matmul', 2)],\n",
       "  'defenses': [('combine self_attention_output and bert attention input 8758245107119',\n",
       "    120),\n",
       "   ('matmul', 5),\n",
       "   ('matmul', 3)],\n",
       "  '[MASK]': [('combine self_attention_output and bert attention input 8758245107119',\n",
       "    128)],\n",
       "  'fort': [('combine self_attention_output and bert attention input 8758245107119',\n",
       "    127),\n",
       "   ('matmul', 1)],\n",
       "  'on': [('matmul', 24),\n",
       "   ('matmul', 86),\n",
       "   ('combine self_attention_output and bert attention input 8758245107119',\n",
       "    18)],\n",
       "  'the': [('matmul', 26),\n",
       "   ('matmul', 95),\n",
       "   ('combine self_attention_output and bert attention input 8758245107119',\n",
       "    7)],\n",
       "  'island': [('matmul', 27), ('matmul', 101)],\n",
       "  ',': [('matmul', 55), ('matmul', 73)],\n",
       "  'but': [('matmul', 34),\n",
       "   ('matmul', 93),\n",
       "   ('combine self_attention_output and bert attention input 8758245107119',\n",
       "    1)],\n",
       "  'it': [('matmul', 39),\n",
       "   ('matmul', 70),\n",
       "   ('combine self_attention_output and bert attention input 8758245107119',\n",
       "    19)],\n",
       "  'is': [('matmul', 49),\n",
       "   ('matmul', 76),\n",
       "   ('combine self_attention_output and bert attention input 8758245107119',\n",
       "    3)],\n",
       "  'unidentified': [('matmul', 28), ('matmul', 100)],\n",
       "  '.': [('matmul', 50), ('matmul', 78)],\n",
       "  '[SEP]': [('matmul', 42), ('matmul', 86)]},\n",
       " {'[CLS]': DeviceArray(-28.904734, dtype=float32),\n",
       "  'a': DeviceArray(79.73425, dtype=float32),\n",
       "  '1770': DeviceArray(-7.1745143, dtype=float32),\n",
       "  '##s': DeviceArray(-2.7482662, dtype=float32),\n",
       "  'map': DeviceArray(-17.659466, dtype=float32),\n",
       "  'of': DeviceArray(-4.1306715, dtype=float32),\n",
       "  'philadelphia': DeviceArray(-19.473045, dtype=float32),\n",
       "  \"'\": DeviceArray(-6.3064027, dtype=float32),\n",
       "  's': DeviceArray(-6.633123, dtype=float32),\n",
       "  'naval': DeviceArray(12.5280485, dtype=float32),\n",
       "  'defenses': DeviceArray(66.57097, dtype=float32),\n",
       "  '[MASK]': DeviceArray(289.098, dtype=float32),\n",
       "  'fort': DeviceArray(26.292835, dtype=float32),\n",
       "  'on': DeviceArray(-5.327896, dtype=float32),\n",
       "  'the': DeviceArray(-3.2386632, dtype=float32),\n",
       "  'island': DeviceArray(-9.03645, dtype=float32),\n",
       "  ',': DeviceArray(-3.7696393, dtype=float32),\n",
       "  'but': DeviceArray(-5.7957606, dtype=float32),\n",
       "  'it': DeviceArray(-2.9873562, dtype=float32),\n",
       "  'is': DeviceArray(-7.618865, dtype=float32),\n",
       "  'unidentified': DeviceArray(-6.953268, dtype=float32),\n",
       "  '.': DeviceArray(-4.607679, dtype=float32),\n",
       "  '[SEP]': DeviceArray(-6.0209675, dtype=float32)})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summarize_max_grad_parents_bert_attn_input(layer0_bert_attn_input, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'[CLS]': [('matmul', 79), ('matmul', 49)],\n",
       "  'a': [('matmul', 96), ('matmul', 32)],\n",
       "  '1770': [('matmul', 77), ('matmul', 51)],\n",
       "  '##s': [('matmul', 53), ('matmul', 75)],\n",
       "  'map': [('matmul', 49), ('matmul', 79)],\n",
       "  'of': [('matmul', 89), ('matmul', 39)],\n",
       "  'philadelphia': [('matmul', 76), ('matmul', 52)],\n",
       "  \"'\": [('matmul', 98), ('matmul', 30)],\n",
       "  's': [('matmul', 78), ('matmul', 50)],\n",
       "  'naval': [('matmul', 33), ('matmul', 95)],\n",
       "  'defenses': [('matmul', 93), ('matmul', 35)],\n",
       "  '[MASK]': [('matmul', 71),\n",
       "   ('combine self_attention_output and bert attention input 8758245089735',\n",
       "    57)],\n",
       "  'fort': [('matmul', 73), ('matmul', 55)],\n",
       "  'on': [('matmul', 72), ('matmul', 56)],\n",
       "  'the': [('matmul', 94), ('matmul', 34)],\n",
       "  'island': [('matmul', 77), ('matmul', 51)],\n",
       "  ',': [('matmul', 72), ('matmul', 56)],\n",
       "  'but': [('matmul', 77), ('matmul', 51)],\n",
       "  'it': [('matmul', 92), ('matmul', 36)],\n",
       "  'is': [('matmul', 72), ('matmul', 56)],\n",
       "  'unidentified': [('matmul', 79), ('matmul', 49)],\n",
       "  '.': [('matmul', 86), ('matmul', 42)],\n",
       "  '[SEP]': [('matmul', 81), ('matmul', 47)]},\n",
       " {'[CLS]': DeviceArray(-6.0112686, dtype=float32),\n",
       "  'a': DeviceArray(-85.850555, dtype=float32),\n",
       "  '1770': DeviceArray(-0.00479437, dtype=float32),\n",
       "  '##s': DeviceArray(-0.03525598, dtype=float32),\n",
       "  'map': DeviceArray(-0.31672207, dtype=float32),\n",
       "  'of': DeviceArray(-1.340429, dtype=float32),\n",
       "  'philadelphia': DeviceArray(-0.7382939, dtype=float32),\n",
       "  \"'\": DeviceArray(-3.0486362, dtype=float32),\n",
       "  's': DeviceArray(-2.8302042, dtype=float32),\n",
       "  'naval': DeviceArray(-19.589146, dtype=float32),\n",
       "  'defenses': DeviceArray(-73.57641, dtype=float32),\n",
       "  '[MASK]': DeviceArray(-105.016365, dtype=float32),\n",
       "  'fort': DeviceArray(-25.733095, dtype=float32),\n",
       "  'on': DeviceArray(-2.0193076, dtype=float32),\n",
       "  'the': DeviceArray(-0.696461, dtype=float32),\n",
       "  'island': DeviceArray(-0.23464568, dtype=float32),\n",
       "  ',': DeviceArray(-0.24671903, dtype=float32),\n",
       "  'but': DeviceArray(-0.39490497, dtype=float32),\n",
       "  'it': DeviceArray(-1.329987, dtype=float32),\n",
       "  'is': DeviceArray(-0.79144835, dtype=float32),\n",
       "  'unidentified': DeviceArray(-0.06926788, dtype=float32),\n",
       "  '.': DeviceArray(-0.0180961, dtype=float32),\n",
       "  '[SEP]': DeviceArray(-0.01881858, dtype=float32)})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summarize_max_grad_parents_bert_attn_input(layer1_bert_attn_input, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[CLS]': [('matmul', 53),\n",
       "  ('matmul', 74),\n",
       "  ('combine self_attention_output and bert attention input 8758245107119', 1)],\n",
       " 'a': [('combine self_attention_output and bert attention input 8758245107119',\n",
       "   128)],\n",
       " '1770': [('matmul', 31), ('matmul', 97)],\n",
       " '##s': [('matmul', 31), ('matmul', 97)],\n",
       " 'map': [('matmul', 68), ('matmul', 60)],\n",
       " 'of': [('matmul', 29),\n",
       "  ('matmul', 87),\n",
       "  ('combine self_attention_output and bert attention input 8758245107119',\n",
       "   12)],\n",
       " 'philadelphia': [('matmul', 85), ('matmul', 43)],\n",
       " \"'\": [('matmul', 25),\n",
       "  ('matmul', 84),\n",
       "  ('combine self_attention_output and bert attention input 8758245107119',\n",
       "   19)],\n",
       " 's': [('matmul', 55),\n",
       "  ('matmul', 56),\n",
       "  ('combine self_attention_output and bert attention input 8758245107119',\n",
       "   17)],\n",
       " 'naval': [('combine self_attention_output and bert attention input 8758245107119',\n",
       "   104),\n",
       "  ('matmul', 22),\n",
       "  ('matmul', 2)],\n",
       " 'defenses': [('combine self_attention_output and bert attention input 8758245107119',\n",
       "   120),\n",
       "  ('matmul', 5),\n",
       "  ('matmul', 3)],\n",
       " '[MASK]': [('combine self_attention_output and bert attention input 8758245107119',\n",
       "   128)],\n",
       " 'fort': [('combine self_attention_output and bert attention input 8758245107119',\n",
       "   127),\n",
       "  ('matmul', 1)],\n",
       " 'on': [('matmul', 24),\n",
       "  ('matmul', 86),\n",
       "  ('combine self_attention_output and bert attention input 8758245107119',\n",
       "   18)],\n",
       " 'the': [('matmul', 26),\n",
       "  ('matmul', 95),\n",
       "  ('combine self_attention_output and bert attention input 8758245107119', 7)],\n",
       " 'island': [('matmul', 27), ('matmul', 101)],\n",
       " ',': [('matmul', 55), ('matmul', 73)],\n",
       " 'but': [('matmul', 34),\n",
       "  ('matmul', 93),\n",
       "  ('combine self_attention_output and bert attention input 8758245107119', 1)],\n",
       " 'it': [('matmul', 39),\n",
       "  ('matmul', 70),\n",
       "  ('combine self_attention_output and bert attention input 8758245107119',\n",
       "   19)],\n",
       " 'is': [('matmul', 49),\n",
       "  ('matmul', 76),\n",
       "  ('combine self_attention_output and bert attention input 8758245107119', 3)],\n",
       " 'unidentified': [('matmul', 28), ('matmul', 100)],\n",
       " '.': [('matmul', 50), ('matmul', 78)],\n",
       " '[SEP]': [('matmul', 42), ('matmul', 86)]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of hidden units corresponding to each max grad parent option (for the input to bert attention Node)\n",
    "{\n",
    "    tokens[i]: [\n",
    "        (k.name, v) for k,v in Counter(layer0_bert_attn_input.get_max_grad_parent()[0, i]).items()\n",
    "    ] for i in range(len(layer0_bert_attn_input.get_max_grad_parent()[0]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[CLS]': DeviceArray(-28.904734, dtype=float32),\n",
       " 'a': DeviceArray(79.73425, dtype=float32),\n",
       " '1770': DeviceArray(-7.1745143, dtype=float32),\n",
       " '##s': DeviceArray(-2.7482662, dtype=float32),\n",
       " 'map': DeviceArray(-17.659466, dtype=float32),\n",
       " 'of': DeviceArray(-4.1306715, dtype=float32),\n",
       " 'philadelphia': DeviceArray(-19.473045, dtype=float32),\n",
       " \"'\": DeviceArray(-6.3064027, dtype=float32),\n",
       " 's': DeviceArray(-6.633123, dtype=float32),\n",
       " 'naval': DeviceArray(12.5280485, dtype=float32),\n",
       " 'defenses': DeviceArray(66.57097, dtype=float32),\n",
       " '[MASK]': DeviceArray(289.098, dtype=float32),\n",
       " 'fort': DeviceArray(26.292835, dtype=float32),\n",
       " 'on': DeviceArray(-5.327896, dtype=float32),\n",
       " 'the': DeviceArray(-3.2386632, dtype=float32),\n",
       " 'island': DeviceArray(-9.03645, dtype=float32),\n",
       " ',': DeviceArray(-3.7696393, dtype=float32),\n",
       " 'but': DeviceArray(-5.7957606, dtype=float32),\n",
       " 'it': DeviceArray(-2.9873562, dtype=float32),\n",
       " 'is': DeviceArray(-7.618865, dtype=float32),\n",
       " 'unidentified': DeviceArray(-6.953268, dtype=float32),\n",
       " '.': DeviceArray(-4.607679, dtype=float32),\n",
       " '[SEP]': DeviceArray(-6.0209675, dtype=float32)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of hidden units corresponding to each max grad parent option (for the input to bert attention Node)\n",
    "skip_and_attn_max_grads_per_word = []\n",
    "for i in range(len(layer0_bert_attn_input.get_max_grad_parent()[0])): # each word\n",
    "    max_grad_parent_for_emb = layer0_bert_attn_input.get_max_grad_parent()[0, i] # shape = (emb_sz,)\n",
    "    skip_max_grad = 0\n",
    "    attn_max_grad = 0\n",
    "    for j in range(len(max_grad_parent_for_emb)):\n",
    "        emb_unit_max_grad_val = layer0_bert_attn_input.max_grad_of_output_wrt_node[0][0][i][j]\n",
    "        emb_unit_max_grad_parent = layer0_bert_attn_input.max_grad_of_output_wrt_node[1][0][i][j]\n",
    "        if emb_unit_max_grad_parent.name == \"matmul\":\n",
    "            attn_max_grad += emb_unit_max_grad_val\n",
    "        elif \"combine self_attention_output and bert attention input\" in emb_unit_max_grad_parent.name:\n",
    "            skip_max_grad += emb_unit_max_grad_val\n",
    "        else:\n",
    "            raise ValueError(f\"uhoh! received an unexpected parent, {emb_unit_max_grad_parent}\")\n",
    "    skip_and_attn_max_grads_per_word.append((skip_max_grad, attn_max_grad))\n",
    "\n",
    "{tokens[i]: skip_and_attn_max_grads_per_word[i][0] - skip_and_attn_max_grads_per_word[i][1] for i in range(len(skip_and_attn_max_grads_per_word))}\n",
    "# print({tokens[i]: skip_and_attn_max_grads_per_word[i][0] for i in range(len(skip_and_attn_max_grads_per_word))})\n",
    "\n",
    "# {\n",
    "#     tokenizer.convert_ids_to_tokens(input_ids[i]): [\n",
    "#         (k.name, v) for k,v in Counter(layer0_bert_attn_input.get_max_grad_parent()[0, i]).items()\n",
    "#     ] for i in range(len(layer0_bert_attn_input.get_max_grad_parent()[0]))\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_bf.visualize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('jax-hf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b4cc9cc217af6b7e12b7da5c82d5884fde07a0e0f6b7f76767c2fbf53f076f9a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
