{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2b74a5272730>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jax.config import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "from jax import numpy as jnp\n",
    "import transformers\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForMaskedLM, \n",
    "    BertForMaskedLM, \n",
    "    BertTokenizer, \n",
    "    BertTokenizerFast, \n",
    "    BertEmbeddings,\n",
    "    BfBertEmbeddings,\n",
    "    BertConfig,\n",
    "    BertSelfAttention,\n",
    "    BfBertSelfAttention,\n",
    "    BertSelfOutput,\n",
    "    BfBertSelfOutput,\n",
    "    BertAttention,\n",
    "    BfBertAttention,\n",
    ")\n",
    "from brunoflow.ad.utils import check_node_equals_tensor, check_node_allclose_tensor\n",
    "from utils import check_bf_param_weights_match_torch, check_bf_model_outputs_match_torch_outputs, check_bf_param_grads_allclose_torch\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-24 01:14:05.873009: W external/org_tensorflow/tensorflow/compiler/xla/service/platform_util.cc:193] unable to create StreamExecutor for CUDA:0: failed initializing StreamExecutor for CUDA device ordinal 0: INTERNAL: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_DEVICE_UNAVAILABLE: CUDA-capable device(s) is/are busy or unavailable\n",
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "# Init torch and bf models\n",
    "config = BertConfig.from_pretrained(pretrained_model_name_or_path=\"../../brunoflow/models/bert/config.json\")\n",
    "torch_model = BertAttention(config)\n",
    "bf_model = BfBertAttention(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init inputs to bf and torch models\n",
    "hidden_states_torch = torch.randn(size=(2, 19, 768))\n",
    "attention_mask_torch = torch.randn(size=(2, 1, 1, 19))\n",
    "\n",
    "hidden_states = jnp.array(hidden_states_torch.numpy(), dtype=jnp.float64)\n",
    "attention_mask = jnp.array(attention_mask_torch.numpy(), dtype=jnp.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.8 ms, sys: 1.81 ms, total: 16.7 ms\n",
      "Wall time: 15.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "outputs_torch = torch_model(hidden_states_torch, attention_mask_torch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 743 ms, sys: 19.9 ms, total: 763 ms\n",
      "Wall time: 737 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "outputs_bf = bf_model(hidden_states, attention_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that forward pass for bf works and matches output shape with torch\n",
    "if isinstance(outputs_bf, (list, tuple)):\n",
    "    # Handle case where outputs is a tuple/list and not just a single item\n",
    "    assert len(outputs_bf) == len(outputs_torch)\n",
    "    for i in range(len(outputs_bf)):\n",
    "        out_bf, out_torch = outputs_bf[i], outputs_torch[i] \n",
    "        assert(out_torch.shape == out_bf.shape)\n",
    "\n",
    "else:\n",
    "    assert(outputs_torch.shape == outputs_bf.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save torch BertSelfAttention to file\n",
    "save_path = \"bertattention_torch.pt\"\n",
    "torch.save(torch_model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load state dict for BertSelfAttention into BF and check weights, outputs, and backprop\n",
    "bf_model.load_state_dict(torch.load(save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check weights of BF model and Torch model match exactly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of param weight self.query.bias for bf and torch are equal? True\n",
      "Value of param weight self.query.weight for bf and torch are equal? True\n",
      "Value of param weight self.key.bias for bf and torch are equal? True\n",
      "Value of param weight self.key.weight for bf and torch are equal? True\n",
      "Value of param weight self.value.bias for bf and torch are equal? True\n",
      "Value of param weight self.value.weight for bf and torch are equal? True\n",
      "Value of param weight output.dense.bias for bf and torch are equal? True\n",
      "Value of param weight output.dense.weight for bf and torch are equal? True\n",
      "Value of param weight output.LayerNorm.weight for bf and torch are equal? True\n",
      "Value of param weight output.LayerNorm.bias for bf and torch are equal? True\n"
     ]
    }
   ],
   "source": [
    "# Check weights match\n",
    "check_bf_param_weights_match_torch(bf_model, torch_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check model output after forward pass matches for BF and Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of bf and torch are within 1e-06? True\n"
     ]
    }
   ],
   "source": [
    "# Check output from forward passes match for bf and torch\n",
    "torch_model.train(False)\n",
    "outputs_bf = bf_model(hidden_states=hidden_states, attention_mask=attention_mask)\n",
    "outputs_torch = torch_model(hidden_states=hidden_states_torch, attention_mask=attention_mask_torch)\n",
    "\n",
    "if isinstance(outputs_bf, (list, tuple)):\n",
    "    assert len(outputs_bf) == len(outputs_torch)\n",
    "    for i in range(len(outputs_bf)):\n",
    "        out_bf, out_torch = outputs_bf[i], outputs_torch[i]\n",
    "        check_bf_model_outputs_match_torch_outputs(out_bf, out_torch, atol=1e-6)\n",
    "else:\n",
    "    check_bf_model_outputs_match_torch_outputs(outputs_bf, outputs_torch, atol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check grad after backward pass matches for BF and torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.5 ms, sys: 1.08 ms, total: 15.6 ms\n",
      "Wall time: 14.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Torch backward pass\n",
    "torch_model.train(True)\n",
    "outputs_torch[0].backward(gradient=torch.ones_like(outputs_torch[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.36 s, sys: 16.9 s, total: 25.3 s\n",
      "Wall time: 19.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# BF backward pass\n",
    "outputs_bf[0].backprop(values_to_compute=(\"grad\",))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad of param self.query.bias for bf and torch are within 1e-05? True\n",
      "Grad of param self.query.weight for bf and torch are within 1e-05? True\n",
      "Grad of param self.key.bias for bf and torch are within 1e-05? True\n",
      "Grad of param self.key.weight for bf and torch are within 1e-05? True\n",
      "Grad of param self.value.bias for bf and torch are within 1e-05? True\n",
      "Grad of param self.value.weight for bf and torch are within 1e-05? True\n",
      "Grad of param output.dense.bias for bf and torch are within 1e-05? True\n",
      "Grad of param output.dense.weight for bf and torch are within 1e-05? True\n",
      "Grad of param output.LayerNorm.weight for bf and torch are within 1e-05? True\n",
      "Grad of param output.LayerNorm.bias for bf and torch are within 1e-05? True\n"
     ]
    }
   ],
   "source": [
    "# Run the actual check\n",
    "check_bf_param_grads_allclose_torch(bf_model, torch_model, atol=1e-5, print_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "b4cc9cc217af6b7e12b7da5c82d5884fde07a0e0f6b7f76767c2fbf53f076f9a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
