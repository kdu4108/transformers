{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: XLA_PYTHON_CLIENT_PREALLOCATE=false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/miniconda3/envs/jax-hf/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fbecdfde550>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%env XLA_PYTHON_CLIENT_PREALLOCATE=false\n",
    "import brunoflow as bf\n",
    "from brunoflow.ad.utils import check_node_equals_tensor, check_node_allclose_tensor\n",
    "from jax import numpy as jnp\n",
    "import transformers\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForMaskedLM, \n",
    "    BertForMaskedLM, \n",
    "    BertTokenizer, \n",
    "    BertTokenizerFast, \n",
    "    BertEmbeddings,\n",
    "    BfBertEmbeddings,\n",
    "    BfBertEncoder,\n",
    "    BertConfig,\n",
    "    BfBertSelfAttention,\n",
    "    BfBertForMaskedLM,\n",
    ")\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  7592,  1045,  2215,  2000,  4521,  2070,   103,  6240,  2651,\n",
      "          1012,  2009,  1005,  1055, 15060,   103,  2035,   999,   102],\n",
      "        [  101, 10930, 10930,  2054,  1005,  1055,  2039,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0]])\n"
     ]
    }
   ],
   "source": [
    "# Establish data\n",
    "model_id = \"google/bert_uncased_L-2_H-128_A-2\"\n",
    "config = BertConfig.from_pretrained(pretrained_model_name_or_path=\"../../brunoflow/models/bert/config-toy.json\")\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_id)\n",
    "text = [\"hello I want to eat some [MASK] meat today. It's thanksgiving [MASK] all!\", \"yo yo what's up\"]\n",
    "\n",
    "# tokenize text and pass into model\n",
    "tokens = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "jax_input_ids = bf.Node(jnp.array(input_ids.numpy(), dtype=int), name=\"inputs\")\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/code/rycolab/transformers/src/transformers/models/bert/modeling_bf_bert.py:180: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in zeros is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  bf.Parameter(jnp.zeros(self.position_ids.shape, dtype=jnp.int64), name=\"position_ids\"),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: not detaching params for module BfBertForMaskedLM when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertModel when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertEmbeddings when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Embedding(word_embeddings) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Embedding(position_embeddings) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Embedding(token_type_embeddings) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module LayerNorm(in BfBertEmbeddings) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Dropout when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertEncoder when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module ModuleList when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertLayer when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertAttention when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertSelfAttention when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Linear(query) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Linear(key) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Linear(value) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Dropout when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertSelfOutput when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Linear(in BfBertSelfOutput) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module LayerNorm(in BfBertSelfOutput) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Dropout(in BfBertSelfOutput) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertIntermediate when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Linear(in BfBertIntermediate) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertOutput when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Linear(in BfBertOutput) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module LayerNorm(in BfBertOutput) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Dropout(in BfBertOutput) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertLayer when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertAttention when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertSelfAttention when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Linear(query) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Linear(key) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Linear(value) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Dropout when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertSelfOutput when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Linear(in BfBertSelfOutput) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module LayerNorm(in BfBertSelfOutput) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Dropout(in BfBertSelfOutput) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertIntermediate when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Linear(in BfBertIntermediate) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertOutput when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Linear(in BfBertOutput) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module LayerNorm(in BfBertOutput) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Dropout(in BfBertOutput) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertOnlyMLMHead when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertLMPredictionHead when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module BfBertPredictionHeadTransform when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Linear(in BfBertPredictionHeadTransform) when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module LayerNorm when saving state dict bc BF doesn't support that.\n",
      "WARNING: not detaching params for module Linear(in BfBertLMPredictionHead) when saving state dict bc BF doesn't support that.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bert_uncased_L-2_H-128_A-2 were not used when initializing BfBertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BfBertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BfBertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "m = BfBertForMaskedLM.from_pretrained(model_id)\n",
    "# m = BfBertForMaskedLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = m(jax_input_ids).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "qoi = logits[0, 0, 200] - logits[0, 0, 201]\n",
    "qoi.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BfBertEmbeddings(\n",
      "  (word_embeddings): Embedding(word_embeddings)(20, 4, padding_idx=0)\n",
      "  (position_embeddings): Embedding(position_embeddings)(32, 4)\n",
      "  (token_type_embeddings): Embedding(token_type_embeddings)(2, 4)\n",
      "  (LayerNorm): LayerNorm(in BfBertEmbeddings)((4,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/code/rycolab/transformers/src/transformers/models/bert/modeling_bf_bert.py:180: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in zeros is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  bf.Parameter(jnp.zeros(self.position_ids.shape, dtype=jnp.int64), name=\"position_ids\"),\n"
     ]
    }
   ],
   "source": [
    "# Create BfBertEmbeddings and BertEmbeddings\n",
    "bf_embs = BfBertEmbeddings(config)\n",
    "# Visualize output of forward pass of BfBertEmbeddings\n",
    "bf_embs.train(False)\n",
    "out_bf = bf_embs(input_ids=jax_input_ids)\n",
    "out_bf.visualize(collapse_to_modules=True)\n",
    "print(bf_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_56340/1679073517.py:5: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in array is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  hidden_states = bf.Node(jnp.array(hidden_states_torch.numpy(), dtype=jnp.float64), name=\"hidden_states\")\n",
      "/tmp/ipykernel_56340/1679073517.py:6: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in array is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  attention_mask = bf.Node(jnp.array(attention_mask_torch.numpy(), dtype=jnp.float64), name=\"attention_mask\")\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "dot_general requires contracting dimensions to have the same shape, got (128,) and (4,).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m attention_mask \u001b[39m=\u001b[39m bf\u001b[39m.\u001b[39mNode(jnp\u001b[39m.\u001b[39marray(attention_mask_torch\u001b[39m.\u001b[39mnumpy(), dtype\u001b[39m=\u001b[39mjnp\u001b[39m.\u001b[39mfloat64), name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m bert_enc \u001b[39m=\u001b[39m BfBertEncoder(config)\n\u001b[0;32m----> 8\u001b[0m out \u001b[39m=\u001b[39m bert_enc(hidden_states, attention_mask)\n\u001b[1;32m      9\u001b[0m out\u001b[39m.\u001b[39mlast_hidden_state\u001b[39m.\u001b[39mvisualize(collapse_to_modules\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/code/rycolab/brunoflow/brunoflow/net/network.py:53\u001b[0m, in \u001b[0;36mNetwork.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 53\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/code/rycolab/transformers/src/transformers/models/bert/modeling_bf_bert.py:629\u001b[0m, in \u001b[0;36mBfBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    606\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mGradient checkpointing is not implemented with brunoflow.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    607\u001b[0m     \u001b[39m# if use_cache:\u001b[39;00m\n\u001b[1;32m    608\u001b[0m     \u001b[39m#     logger.warning(\u001b[39;00m\n\u001b[1;32m    609\u001b[0m     \u001b[39m#         \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    627\u001b[0m \n\u001b[1;32m    628\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 629\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    630\u001b[0m         hidden_states,\n\u001b[1;32m    631\u001b[0m         attention_mask,\n\u001b[1;32m    632\u001b[0m         layer_head_mask,\n\u001b[1;32m    633\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    634\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    635\u001b[0m         past_key_value,\n\u001b[1;32m    636\u001b[0m         output_attentions,\n\u001b[1;32m    637\u001b[0m     )\n\u001b[1;32m    639\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    640\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/code/rycolab/brunoflow/brunoflow/net/network.py:53\u001b[0m, in \u001b[0;36mNetwork.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 53\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/code/rycolab/transformers/src/transformers/models/bert/modeling_bf_bert.py:506\u001b[0m, in \u001b[0;36mBfBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    495\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    496\u001b[0m     hidden_states: bf\u001b[39m.\u001b[39mNode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    503\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[bf\u001b[39m.\u001b[39mNode]:\n\u001b[1;32m    504\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    505\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 506\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    507\u001b[0m         hidden_states,\n\u001b[1;32m    508\u001b[0m         attention_mask,\n\u001b[1;32m    509\u001b[0m         head_mask,\n\u001b[1;32m    510\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    511\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    512\u001b[0m     )\n\u001b[1;32m    513\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    515\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/code/rycolab/brunoflow/brunoflow/net/network.py:53\u001b[0m, in \u001b[0;36mNetwork.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 53\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/code/rycolab/transformers/src/transformers/models/bert/modeling_bf_bert.py:433\u001b[0m, in \u001b[0;36mBfBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    423\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    424\u001b[0m     hidden_states: bf\u001b[39m.\u001b[39mNode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    430\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    431\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[bf\u001b[39m.\u001b[39mNode]:\n\u001b[1;32m    432\u001b[0m     hidden_states\u001b[39m.\u001b[39mname \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39minput to bertattention \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mhash\u001b[39m(\u001b[39mself\u001b[39m))\n\u001b[0;32m--> 433\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    434\u001b[0m         hidden_states,\n\u001b[1;32m    435\u001b[0m         attention_mask,\n\u001b[1;32m    436\u001b[0m         head_mask,\n\u001b[1;32m    437\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    438\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    439\u001b[0m         past_key_value,\n\u001b[1;32m    440\u001b[0m         output_attentions,\n\u001b[1;32m    441\u001b[0m     )  \u001b[39m# this calls selfattention (orange \"Multi-Head attention\") on the hidden states/input embeddings\u001b[39;00m\n\u001b[1;32m    442\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(\n\u001b[1;32m    443\u001b[0m         self_outputs[\u001b[39m0\u001b[39m], hidden_states\n\u001b[1;32m    444\u001b[0m     )  \u001b[39m# this combines the hidden states/input embeddings with the selfattention outputs and then normalizes\u001b[39;00m\n\u001b[1;32m    445\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/code/rycolab/brunoflow/brunoflow/net/network.py:53\u001b[0m, in \u001b[0;36mNetwork.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 53\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/code/rycolab/transformers/src/transformers/models/bert/modeling_bf_bert.py:275\u001b[0m, in \u001b[0;36mBfBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    266\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    267\u001b[0m     hidden_states: bf\u001b[39m.\u001b[39mNode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    273\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    274\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[bf\u001b[39m.\u001b[39mNode]:\n\u001b[0;32m--> 275\u001b[0m     mixed_query_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mquery(hidden_states)\n\u001b[1;32m    276\u001b[0m     mixed_query_layer\u001b[39m.\u001b[39mname \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbertselfattention mixed_query_layer\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m     \u001b[39m# If this is instantiated as a cross-attention module, the keys\u001b[39;00m\n\u001b[1;32m    279\u001b[0m     \u001b[39m# and values come from an encoder; the attention mask needs to be\u001b[39;00m\n\u001b[1;32m    280\u001b[0m     \u001b[39m# such that the encoder's padding tokens are not attended to.\u001b[39;00m\n",
      "File \u001b[0;32m~/code/rycolab/brunoflow/brunoflow/net/network.py:53\u001b[0m, in \u001b[0;36mNetwork.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 53\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/code/rycolab/brunoflow/brunoflow/net/linear.py:39\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 39\u001b[0m     out \u001b[39m=\u001b[39m matmul(x, matrix_transpose(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight))\n\u001b[1;32m     40\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m         out \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias\n",
      "File \u001b[0;32m~/code/rycolab/brunoflow/brunoflow/func/function.py:59\u001b[0m, in \u001b[0;36mmake_function.<locals>.autodiff_function\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m     57\u001b[0m in_vals \u001b[39m=\u001b[39m [ad\u001b[39m.\u001b[39mvalue(a) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m args]\n\u001b[1;32m     58\u001b[0m \u001b[39m# Apply the forward function\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m out_val \u001b[39m=\u001b[39m forward(\u001b[39m*\u001b[39;49min_vals)\n\u001b[1;32m     60\u001b[0m \u001b[39m# print(\"args:\", args)\u001b[39;00m\n\u001b[1;32m     61\u001b[0m name: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m name_fct(\u001b[39m*\u001b[39margs) \u001b[39mif\u001b[39;00m name_fct \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "File \u001b[0;32m~/code/rycolab/brunoflow/brunoflow/func/linalg.py:344\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(A, B)\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    330\u001b[0m                 jnp\u001b[39m.\u001b[39mmin(jnp\u001b[39m.\u001b[39meinsum(\u001b[39m\"\u001b[39m\u001b[39mij, bkj -> jbki\u001b[39m\u001b[39m\"\u001b[39m, B_factor, out_grad), axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m),\n\u001b[1;32m    331\u001b[0m                 jnp\u001b[39m.\u001b[39mmin(jnp\u001b[39m.\u001b[39meinsum(\u001b[39m\"\u001b[39m\u001b[39mbki, bkj -> kbij\u001b[39m\u001b[39m\"\u001b[39m, A_factor, out_grad), axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    334\u001b[0m                 jnp\u001b[39m.\u001b[39margmin(jnp\u001b[39m.\u001b[39meinsum(\u001b[39m\"\u001b[39m\u001b[39mbki, bkj -> kbij\u001b[39m\u001b[39m\"\u001b[39m, A_factor, out_grad), axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m),\n\u001b[1;32m    335\u001b[0m             )\n\u001b[1;32m    337\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    338\u001b[0m         jnp\u001b[39m.\u001b[39mmatmul(out_grad, __np_matrix_transpose(B_factor)),\n\u001b[1;32m    339\u001b[0m         jnp\u001b[39m.\u001b[39mmatmul(__np_matrix_transpose(A_factor), out_grad),\n\u001b[1;32m    340\u001b[0m     )\n\u001b[1;32m    343\u001b[0m matmul \u001b[39m=\u001b[39m make_function(\n\u001b[0;32m--> 344\u001b[0m     jax\u001b[39m.\u001b[39mjit(\u001b[39mlambda\u001b[39;00m A, B: jnp\u001b[39m.\u001b[39;49mmatmul(A, B)),\n\u001b[1;32m    345\u001b[0m     matmul_backward,\n\u001b[1;32m    346\u001b[0m     \u001b[39m# jax.jit(matmul_backward),\u001b[39;00m\n\u001b[1;32m    347\u001b[0m     construct_double_variable_fct_name(\u001b[39m\"\u001b[39m\u001b[39mmatmul\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m    348\u001b[0m )\n\u001b[1;32m    349\u001b[0m Node\u001b[39m.\u001b[39m\u001b[39m__matmul__\u001b[39m \u001b[39m=\u001b[39m matmul\n\u001b[1;32m    350\u001b[0m Node\u001b[39m.\u001b[39m\u001b[39m__rmatmul__\u001b[39m \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m A, B: Node\u001b[39m.\u001b[39m\u001b[39m__mul__\u001b[39m(B, A)\n",
      "    \u001b[0;31m[... skipping hidden 5 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/jax-hf/lib/python3.9/site-packages/jax/_src/numpy/lax_numpy.py:2979\u001b[0m, in \u001b[0;36mmatmul\u001b[0;34m(a, b, precision)\u001b[0m\n\u001b[1;32m   2977\u001b[0m a \u001b[39m=\u001b[39m lax\u001b[39m.\u001b[39msqueeze(a, \u001b[39mtuple\u001b[39m(a_squeeze))\n\u001b[1;32m   2978\u001b[0m b \u001b[39m=\u001b[39m lax\u001b[39m.\u001b[39msqueeze(b, \u001b[39mtuple\u001b[39m(b_squeeze))\n\u001b[0;32m-> 2979\u001b[0m out \u001b[39m=\u001b[39m lax\u001b[39m.\u001b[39;49mdot_general(\n\u001b[1;32m   2980\u001b[0m   a, b, (((ndim(a) \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m,), (ndim(b) \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m b_is_mat,)), (a_batch, b_batch)),\n\u001b[1;32m   2981\u001b[0m   precision\u001b[39m=\u001b[39;49mprecision)\n\u001b[1;32m   2982\u001b[0m \u001b[39mreturn\u001b[39;00m lax\u001b[39m.\u001b[39mtranspose(out, perm)\n",
      "    \u001b[0;31m[... skipping hidden 7 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/jax-hf/lib/python3.9/site-packages/jax/_src/lax/lax.py:2544\u001b[0m, in \u001b[0;36m_dot_general_shape_rule\u001b[0;34m(lhs, rhs, dimension_numbers, precision, preferred_element_type)\u001b[0m\n\u001b[1;32m   2541\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m core\u001b[39m.\u001b[39msymbolic_equal_shape(lhs_contracting_shape, rhs_contracting_shape):\n\u001b[1;32m   2542\u001b[0m   msg \u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mdot_general requires contracting dimensions to have the same \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2543\u001b[0m          \u001b[39m\"\u001b[39m\u001b[39mshape, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 2544\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg\u001b[39m.\u001b[39mformat(lhs_contracting_shape, rhs_contracting_shape))\n\u001b[1;32m   2546\u001b[0m \u001b[39mreturn\u001b[39;00m _dot_general_shape_computation(lhs\u001b[39m.\u001b[39mshape, rhs\u001b[39m.\u001b[39mshape, dimension_numbers)\n",
      "\u001b[0;31mTypeError\u001b[0m: dot_general requires contracting dimensions to have the same shape, got (128,) and (4,)."
     ]
    }
   ],
   "source": [
    "# Init inputs to bf and torch models\n",
    "hidden_states_torch = torch.randn(size=(2, 19, 128))\n",
    "attention_mask_torch = torch.randn(size=(2, 1, 1, 19))\n",
    "\n",
    "hidden_states = bf.Node(jnp.array(hidden_states_torch.numpy(), dtype=jnp.float64), name=\"hidden_states\")\n",
    "attention_mask = bf.Node(jnp.array(attention_mask_torch.numpy(), dtype=jnp.float64), name=\"attention_mask\")\n",
    "bert_enc = BfBertEncoder(config)\n",
    "out = bert_enc(hidden_states, attention_mask)\n",
    "out.last_hidden_state.visualize(collapse_to_modules=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_128456/2583383163.py:5: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in array is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  hidden_states = bf.Node(jnp.array(hidden_states_torch.numpy(), dtype=jnp.float64), name=\"hidden_states\")\n",
      "/tmp/ipykernel_128456/2583383163.py:6: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in array is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  attention_mask = bf.Node(jnp.array(attention_mask_torch.numpy(), dtype=jnp.float64), name=\"attention_mask\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Node of name matmul has inputs from different modules, {Linear(key)(), Linear(query)()}. Picking its module to be the FIRST one, Linear(query)().\n",
      "WARNING: Node of name matmul has inputs from different modules, {Linear(value)(), Linear(query)()}. Picking its module to be the FIRST one, Linear(query)().\n"
     ]
    }
   ],
   "source": [
    "# Init inputs to bf and torch models\n",
    "hidden_states_torch = torch.randn(size=(2, 19, 128))\n",
    "attention_mask_torch = torch.randn(size=(2, 1, 1, 19))\n",
    "\n",
    "hidden_states = bf.Node(jnp.array(hidden_states_torch.numpy(), dtype=jnp.float64), name=\"hidden_states\")\n",
    "attention_mask = bf.Node(jnp.array(attention_mask_torch.numpy(), dtype=jnp.float64), name=\"attention_mask\")\n",
    "bert = BfBertSelfAttention(config)\n",
    "out = bert(hidden_states, attention_mask)\n",
    "out[0].visualize(collapse_to_modules=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('jax-hf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b4cc9cc217af6b7e12b7da5c82d5884fde07a0e0f6b7f76767c2fbf53f076f9a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
