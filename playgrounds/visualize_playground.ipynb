{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: XLA_PYTHON_CLIENT_PREALLOCATE=false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/miniconda3/envs/jax-hf/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f628415a4b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%env XLA_PYTHON_CLIENT_PREALLOCATE=false\n",
    "import brunoflow as bf\n",
    "from brunoflow.ad.utils import check_node_equals_tensor, check_node_allclose_tensor\n",
    "from jax import numpy as jnp\n",
    "import transformers\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForMaskedLM, \n",
    "    BertForMaskedLM, \n",
    "    BertTokenizer, \n",
    "    BertTokenizerFast, \n",
    "    BertEmbeddings,\n",
    "    BfBertEmbeddings,\n",
    "    BfBertEncoder,\n",
    "    BertConfig,\n",
    "    BfBertSelfAttention,\n",
    ")\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-10 13:26:05.608504: E external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  7592,  1045,  2215,  2000,  4521,  2070,   103,  6240,  2651,\n",
      "          1012,  2009,  1005,  1055, 15060,   103,  2035,   999,   102],\n",
      "        [  101, 10930, 10930,  2054,  1005,  1055,  2039,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0]])\n"
     ]
    }
   ],
   "source": [
    "# Establish data\n",
    "model_id = \"google/bert_uncased_L-2_H-128_A-2\"\n",
    "config = BertConfig.from_pretrained(pretrained_model_name_or_path=\"../../brunoflow/models/bert/config-tiny.json\")\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_id)\n",
    "text = [\"hello I want to eat some [MASK] meat today. It's thanksgiving [MASK] all!\", \"yo yo what's up\"]\n",
    "\n",
    "# tokenize text and pass into model\n",
    "tokens = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "jax_input_ids = bf.Node(jnp.array(input_ids.numpy(), dtype=int), name=\"inputs\")\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/code/rycolab/transformers/src/transformers/models/bert/modeling_bf_bert.py:178: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in zeros is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  bf.Parameter(jnp.zeros(self.position_ids.shape, dtype=jnp.int64), name=\"position_ids\"),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Node of name + has inputs from different modules, {Embedding(token_type_embeddings)(2, 128), Embedding(word_embeddings)(30522, 128, padding_idx=0)}. Picking its module to be the FIRST one, Embedding(word_embeddings)(30522, 128, padding_idx=0).\n",
      "WARNING: Node of name + has inputs from different modules, {Embedding(word_embeddings)(30522, 128, padding_idx=0), Embedding(position_embeddings)(512, 128)}. Picking its module to be the FIRST one, Embedding(word_embeddings)(30522, 128, padding_idx=0).\n",
      "WARNING: Node of name * has inputs from different modules, {Embedding(word_embeddings)(30522, 128, padding_idx=0), LayerNorm(in BfBertEmbeddings)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertEmbeddings)((128,), eps=1e-12, elementwise_affine=True).\n",
      "BfBertEmbeddings(\n",
      "  (word_embeddings): Embedding(word_embeddings)(30522, 128, padding_idx=0)\n",
      "  (position_embeddings): Embedding(position_embeddings)(512, 128)\n",
      "  (token_type_embeddings): Embedding(token_type_embeddings)(2, 128)\n",
      "  (LayerNorm): LayerNorm(in BfBertEmbeddings)((128,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create BfBertEmbeddings and BertEmbeddings\n",
    "bf_embs = BfBertEmbeddings(config)\n",
    "# Visualize output of forward pass of BfBertEmbeddings\n",
    "bf_embs.train(False)\n",
    "out_bf = bf_embs(input_ids=jax_input_ids)\n",
    "out_bf.visualize(collapse_to_modules=True)\n",
    "print(bf_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_207089/1679073517.py:5: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in array is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  hidden_states = bf.Node(jnp.array(hidden_states_torch.numpy(), dtype=jnp.float64), name=\"hidden_states\")\n",
      "/tmp/ipykernel_207089/1679073517.py:6: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in array is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  attention_mask = bf.Node(jnp.array(attention_mask_torch.numpy(), dtype=jnp.float64), name=\"attention_mask\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Node of name matmul has inputs from different modules, {Linear(query)(), Linear(key)()}. Picking its module to be the FIRST one, Linear(query)().\n",
      "WARNING: Node of name matmul has inputs from different modules, {Linear(value)(), Linear(query)()}. Picking its module to be the FIRST one, Linear(query)().\n",
      "WARNING: Node of name matmul has inputs from different modules, {Linear(in BfBertSelfOutput)(), Linear(query)()}. Picking its module to be the FIRST one, Linear(query)().\n",
      "WARNING: Node of name + has inputs from different modules, {Linear(in BfBertSelfOutput)(), Linear(query)()}. Picking its module to be the FIRST one, Linear(query)().\n",
      "WARNING: Node of name * has inputs from different modules, {Linear(query)(), LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name matmul has inputs from different modules, {Linear(in BfBertIntermediate)(), LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name + has inputs from different modules, {Linear(in BfBertIntermediate)(), LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name matmul has inputs from different modules, {Linear(in BfBertOutput)(), LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name + has inputs from different modules, {Linear(in BfBertOutput)(), LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name * has inputs from different modules, {LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True), LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name matmul has inputs from different modules, {Linear(query)(), LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name + has inputs from different modules, {Linear(query)(), LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name matmul has inputs from different modules, {Linear(key)(), LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name + has inputs from different modules, {Linear(key)(), LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name matmul has inputs from different modules, {Linear(value)(), LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name + has inputs from different modules, {Linear(value)(), LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name matmul has inputs from different modules, {Linear(in BfBertSelfOutput)(), LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name + has inputs from different modules, {Linear(in BfBertSelfOutput)(), LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name * has inputs from different modules, {LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True), LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name matmul has inputs from different modules, {LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True), Linear(in BfBertIntermediate)()}. Picking its module to be the FIRST one, LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name + has inputs from different modules, {LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True), Linear(in BfBertIntermediate)()}. Picking its module to be the FIRST one, LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name matmul has inputs from different modules, {Linear(in BfBertOutput)(), LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name + has inputs from different modules, {Linear(in BfBertOutput)(), LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True).\n",
      "WARNING: Node of name * has inputs from different modules, {LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True), LayerNorm(in BfBertSelfOutput)((128,), eps=1e-12, elementwise_affine=True)}. Picking its module to be the FIRST one, LayerNorm(in BfBertOutput)((128,), eps=1e-12, elementwise_affine=True).\n"
     ]
    }
   ],
   "source": [
    "# Init inputs to bf and torch models\n",
    "hidden_states_torch = torch.randn(size=(2, 19, 128))\n",
    "attention_mask_torch = torch.randn(size=(2, 1, 1, 19))\n",
    "\n",
    "hidden_states = bf.Node(jnp.array(hidden_states_torch.numpy(), dtype=jnp.float64), name=\"hidden_states\")\n",
    "attention_mask = bf.Node(jnp.array(attention_mask_torch.numpy(), dtype=jnp.float64), name=\"attention_mask\")\n",
    "bert_enc = BfBertEncoder(config)\n",
    "out = bert_enc(hidden_states, attention_mask)\n",
    "out.last_hidden_state.visualize(collapse_to_modules=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_128456/2583383163.py:5: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in array is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  hidden_states = bf.Node(jnp.array(hidden_states_torch.numpy(), dtype=jnp.float64), name=\"hidden_states\")\n",
      "/tmp/ipykernel_128456/2583383163.py:6: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in array is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  attention_mask = bf.Node(jnp.array(attention_mask_torch.numpy(), dtype=jnp.float64), name=\"attention_mask\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Node of name matmul has inputs from different modules, {Linear(key)(), Linear(query)()}. Picking its module to be the FIRST one, Linear(query)().\n",
      "WARNING: Node of name matmul has inputs from different modules, {Linear(value)(), Linear(query)()}. Picking its module to be the FIRST one, Linear(query)().\n"
     ]
    }
   ],
   "source": [
    "# Init inputs to bf and torch models\n",
    "hidden_states_torch = torch.randn(size=(2, 19, 128))\n",
    "attention_mask_torch = torch.randn(size=(2, 1, 1, 19))\n",
    "\n",
    "hidden_states = bf.Node(jnp.array(hidden_states_torch.numpy(), dtype=jnp.float64), name=\"hidden_states\")\n",
    "attention_mask = bf.Node(jnp.array(attention_mask_torch.numpy(), dtype=jnp.float64), name=\"attention_mask\")\n",
    "bert = BfBertSelfAttention(config)\n",
    "out = bert(hidden_states, attention_mask)\n",
    "out[0].visualize(collapse_to_modules=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('jax-hf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b4cc9cc217af6b7e12b7da5c82d5884fde07a0e0f6b7f76767c2fbf53f076f9a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
