{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: XLA_PYTHON_CLIENT_PREALLOCATE=false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/miniconda3/envs/jax-hf/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f4e4679f6f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%env XLA_PYTHON_CLIENT_PREALLOCATE=false\n",
    "from jax.config import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "from jax import numpy as jnp\n",
    "from dataclasses import is_dataclass\n",
    "import transformers\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForMaskedLM, \n",
    "    BertForMaskedLM, \n",
    "    BertTokenizer, \n",
    "    BertTokenizerFast, \n",
    "    BertEmbeddings,\n",
    "    BfBertEmbeddings,\n",
    "    BertConfig,\n",
    "    BertSelfAttention,\n",
    "    BfBertSelfAttention,\n",
    "    BertSelfOutput,\n",
    "    BfBertSelfOutput,\n",
    "    BertAttention,\n",
    "    BfBertAttention,\n",
    "    BertLayer,\n",
    "    BfBertLayer,\n",
    "    BertEncoder,\n",
    "    BfBertEncoder,\n",
    "    BaseModelOutputWithPastAndCrossAttentions,\n",
    "    BfBaseModelOutputWithPastAndCrossAttentions,\n",
    "    BertOnlyMLMHead,\n",
    "    BfBertOnlyMLMHead,\n",
    "    BertPredictionHeadTransform,\n",
    "    BfBertPredictionHeadTransform,\n",
    "    BertLMPredictionHead,\n",
    "    BfBertLMPredictionHead\n",
    ")\n",
    "from brunoflow.ad.utils import check_node_equals_tensor, check_node_allclose_tensor\n",
    "from utils import check_bf_param_weights_match_torch, check_equivalent_class, check_dataclass_keys_match, check_model_outputs_allclose, check_bf_model_outputs_match_torch_outputs, check_bf_param_grads_allclose_torch\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-04 23:08:57.203807: E external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "# Init torch and bf models\n",
    "config = BertConfig.from_pretrained(pretrained_model_name_or_path=\"../../brunoflow/models/bert/config-tiny.json\")\n",
    "torch_model = BertLMPredictionHead(config)\n",
    "bf_model = BfBertLMPredictionHead(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init inputs to bf and torch models\n",
    "hidden_states_torch = torch.randn(size=(2, 19, 128))\n",
    "\n",
    "hidden_states = jnp.array(hidden_states_torch.numpy(), dtype=jnp.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.6 ms, sys: 2.74 ms, total: 11.3 ms\n",
      "Wall time: 5.58 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "outputs_torch = torch_model(hidden_states_torch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(outputs_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 471 ms, sys: 61.1 ms, total: 532 ms\n",
      "Wall time: 373 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "outputs_bf = bf_model(hidden_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that forward pass for bf works and matches output shape with torch\n",
    "if isinstance(outputs_bf, (list, tuple)):\n",
    "    # Handle case where outputs is a tuple/list and not just a single item\n",
    "    assert len(outputs_bf) == len(outputs_torch)\n",
    "    for i in range(len(outputs_bf)):\n",
    "        out_bf, out_torch = outputs_bf[i], outputs_torch[i] \n",
    "        assert(out_torch.shape == out_bf.shape)\n",
    "elif is_dataclass(outputs_bf):\n",
    "    check_equivalent_class(outputs_bf, outputs_torch)\n",
    "    check_dataclass_keys_match(outputs_bf, outputs_torch)\n",
    "else:\n",
    "    assert(outputs_torch.shape == outputs_bf.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save torch bertonlymlmhead to file\n",
    "save_path = \"bertonlymlmhead_torch.pt\"\n",
    "torch.save(torch_model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load state dict for bertonlymlmhead into BF and check weights, outputs, and backprop\n",
    "bf_model.load_state_dict(torch.load(save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check weights of BF model and Torch model match exactly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of param weight bias for bf and torch are equal? True\n",
      "Value of param weight transform.dense.weight for bf and torch are equal? True\n",
      "Value of param weight transform.dense.bias for bf and torch are equal? True\n",
      "Value of param weight transform.LayerNorm.weight for bf and torch are equal? True\n",
      "Value of param weight transform.LayerNorm.bias for bf and torch are equal? True\n",
      "Value of param weight decoder.weight for bf and torch are equal? True\n"
     ]
    }
   ],
   "source": [
    "# Check weights match\n",
    "check_bf_param_weights_match_torch(bf_model, torch_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check model output after forward pass matches for BF and Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of bf and torch are within 0.0001? True\n"
     ]
    }
   ],
   "source": [
    "# Check output from forward passes match for bf and torch\n",
    "torch_model.train(False)\n",
    "outputs_bf = bf_model(hidden_states=hidden_states)\n",
    "outputs_torch = torch_model(hidden_states=hidden_states_torch)\n",
    "\n",
    "if isinstance(outputs_bf, (list, tuple)):\n",
    "    assert len(outputs_bf) == len(outputs_torch)\n",
    "    for i in range(len(outputs_bf)):\n",
    "        out_bf, out_torch = outputs_bf[i], outputs_torch[i]\n",
    "        check_bf_model_outputs_match_torch_outputs(out_bf, out_torch, atol=1e-6)\n",
    "elif is_dataclass(outputs_bf):\n",
    "    check_model_outputs_allclose(outputs_bf, outputs_torch, print_stats=True, atol=1e-2)\n",
    "else:\n",
    "    check_bf_model_outputs_match_torch_outputs(outputs_bf, outputs_torch, atol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check grad after backward pass matches for BF and torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 74 ms, sys: 697 Âµs, total: 74.7 ms\n",
      "Wall time: 9.82 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/miniconda3/envs/jax-hf/lib/python3.9/site-packages/torch/autograd/__init__.py:173: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  /home/conda/feedstock_root/build_artifacts/pytorch-recipe_1664405705473/work/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Torch backward pass\n",
    "torch_model.train(True)\n",
    "\n",
    "if isinstance(outputs_torch, (list, tuple)):\n",
    "    assert len(outputs_bf) == len(outputs_torch)\n",
    "    backprop_node_torch = outputs_torch[0]\n",
    "elif is_dataclass(outputs_torch):\n",
    "    backprop_node_torch = outputs_torch.last_hidden_state\n",
    "else:\n",
    "    backprop_node_torch = outputs_torch\n",
    "    \n",
    "backprop_node_torch.backward(gradient=torch.ones_like(backprop_node_torch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.97 s, sys: 793 ms, total: 2.76 s\n",
      "Wall time: 1.96 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# BF backward pass\n",
    "\n",
    "if isinstance(outputs_bf, (list, tuple)):\n",
    "    assert len(outputs_bf) == len(outputs_torch)\n",
    "    backprop_node = outputs_bf[0]\n",
    "elif is_dataclass(outputs_torch):\n",
    "    backprop_node = outputs_bf.last_hidden_state\n",
    "else:\n",
    "    backprop_node = outputs_bf\n",
    "    \n",
    "backprop_node.backprop(values_to_compute=(\"grad\",))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad of param bias for bf and torch are within 0.001? True\n",
      "Grad of param transform.dense.weight for bf and torch are within 0.001? True\n",
      "Grad of param transform.dense.bias for bf and torch are within 0.001? True\n",
      "Grad of param transform.LayerNorm.weight for bf and torch are within 0.001? True\n",
      "Grad of param transform.LayerNorm.bias for bf and torch are within 0.001? True\n",
      "Grad of param decoder.weight for bf and torch are within 0.001? True\n"
     ]
    }
   ],
   "source": [
    "# Run the actual check\n",
    "check_bf_param_grads_allclose_torch(bf_model, torch_model, atol=1e-3, print_output=True, use_assert=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "b4cc9cc217af6b7e12b7da5c82d5884fde07a0e0f6b7f76767c2fbf53f076f9a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
