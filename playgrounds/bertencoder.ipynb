{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: XLA_PYTHON_CLIENT_PREALLOCATE=false\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2ab1a7002790>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%env XLA_PYTHON_CLIENT_PREALLOCATE=false\n",
    "from jax.config import config\n",
    "config.update(\"jax_enable_x64\", False)\n",
    "from jax import numpy as jnp\n",
    "from dataclasses import is_dataclass\n",
    "import transformers\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForMaskedLM, \n",
    "    BertForMaskedLM, \n",
    "    BertTokenizer, \n",
    "    BertTokenizerFast, \n",
    "    BertEmbeddings,\n",
    "    BfBertEmbeddings,\n",
    "    BertConfig,\n",
    "    BertSelfAttention,\n",
    "    BfBertSelfAttention,\n",
    "    BertSelfOutput,\n",
    "    BfBertSelfOutput,\n",
    "    BertAttention,\n",
    "    BfBertAttention,\n",
    "    BertLayer,\n",
    "    BfBertLayer,\n",
    "    BertEncoder,\n",
    "    BfBertEncoder,\n",
    "    BaseModelOutputWithPastAndCrossAttentions,\n",
    "    BfBaseModelOutputWithPastAndCrossAttentions,\n",
    ")\n",
    "from brunoflow.ad.utils import check_node_equals_tensor, check_node_allclose_tensor\n",
    "from utils import check_bf_param_weights_match_torch, check_equivalent_class, check_dataclass_keys_match, check_model_outputs_allclose, check_bf_model_outputs_match_torch_outputs, check_bf_param_grads_allclose_torch\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init torch and bf models\n",
    "config = BertConfig.from_pretrained(pretrained_model_name_or_path=\"../../brunoflow/models/bert/config.json\")\n",
    "torch_model = BertEncoder(config)\n",
    "bf_model = BfBertEncoder(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/252165267.tmpdir/ipykernel_125792/3409068801.py:5: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in array is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  hidden_states = jnp.array(hidden_states_torch.numpy(), dtype=jnp.float64)\n",
      "/scratch/252165267.tmpdir/ipykernel_125792/3409068801.py:6: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in array is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  attention_mask = jnp.array(attention_mask_torch.numpy(), dtype=jnp.float64)\n"
     ]
    }
   ],
   "source": [
    "# Init inputs to bf and torch models\n",
    "hidden_states_torch = torch.randn(size=(2, 19, 768))\n",
    "attention_mask_torch = torch.randn(size=(2, 1, 1, 19))\n",
    "\n",
    "hidden_states = jnp.array(hidden_states_torch.numpy(), dtype=jnp.float64)\n",
    "attention_mask = jnp.array(attention_mask_torch.numpy(), dtype=jnp.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 461 ms, sys: 19.5 ms, total: 481 ms\n",
      "Wall time: 485 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "outputs_torch = torch_model(hidden_states_torch, attention_mask_torch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(outputs_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.54 s, sys: 1.46 s, total: 4.99 s\n",
      "Wall time: 7.75 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "outputs_bf = bf_model(hidden_states, attention_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that forward pass for bf works and matches output shape with torch\n",
    "if isinstance(outputs_bf, (list, tuple)):\n",
    "    # Handle case where outputs is a tuple/list and not just a single item\n",
    "    assert len(outputs_bf) == len(outputs_torch)\n",
    "    for i in range(len(outputs_bf)):\n",
    "        out_bf, out_torch = outputs_bf[i], outputs_torch[i] \n",
    "        assert(out_torch.shape == out_bf.shape)\n",
    "elif is_dataclass(outputs_bf):\n",
    "    check_equivalent_class(outputs_bf, outputs_torch)\n",
    "    check_dataclass_keys_match(outputs_bf, outputs_torch)\n",
    "else:\n",
    "    assert(outputs_torch.shape == outputs_bf.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save torch BertSelfAttention to file\n",
    "save_path = \"bertlayer_torch.pt\"\n",
    "torch.save(torch_model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load state dict for BertSelfAttention into BF and check weights, outputs, and backprop\n",
    "bf_model.load_state_dict(torch.load(save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check weights of BF model and Torch model match exactly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of param weight layer.0.attention.self.query.bias for bf and torch are equal? True\n",
      "Value of param weight layer.0.attention.self.query.weight for bf and torch are equal? True\n",
      "Value of param weight layer.0.attention.self.key.bias for bf and torch are equal? True\n",
      "Value of param weight layer.0.attention.self.key.weight for bf and torch are equal? True\n",
      "Value of param weight layer.0.attention.self.value.bias for bf and torch are equal? True\n",
      "Value of param weight layer.0.attention.self.value.weight for bf and torch are equal? True\n",
      "Value of param weight layer.0.attention.output.dense.bias for bf and torch are equal? True\n",
      "Value of param weight layer.0.attention.output.dense.weight for bf and torch are equal? True\n",
      "Value of param weight layer.0.attention.output.LayerNorm.weight for bf and torch are equal? True\n",
      "Value of param weight layer.0.attention.output.LayerNorm.bias for bf and torch are equal? True\n",
      "Value of param weight layer.0.intermediate.dense.bias for bf and torch are equal? True\n",
      "Value of param weight layer.0.intermediate.dense.weight for bf and torch are equal? True\n",
      "Value of param weight layer.0.output.dense.bias for bf and torch are equal? True\n",
      "Value of param weight layer.0.output.dense.weight for bf and torch are equal? True\n",
      "Value of param weight layer.0.output.LayerNorm.weight for bf and torch are equal? True\n",
      "Value of param weight layer.0.output.LayerNorm.bias for bf and torch are equal? True\n",
      "Value of param weight layer.1.attention.self.query.bias for bf and torch are equal? True\n",
      "Value of param weight layer.1.attention.self.query.weight for bf and torch are equal? True\n",
      "Value of param weight layer.1.attention.self.key.bias for bf and torch are equal? True\n",
      "Value of param weight layer.1.attention.self.key.weight for bf and torch are equal? True\n",
      "Value of param weight layer.1.attention.self.value.bias for bf and torch are equal? True\n",
      "Value of param weight layer.1.attention.self.value.weight for bf and torch are equal? True\n",
      "Value of param weight layer.1.attention.output.dense.bias for bf and torch are equal? True\n",
      "Value of param weight layer.1.attention.output.dense.weight for bf and torch are equal? True\n",
      "Value of param weight layer.1.attention.output.LayerNorm.weight for bf and torch are equal? True\n",
      "Value of param weight layer.1.attention.output.LayerNorm.bias for bf and torch are equal? True\n",
      "Value of param weight layer.1.intermediate.dense.bias for bf and torch are equal? True\n",
      "Value of param weight layer.1.intermediate.dense.weight for bf and torch are equal? True\n",
      "Value of param weight layer.1.output.dense.bias for bf and torch are equal? True\n",
      "Value of param weight layer.1.output.dense.weight for bf and torch are equal? True\n",
      "Value of param weight layer.1.output.LayerNorm.weight for bf and torch are equal? True\n",
      "Value of param weight layer.1.output.LayerNorm.bias for bf and torch are equal? True\n",
      "Value of param weight layer.2.attention.self.query.bias for bf and torch are equal? True\n",
      "Value of param weight layer.2.attention.self.query.weight for bf and torch are equal? True\n",
      "Value of param weight layer.2.attention.self.key.bias for bf and torch are equal? True\n",
      "Value of param weight layer.2.attention.self.key.weight for bf and torch are equal? True\n",
      "Value of param weight layer.2.attention.self.value.bias for bf and torch are equal? True\n",
      "Value of param weight layer.2.attention.self.value.weight for bf and torch are equal? True\n",
      "Value of param weight layer.2.attention.output.dense.bias for bf and torch are equal? True\n",
      "Value of param weight layer.2.attention.output.dense.weight for bf and torch are equal? True\n",
      "Value of param weight layer.2.attention.output.LayerNorm.weight for bf and torch are equal? True\n",
      "Value of param weight layer.2.attention.output.LayerNorm.bias for bf and torch are equal? True\n",
      "Value of param weight layer.2.intermediate.dense.bias for bf and torch are equal? True\n",
      "Value of param weight layer.2.intermediate.dense.weight for bf and torch are equal? True\n",
      "Value of param weight layer.2.output.dense.bias for bf and torch are equal? True\n",
      "Value of param weight layer.2.output.dense.weight for bf and torch are equal? True\n",
      "Value of param weight layer.2.output.LayerNorm.weight for bf and torch are equal? True\n",
      "Value of param weight layer.2.output.LayerNorm.bias for bf and torch are equal? True\n",
      "Value of param weight layer.3.attention.self.query.bias for bf and torch are equal? True\n",
      "Value of param weight layer.3.attention.self.query.weight for bf and torch are equal? True\n",
      "Value of param weight layer.3.attention.self.key.bias for bf and torch are equal? True\n",
      "Value of param weight layer.3.attention.self.key.weight for bf and torch are equal? True\n",
      "Value of param weight layer.3.attention.self.value.bias for bf and torch are equal? True\n",
      "Value of param weight layer.3.attention.self.value.weight for bf and torch are equal? True\n",
      "Value of param weight layer.3.attention.output.dense.bias for bf and torch are equal? True\n",
      "Value of param weight layer.3.attention.output.dense.weight for bf and torch are equal? True\n",
      "Value of param weight layer.3.attention.output.LayerNorm.weight for bf and torch are equal? True\n",
      "Value of param weight layer.3.attention.output.LayerNorm.bias for bf and torch are equal? True\n",
      "Value of param weight layer.3.intermediate.dense.bias for bf and torch are equal? True\n",
      "Value of param weight layer.3.intermediate.dense.weight for bf and torch are equal? True\n",
      "Value of param weight layer.3.output.dense.bias for bf and torch are equal? True\n",
      "Value of param weight layer.3.output.dense.weight for bf and torch are equal? True\n",
      "Value of param weight layer.3.output.LayerNorm.weight for bf and torch are equal? True\n",
      "Value of param weight layer.3.output.LayerNorm.bias for bf and torch are equal? True\n",
      "Value of param weight layer.4.attention.self.query.bias for bf and torch are equal? True\n",
      "Value of param weight layer.4.attention.self.query.weight for bf and torch are equal? True\n",
      "Value of param weight layer.4.attention.self.key.bias for bf and torch are equal? True\n",
      "Value of param weight layer.4.attention.self.key.weight for bf and torch are equal? True\n",
      "Value of param weight layer.4.attention.self.value.bias for bf and torch are equal? True\n",
      "Value of param weight layer.4.attention.self.value.weight for bf and torch are equal? True\n",
      "Value of param weight layer.4.attention.output.dense.bias for bf and torch are equal? True\n",
      "Value of param weight layer.4.attention.output.dense.weight for bf and torch are equal? True\n",
      "Value of param weight layer.4.attention.output.LayerNorm.weight for bf and torch are equal? True\n",
      "Value of param weight layer.4.attention.output.LayerNorm.bias for bf and torch are equal? True\n",
      "Value of param weight layer.4.intermediate.dense.bias for bf and torch are equal? True\n",
      "Value of param weight layer.4.intermediate.dense.weight for bf and torch are equal? True\n",
      "Value of param weight layer.4.output.dense.bias for bf and torch are equal? True\n",
      "Value of param weight layer.4.output.dense.weight for bf and torch are equal? True\n",
      "Value of param weight layer.4.output.LayerNorm.weight for bf and torch are equal? True\n",
      "Value of param weight layer.4.output.LayerNorm.bias for bf and torch are equal? True\n",
      "Value of param weight layer.5.attention.self.query.bias for bf and torch are equal? True\n",
      "Value of param weight layer.5.attention.self.query.weight for bf and torch are equal? True\n",
      "Value of param weight layer.5.attention.self.key.bias for bf and torch are equal? True\n",
      "Value of param weight layer.5.attention.self.key.weight for bf and torch are equal? True\n",
      "Value of param weight layer.5.attention.self.value.bias for bf and torch are equal? True\n",
      "Value of param weight layer.5.attention.self.value.weight for bf and torch are equal? True\n",
      "Value of param weight layer.5.attention.output.dense.bias for bf and torch are equal? True\n",
      "Value of param weight layer.5.attention.output.dense.weight for bf and torch are equal? True\n",
      "Value of param weight layer.5.attention.output.LayerNorm.weight for bf and torch are equal? True\n",
      "Value of param weight layer.5.attention.output.LayerNorm.bias for bf and torch are equal? True\n",
      "Value of param weight layer.5.intermediate.dense.bias for bf and torch are equal? True\n",
      "Value of param weight layer.5.intermediate.dense.weight for bf and torch are equal? True\n",
      "Value of param weight layer.5.output.dense.bias for bf and torch are equal? True\n",
      "Value of param weight layer.5.output.dense.weight for bf and torch are equal? True\n",
      "Value of param weight layer.5.output.LayerNorm.weight for bf and torch are equal? True\n",
      "Value of param weight layer.5.output.LayerNorm.bias for bf and torch are equal? True\n",
      "Value of param weight layer.6.attention.self.query.bias for bf and torch are equal? True\n",
      "Value of param weight layer.6.attention.self.query.weight for bf and torch are equal? True\n",
      "Value of param weight layer.6.attention.self.key.bias for bf and torch are equal? True\n",
      "Value of param weight layer.6.attention.self.key.weight for bf and torch are equal? True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of param weight layer.6.attention.self.value.bias for bf and torch are equal? True\n",
      "Value of param weight layer.6.attention.self.value.weight for bf and torch are equal? True\n",
      "Value of param weight layer.6.attention.output.dense.bias for bf and torch are equal? True\n",
      "Value of param weight layer.6.attention.output.dense.weight for bf and torch are equal? True\n",
      "Value of param weight layer.6.attention.output.LayerNorm.weight for bf and torch are equal? True\n",
      "Value of param weight layer.6.attention.output.LayerNorm.bias for bf and torch are equal? True\n",
      "Value of param weight layer.6.intermediate.dense.bias for bf and torch are equal? True\n",
      "Value of param weight layer.6.intermediate.dense.weight for bf and torch are equal? True\n",
      "Value of param weight layer.6.output.dense.bias for bf and torch are equal? True\n",
      "Value of param weight layer.6.output.dense.weight for bf and torch are equal? True\n",
      "Value of param weight layer.6.output.LayerNorm.weight for bf and torch are equal? True\n",
      "Value of param weight layer.6.output.LayerNorm.bias for bf and torch are equal? True\n",
      "Value of param weight layer.7.attention.self.query.bias for bf and torch are equal? True\n",
      "Value of param weight layer.7.attention.self.query.weight for bf and torch are equal? True\n",
      "Value of param weight layer.7.attention.self.key.bias for bf and torch are equal? True\n",
      "Value of param weight layer.7.attention.self.key.weight for bf and torch are equal? True\n",
      "Value of param weight layer.7.attention.self.value.bias for bf and torch are equal? True\n",
      "Value of param weight layer.7.attention.self.value.weight for bf and torch are equal? True\n",
      "Value of param weight layer.7.attention.output.dense.bias for bf and torch are equal? True\n",
      "Value of param weight layer.7.attention.output.dense.weight for bf and torch are equal? True\n",
      "Value of param weight layer.7.attention.output.LayerNorm.weight for bf and torch are equal? True\n",
      "Value of param weight layer.7.attention.output.LayerNorm.bias for bf and torch are equal? True\n",
      "Value of param weight layer.7.intermediate.dense.bias for bf and torch are equal? True\n",
      "Value of param weight layer.7.intermediate.dense.weight for bf and torch are equal? True\n",
      "Value of param weight layer.7.output.dense.bias for bf and torch are equal? True\n",
      "Value of param weight layer.7.output.dense.weight for bf and torch are equal? True\n",
      "Value of param weight layer.7.output.LayerNorm.weight for bf and torch are equal? True\n",
      "Value of param weight layer.7.output.LayerNorm.bias for bf and torch are equal? True\n",
      "Value of param weight layer.8.attention.self.query.bias for bf and torch are equal? True\n",
      "Value of param weight layer.8.attention.self.query.weight for bf and torch are equal? True\n",
      "Value of param weight layer.8.attention.self.key.bias for bf and torch are equal? True\n",
      "Value of param weight layer.8.attention.self.key.weight for bf and torch are equal? True\n",
      "Value of param weight layer.8.attention.self.value.bias for bf and torch are equal? True\n",
      "Value of param weight layer.8.attention.self.value.weight for bf and torch are equal? True\n",
      "Value of param weight layer.8.attention.output.dense.bias for bf and torch are equal? True\n",
      "Value of param weight layer.8.attention.output.dense.weight for bf and torch are equal? True\n",
      "Value of param weight layer.8.attention.output.LayerNorm.weight for bf and torch are equal? True\n",
      "Value of param weight layer.8.attention.output.LayerNorm.bias for bf and torch are equal? True\n",
      "Value of param weight layer.8.intermediate.dense.bias for bf and torch are equal? True\n",
      "Value of param weight layer.8.intermediate.dense.weight for bf and torch are equal? True\n",
      "Value of param weight layer.8.output.dense.bias for bf and torch are equal? True\n",
      "Value of param weight layer.8.output.dense.weight for bf and torch are equal? True\n",
      "Value of param weight layer.8.output.LayerNorm.weight for bf and torch are equal? True\n",
      "Value of param weight layer.8.output.LayerNorm.bias for bf and torch are equal? True\n",
      "Value of param weight layer.9.attention.self.query.bias for bf and torch are equal? True\n",
      "Value of param weight layer.9.attention.self.query.weight for bf and torch are equal? True\n",
      "Value of param weight layer.9.attention.self.key.bias for bf and torch are equal? True\n",
      "Value of param weight layer.9.attention.self.key.weight for bf and torch are equal? True\n",
      "Value of param weight layer.9.attention.self.value.bias for bf and torch are equal? True\n",
      "Value of param weight layer.9.attention.self.value.weight for bf and torch are equal? True\n",
      "Value of param weight layer.9.attention.output.dense.bias for bf and torch are equal? True\n",
      "Value of param weight layer.9.attention.output.dense.weight for bf and torch are equal? True\n",
      "Value of param weight layer.9.attention.output.LayerNorm.weight for bf and torch are equal? True\n",
      "Value of param weight layer.9.attention.output.LayerNorm.bias for bf and torch are equal? True\n",
      "Value of param weight layer.9.intermediate.dense.bias for bf and torch are equal? True\n",
      "Value of param weight layer.9.intermediate.dense.weight for bf and torch are equal? True\n",
      "Value of param weight layer.9.output.dense.bias for bf and torch are equal? True\n",
      "Value of param weight layer.9.output.dense.weight for bf and torch are equal? True\n",
      "Value of param weight layer.9.output.LayerNorm.weight for bf and torch are equal? True\n",
      "Value of param weight layer.9.output.LayerNorm.bias for bf and torch are equal? True\n",
      "Value of param weight layer.10.attention.self.query.bias for bf and torch are equal? True\n",
      "Value of param weight layer.10.attention.self.query.weight for bf and torch are equal? True\n",
      "Value of param weight layer.10.attention.self.key.bias for bf and torch are equal? True\n",
      "Value of param weight layer.10.attention.self.key.weight for bf and torch are equal? True\n",
      "Value of param weight layer.10.attention.self.value.bias for bf and torch are equal? True\n",
      "Value of param weight layer.10.attention.self.value.weight for bf and torch are equal? True\n",
      "Value of param weight layer.10.attention.output.dense.bias for bf and torch are equal? True\n",
      "Value of param weight layer.10.attention.output.dense.weight for bf and torch are equal? True\n",
      "Value of param weight layer.10.attention.output.LayerNorm.weight for bf and torch are equal? True\n",
      "Value of param weight layer.10.attention.output.LayerNorm.bias for bf and torch are equal? True\n",
      "Value of param weight layer.10.intermediate.dense.bias for bf and torch are equal? True\n",
      "Value of param weight layer.10.intermediate.dense.weight for bf and torch are equal? True\n",
      "Value of param weight layer.10.output.dense.bias for bf and torch are equal? True\n",
      "Value of param weight layer.10.output.dense.weight for bf and torch are equal? True\n",
      "Value of param weight layer.10.output.LayerNorm.weight for bf and torch are equal? True\n",
      "Value of param weight layer.10.output.LayerNorm.bias for bf and torch are equal? True\n",
      "Value of param weight layer.11.attention.self.query.bias for bf and torch are equal? True\n",
      "Value of param weight layer.11.attention.self.query.weight for bf and torch are equal? True\n",
      "Value of param weight layer.11.attention.self.key.bias for bf and torch are equal? True\n",
      "Value of param weight layer.11.attention.self.key.weight for bf and torch are equal? True\n",
      "Value of param weight layer.11.attention.self.value.bias for bf and torch are equal? True\n",
      "Value of param weight layer.11.attention.self.value.weight for bf and torch are equal? True\n",
      "Value of param weight layer.11.attention.output.dense.bias for bf and torch are equal? True\n",
      "Value of param weight layer.11.attention.output.dense.weight for bf and torch are equal? True\n",
      "Value of param weight layer.11.attention.output.LayerNorm.weight for bf and torch are equal? True\n",
      "Value of param weight layer.11.attention.output.LayerNorm.bias for bf and torch are equal? True\n",
      "Value of param weight layer.11.intermediate.dense.bias for bf and torch are equal? True\n",
      "Value of param weight layer.11.intermediate.dense.weight for bf and torch are equal? True\n",
      "Value of param weight layer.11.output.dense.bias for bf and torch are equal? True\n",
      "Value of param weight layer.11.output.dense.weight for bf and torch are equal? True\n",
      "Value of param weight layer.11.output.LayerNorm.weight for bf and torch are equal? True\n",
      "Value of param weight layer.11.output.LayerNorm.bias for bf and torch are equal? True\n"
     ]
    }
   ],
   "source": [
    "# Check weights match\n",
    "check_bf_param_weights_match_torch(bf_model, torch_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check model output after forward pass matches for BF and Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking diff between BF and torch for last_hidden_state:\n",
      "Output of bf and torch are within 0.01? True\n",
      "\tStats on diff in outputs between bf and torch:                   0\n",
      "count  29184.000000\n",
      "mean       0.000298\n",
      "std        0.000224\n",
      "min        0.000000\n",
      "25%        0.000119\n",
      "50%        0.000252\n",
      "75%        0.000430\n",
      "max        0.001816\n"
     ]
    }
   ],
   "source": [
    "# Check output from forward passes match for bf and torch\n",
    "torch_model.train(False)\n",
    "outputs_bf = bf_model(hidden_states=hidden_states, attention_mask=attention_mask)\n",
    "outputs_torch = torch_model(hidden_states=hidden_states_torch, attention_mask=attention_mask_torch)\n",
    "\n",
    "if isinstance(outputs_bf, (list, tuple)):\n",
    "    assert len(outputs_bf) == len(outputs_torch)\n",
    "    for i in range(len(outputs_bf)):\n",
    "        out_bf, out_torch = outputs_bf[i], outputs_torch[i]\n",
    "        check_bf_model_outputs_match_torch_outputs(out_bf, out_torch, atol=1e-6)\n",
    "elif is_dataclass(outputs_bf):\n",
    "    check_model_outputs_allclose(outputs_bf, outputs_torch, print_stats=True, atol=1e-2)\n",
    "else:\n",
    "    check_bf_model_outputs_match_torch_outputs(outputs_bf, outputs_torch, atol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check grad after backward pass matches for BF and torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 775 ms, sys: 86.5 ms, total: 861 ms\n",
      "Wall time: 862 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Torch backward pass\n",
    "torch_model.train(True)\n",
    "\n",
    "if isinstance(outputs_torch, (list, tuple)):\n",
    "    assert len(outputs_bf) == len(outputs_torch)\n",
    "    backprop_node_torch = outputs_torch[0]\n",
    "elif is_dataclass(outputs_torch):\n",
    "    backprop_node_torch = outputs_torch.last_hidden_state\n",
    "else:\n",
    "    backprop_node_torch = outputs_torch\n",
    "    \n",
    "backprop_node_torch.backward(gradient=torch.ones_like(backprop_node_torch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.34 s, sys: 5.73 s, total: 14.1 s\n",
      "Wall time: 22.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# BF backward pass\n",
    "\n",
    "if isinstance(outputs_bf, (list, tuple)):\n",
    "    assert len(outputs_bf) == len(outputs_torch)\n",
    "    backprop_node = outputs_bf[0]\n",
    "elif is_dataclass(outputs_torch):\n",
    "    backprop_node = outputs_bf.last_hidden_state\n",
    "else:\n",
    "    backprop_node = outputs_bf\n",
    "    \n",
    "backprop_node.backprop(values_to_compute=(\"grad\",))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad of param layer.0.attention.self.query.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.0.attention.self.query.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.0.attention.self.key.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.0.attention.self.key.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.0.attention.self.value.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.0.attention.self.value.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.0.attention.output.dense.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.0.attention.output.dense.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.0.attention.output.LayerNorm.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.0.attention.output.LayerNorm.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.0.intermediate.dense.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.0.intermediate.dense.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.0.output.dense.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.0.output.dense.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.0.output.LayerNorm.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.0.output.LayerNorm.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.1.attention.self.query.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.1.attention.self.query.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.1.attention.self.key.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.1.attention.self.key.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.1.attention.self.value.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.1.attention.self.value.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.1.attention.output.dense.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.1.attention.output.dense.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.1.attention.output.LayerNorm.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.1.attention.output.LayerNorm.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.1.intermediate.dense.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.1.intermediate.dense.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.1.output.dense.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.1.output.dense.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.1.output.LayerNorm.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.1.output.LayerNorm.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.2.attention.self.query.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.2.attention.self.query.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.2.attention.self.key.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.2.attention.self.key.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.2.attention.self.value.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.2.attention.self.value.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.2.attention.output.dense.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.2.attention.output.dense.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.2.attention.output.LayerNorm.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.2.attention.output.LayerNorm.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.2.intermediate.dense.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.2.intermediate.dense.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.2.output.dense.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.2.output.dense.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.2.output.LayerNorm.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.2.output.LayerNorm.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.3.attention.self.query.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.3.attention.self.query.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.3.attention.self.key.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.3.attention.self.key.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.3.attention.self.value.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.3.attention.self.value.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.3.attention.output.dense.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.3.attention.output.dense.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.3.attention.output.LayerNorm.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.3.attention.output.LayerNorm.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.3.intermediate.dense.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.3.intermediate.dense.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.3.output.dense.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.3.output.dense.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.3.output.LayerNorm.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.3.output.LayerNorm.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.4.attention.self.query.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.4.attention.self.query.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.4.attention.self.key.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.4.attention.self.key.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.4.attention.self.value.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.4.attention.self.value.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.4.attention.output.dense.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.4.attention.output.dense.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.4.attention.output.LayerNorm.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.4.attention.output.LayerNorm.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.4.intermediate.dense.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.4.intermediate.dense.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.4.output.dense.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.4.output.dense.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.4.output.LayerNorm.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.4.output.LayerNorm.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.5.attention.self.query.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.5.attention.self.query.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.5.attention.self.key.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.5.attention.self.key.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.5.attention.self.value.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.5.attention.self.value.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.5.attention.output.dense.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.5.attention.output.dense.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.5.attention.output.LayerNorm.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.5.attention.output.LayerNorm.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.5.intermediate.dense.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.5.intermediate.dense.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.5.output.dense.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.5.output.dense.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.5.output.LayerNorm.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.5.output.LayerNorm.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.6.attention.self.query.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.6.attention.self.query.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.6.attention.self.key.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.6.attention.self.key.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.6.attention.self.value.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.6.attention.self.value.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.6.attention.output.dense.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.6.attention.output.dense.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.6.attention.output.LayerNorm.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.6.attention.output.LayerNorm.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.6.intermediate.dense.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.6.intermediate.dense.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.6.output.dense.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.6.output.dense.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.6.output.LayerNorm.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.6.output.LayerNorm.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.7.attention.self.query.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.7.attention.self.query.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.7.attention.self.key.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.7.attention.self.key.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.7.attention.self.value.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.7.attention.self.value.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.7.attention.output.dense.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.7.attention.output.dense.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.7.attention.output.LayerNorm.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.7.attention.output.LayerNorm.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.7.intermediate.dense.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.7.intermediate.dense.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.7.output.dense.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.7.output.dense.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.7.output.LayerNorm.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.7.output.LayerNorm.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.8.attention.self.query.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.8.attention.self.query.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.8.attention.self.key.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.8.attention.self.key.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.8.attention.self.value.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.8.attention.self.value.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.8.attention.output.dense.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.8.attention.output.dense.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.8.attention.output.LayerNorm.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.8.attention.output.LayerNorm.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.8.intermediate.dense.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.8.intermediate.dense.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.8.output.dense.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.8.output.dense.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.8.output.LayerNorm.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.8.output.LayerNorm.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.9.attention.self.query.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.9.attention.self.query.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.9.attention.self.key.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.9.attention.self.key.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.9.attention.self.value.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.9.attention.self.value.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.9.attention.output.dense.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.9.attention.output.dense.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.9.attention.output.LayerNorm.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.9.attention.output.LayerNorm.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.9.intermediate.dense.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.9.intermediate.dense.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.9.output.dense.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.9.output.dense.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.9.output.LayerNorm.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.9.output.LayerNorm.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.10.attention.self.query.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.10.attention.self.query.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.10.attention.self.key.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.10.attention.self.key.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.10.attention.self.value.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.10.attention.self.value.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.10.attention.output.dense.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.10.attention.output.dense.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.10.attention.output.LayerNorm.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.10.attention.output.LayerNorm.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.10.intermediate.dense.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.10.intermediate.dense.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.10.output.dense.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.10.output.dense.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.10.output.LayerNorm.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.10.output.LayerNorm.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.11.attention.self.query.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.11.attention.self.query.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.11.attention.self.key.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.11.attention.self.key.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.11.attention.self.value.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.11.attention.self.value.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.11.attention.output.dense.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.11.attention.output.dense.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.11.attention.output.LayerNorm.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.11.attention.output.LayerNorm.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.11.intermediate.dense.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.11.intermediate.dense.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.11.output.dense.bias for bf and torch are within 1e-05? True\n",
      "Grad of param layer.11.output.dense.weight for bf and torch are within 1e-05? True\n",
      "Grad of param layer.11.output.LayerNorm.weight for bf and torch are within 1e-05? False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tStats on diff in grad for layer.11.output.LayerNorm.weight between bf and torch:                 0\n",
      "count  768.000000\n",
      "mean     0.005615\n",
      "std      0.004385\n",
      "min      0.000000\n",
      "25%      0.002037\n",
      "50%      0.004748\n",
      "75%      0.008144\n",
      "max      0.024424\n",
      "Grad of param layer.11.output.LayerNorm.bias for bf and torch are within 1e-05? True\n"
     ]
    }
   ],
   "source": [
    "# Run the actual check\n",
    "check_bf_param_grads_allclose_torch(bf_model, torch_model, atol=1e-5, print_output=True, use_assert=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38.\n",
      " 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38.\n",
      " 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38.\n",
      " 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38.\n",
      " 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38.\n",
      " 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38.\n",
      " 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38.\n",
      " 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38.\n",
      " 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38.\n",
      " 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38.\n",
      " 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38.\n",
      " 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38.\n",
      " 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38.\n",
      " 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38.\n",
      " 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38.\n",
      " 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38.\n",
      " 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38.\n",
      " 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38.\n",
      " 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38.\n",
      " 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38.\n",
      " 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38.\n",
      " 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38.\n",
      " 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38.\n",
      " 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38.\n",
      " 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38.\n",
      " 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38.\n",
      " 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38.\n",
      " 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38.\n",
      " 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38.\n",
      " 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38.\n",
      " 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38.\n",
      " 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38.\n",
      " 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38.\n",
      " 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38.\n",
      " 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38.\n",
      " 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38.\n",
      " 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38.\n",
      " 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38.\n",
      " 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38.\n",
      " 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38.\n",
      " 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38.\n",
      " 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38.\n",
      " 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38. 38.]\n"
     ]
    }
   ],
   "source": [
    "print(dict(bf_model.named_parameters())[\"layer.11.output.LayerNorm.bias\"].grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "        38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "        38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "        38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "        38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "        38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "        38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "        38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "        38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "        38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "        38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "        38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "        38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "        38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "        38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "        38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "        38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "        38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "        38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "        38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "        38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "        38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "        38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "        38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "        38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "        38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "        38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "        38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "        38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "        38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "        38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "        38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "        38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "        38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "        38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "        38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "        38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "        38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "        38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "        38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "        38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "        38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "        38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "        38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "        38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "        38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "        38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "        38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "        38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "        38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "        38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "        38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "        38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "        38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "        38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.])\n"
     ]
    }
   ],
   "source": [
    "print(dict(torch_model.named_parameters())[\"layer.11.output.LayerNorm.bias\"].grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "b4cc9cc217af6b7e12b7da5c82d5884fde07a0e0f6b7f76767c2fbf53f076f9a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
