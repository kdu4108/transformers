{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: XLA_PYTHON_CLIENT_PREALLOCATE=false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/miniconda3/envs/jax-hf/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f67759da6d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%env XLA_PYTHON_CLIENT_PREALLOCATE=false\n",
    "from jax.config import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "from dataclasses import is_dataclass\n",
    "from jax import numpy as jnp\n",
    "import transformers\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForMaskedLM, \n",
    "    BertTokenizer, \n",
    "    BertTokenizerFast, \n",
    "    BertEmbeddings,\n",
    "    BfBertEmbeddings,\n",
    "    BertConfig,\n",
    "    BertSelfAttention,\n",
    "    BfBertSelfAttention,\n",
    "    BertSelfOutput,\n",
    "    BfBertSelfOutput,\n",
    "    BertAttention,\n",
    "    BfBertAttention,\n",
    "    BertLayer,\n",
    "    BfBertLayer,\n",
    "    BertEncoder,\n",
    "    BfBertEncoder,\n",
    "    BaseModelOutputWithPastAndCrossAttentions,\n",
    "    BfBaseModelOutputWithPastAndCrossAttentions,\n",
    "    BertForMaskedLM,\n",
    "    BfBertForMaskedLM,\n",
    ")\n",
    "from brunoflow.ad.utils import check_node_equals_tensor, check_node_allclose_tensor\n",
    "from utils import check_bf_param_weights_match_torch, check_equivalent_class, check_dataclass_keys_match, check_model_outputs_allclose, check_bf_model_outputs_match_torch_outputs, check_bf_param_grads_allclose_torch\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bert_uncased_L-2_H-128_A-2 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2023-01-05 00:04:52.619039: E external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n",
      "WARNING: we don't actually initialize weights here! If we ever need to actually train we can properly implement this.\n"
     ]
    }
   ],
   "source": [
    "# Init torch and bf models\n",
    "BF_FROM_MODEL_ID = False ### NOTE: BECAUSE THIS IS SUPER HACKY THIS SOMEWHAT DOES NOT WORK WHEN SET TO TRUE. FROM_PRETRAINED FOR BRUNOFLOW IS PROBABLY SOMEWHAT BROKEN, BUT AT LEAST THIS IS A WORKAROUND. Also it looks like the errors are only bounded by 0.01 :/.\n",
    "TORCH_FROM_MODEL_ID = True\n",
    "# model_id = \"bert-base-uncased\"\n",
    "model_id = \"google/bert_uncased_L-2_H-128_A-2\"\n",
    "config = BertConfig.from_pretrained(pretrained_model_name_or_path=\"../../brunoflow/models/bert/config-tiny.json\")\n",
    "\n",
    "if TORCH_FROM_MODEL_ID:\n",
    "    torch_model = BertForMaskedLM.from_pretrained(model_id)\n",
    "else:\n",
    "    torch_model = BertForMaskedLM(config)\n",
    "if BF_FROM_MODEL_ID:\n",
    "    bf_model = BfBertForMaskedLM.from_pretrained(model_id)\n",
    "else:\n",
    "    bf_model = BfBertForMaskedLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish data\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_id)\n",
    "text = [\"hello I want to eat some [MASK] meat today. It's thanksgiving [MASK] all!\", \"yo yo what's up\"]\n",
    "tokens = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# Create torch and bf inputs to model\n",
    "input_ids_torch = tokens[\"input_ids\"]\n",
    "labels_torch = torch.ones_like(input_ids_torch)\n",
    "\n",
    "input_ids_bf = jnp.array(input_ids_torch.numpy())\n",
    "labels_bf = jnp.array(labels_torch.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.modeling_outputs.MaskedLMOutput'>\n",
      "CPU times: user 78.4 ms, sys: 0 ns, total: 78.4 ms\n",
      "Wall time: 14.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "outputs_torch = torch_model(input_ids_torch)\n",
    "print(type(outputs_torch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.modeling_bf_outputs.BfMaskedLMOutput'>\n",
      "CPU times: user 1.22 s, sys: 71.1 ms, total: 1.3 s\n",
      "Wall time: 1.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "outputs_bf = bf_model(input_ids_bf)\n",
    "print(type(outputs_bf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that forward pass for bf works and matches output shape with torch\n",
    "if isinstance(outputs_bf, (list, tuple)):\n",
    "    # Handle case where outputs is a tuple/list and not just a single item\n",
    "    assert len(outputs_bf) == len(outputs_torch)\n",
    "    for i in range(len(outputs_bf)):\n",
    "        out_bf, out_torch = outputs_bf[i], outputs_torch[i] \n",
    "        assert(out_torch.shape == out_bf.shape)\n",
    "elif is_dataclass(outputs_bf):\n",
    "    check_equivalent_class(outputs_bf, outputs_torch)\n",
    "    check_dataclass_keys_match(outputs_bf, outputs_torch)\n",
    "else:\n",
    "    assert(outputs_torch.shape == outputs_bf.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save torch BertForMLM to file\n",
    "save_path = \"bertformlm_torch.pt\"\n",
    "torch.save(torch_model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load state dict for BertForMLM into BF and check weights, outputs, and backprop\n",
    "if not BF_FROM_MODEL_ID:\n",
    "    bf_model.load_state_dict(torch.load(save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check weights of BF model and Torch model match exactly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of param weight bert.embeddings.word_embeddings.weight for bf and torch are equal? True\n",
      "Value of param weight bert.embeddings.position_embeddings.weight for bf and torch are equal? True\n",
      "Value of param weight bert.embeddings.token_type_embeddings.weight for bf and torch are equal? True\n",
      "Value of param weight bert.embeddings.LayerNorm.weight for bf and torch are equal? True\n",
      "Value of param weight bert.embeddings.LayerNorm.bias for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.0.attention.self.query.weight for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.0.attention.self.query.bias for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.0.attention.self.key.weight for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.0.attention.self.key.bias for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.0.attention.self.value.weight for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.0.attention.self.value.bias for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.0.attention.output.dense.weight for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.0.attention.output.dense.bias for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.0.attention.output.LayerNorm.weight for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.0.attention.output.LayerNorm.bias for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.0.intermediate.dense.weight for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.0.intermediate.dense.bias for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.0.output.dense.weight for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.0.output.dense.bias for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.0.output.LayerNorm.weight for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.0.output.LayerNorm.bias for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.1.attention.self.query.weight for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.1.attention.self.query.bias for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.1.attention.self.key.weight for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.1.attention.self.key.bias for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.1.attention.self.value.weight for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.1.attention.self.value.bias for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.1.attention.output.dense.weight for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.1.attention.output.dense.bias for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.1.attention.output.LayerNorm.weight for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.1.attention.output.LayerNorm.bias for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.1.intermediate.dense.weight for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.1.intermediate.dense.bias for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.1.output.dense.weight for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.1.output.dense.bias for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.1.output.LayerNorm.weight for bf and torch are equal? True\n",
      "Value of param weight bert.encoder.layer.1.output.LayerNorm.bias for bf and torch are equal? True\n",
      "Value of param weight cls.predictions.bias for bf and torch are equal? True\n",
      "Value of param weight cls.predictions.transform.dense.weight for bf and torch are equal? True\n",
      "Value of param weight cls.predictions.transform.dense.bias for bf and torch are equal? True\n",
      "Value of param weight cls.predictions.transform.LayerNorm.weight for bf and torch are equal? True\n",
      "Value of param weight cls.predictions.transform.LayerNorm.bias for bf and torch are equal? True\n"
     ]
    }
   ],
   "source": [
    "# Check weights match\n",
    "check_bf_param_weights_match_torch(bf_model, torch_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check model output after forward pass matches for BF and Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.dropout 0.1\n",
      "bert.embeddings.dropout 0\n",
      "bert.encoder.layer.0.attention.self.dropout 0.1\n",
      "bert.encoder.layer.0.attention.self.dropout 0\n",
      "bert.encoder.layer.0.attention.output.dropout 0.1\n",
      "bert.encoder.layer.0.attention.output.dropout 0\n",
      "bert.encoder.layer.0.output.dropout 0.1\n",
      "bert.encoder.layer.0.output.dropout 0\n",
      "bert.encoder.layer.1.attention.self.dropout 0.1\n",
      "bert.encoder.layer.1.attention.self.dropout 0\n",
      "bert.encoder.layer.1.attention.output.dropout 0.1\n",
      "bert.encoder.layer.1.attention.output.dropout 0\n",
      "bert.encoder.layer.1.output.dropout 0.1\n",
      "bert.encoder.layer.1.output.dropout 0\n"
     ]
    }
   ],
   "source": [
    "# Set all dropouts to 0\n",
    "for name, module in torch_model.named_modules():\n",
    "    if module._get_name() == \"Dropout\":\n",
    "        print(name, module.p)\n",
    "        module.p = 0\n",
    "        print(name, module.p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking diff between BF and torch for logits:\n",
      "Output of bf and torch are within 0.01? True\n",
      "\tStats on diff in outputs between bf and torch:                   0\n",
      "count  1.159836e+06\n",
      "mean   2.621052e-06\n",
      "std    2.201426e-06\n",
      "min    1.163514e-13\n",
      "25%    9.636658e-07\n",
      "50%    2.073543e-06\n",
      "75%    3.686615e-06\n",
      "max    2.491091e-05\n"
     ]
    }
   ],
   "source": [
    "# Check output from forward passes match for bf and torch\n",
    "torch_model.train(False)\n",
    "bf_model.train(False)\n",
    "\n",
    "outputs_bf = bf_model(input_ids_bf)\n",
    "outputs_torch = torch_model(input_ids_torch)\n",
    "\n",
    "if isinstance(outputs_bf, (list, tuple)):\n",
    "    assert len(outputs_bf) == len(outputs_torch)\n",
    "    for i in range(len(outputs_bf)):\n",
    "        out_bf, out_torch = outputs_bf[i], outputs_torch[i]\n",
    "        check_bf_model_outputs_match_torch_outputs(out_bf, out_torch, atol=1e-6)\n",
    "elif is_dataclass(outputs_bf):\n",
    "    check_model_outputs_allclose(outputs_bf, outputs_torch, print_stats=True, atol=1e-2)\n",
    "else:\n",
    "    check_bf_model_outputs_match_torch_outputs(outputs_bf, outputs_torch, atol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check grad after backward pass matches for BF and torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 112 ms, sys: 0 ns, total: 112 ms\n",
      "Wall time: 17.6 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/miniconda3/envs/jax-hf/lib/python3.9/site-packages/torch/autograd/__init__.py:173: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  /home/conda/feedstock_root/build_artifacts/pytorch-recipe_1664405705473/work/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Torch backward pass\n",
    "torch_model.train(True)\n",
    "\n",
    "if isinstance(outputs_torch, (list, tuple)):\n",
    "    assert len(outputs_bf) == len(outputs_torch)\n",
    "    backprop_node_torch = outputs_torch[0]\n",
    "elif is_dataclass(outputs_torch):\n",
    "    backprop_node_torch = outputs_torch.logits\n",
    "else:\n",
    "    backprop_node_torch = outputs_torch\n",
    "    \n",
    "backprop_node_torch.backward(gradient=torch.ones_like(backprop_node_torch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/miniconda3/envs/jax-hf/lib/python3.9/site-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int64. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\"scatter inputs have incompatible types: cannot safely cast \"\n",
      "/home/kevin/miniconda3/envs/jax-hf/lib/python3.9/site-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int64. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\"scatter inputs have incompatible types: cannot safely cast \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.3 s, sys: 6.45 s, total: 17.8 s\n",
      "Wall time: 8.85 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# BF backward pass\n",
    "\n",
    "if isinstance(outputs_bf, (list, tuple)):\n",
    "    assert len(outputs_bf) == len(outputs_torch)\n",
    "    backprop_node = outputs_bf[0]\n",
    "elif is_dataclass(outputs_torch):\n",
    "    backprop_node = outputs_bf.logits\n",
    "else:\n",
    "    backprop_node = outputs_bf\n",
    "    \n",
    "backprop_node.backprop(values_to_compute=(\"grad\",))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad of param bert.embeddings.word_embeddings.weight for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.embeddings.position_embeddings.weight for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.embeddings.token_type_embeddings.weight for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.embeddings.LayerNorm.weight for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.embeddings.LayerNorm.bias for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.0.attention.self.query.weight for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.0.attention.self.query.bias for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.0.attention.self.key.weight for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.0.attention.self.key.bias for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.0.attention.self.value.weight for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.0.attention.self.value.bias for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.0.attention.output.dense.weight for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.0.attention.output.dense.bias for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.0.attention.output.LayerNorm.weight for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.0.attention.output.LayerNorm.bias for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.0.intermediate.dense.weight for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.0.intermediate.dense.bias for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.0.output.dense.weight for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.0.output.dense.bias for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.0.output.LayerNorm.weight for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.0.output.LayerNorm.bias for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.1.attention.self.query.weight for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.1.attention.self.query.bias for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.1.attention.self.key.weight for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.1.attention.self.key.bias for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.1.attention.self.value.weight for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.1.attention.self.value.bias for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.1.attention.output.dense.weight for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.1.attention.output.dense.bias for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.1.attention.output.LayerNorm.weight for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.1.attention.output.LayerNorm.bias for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.1.intermediate.dense.weight for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.1.intermediate.dense.bias for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.1.output.dense.weight for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.1.output.dense.bias for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.1.output.LayerNorm.weight for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param bert.encoder.layer.1.output.LayerNorm.bias for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param cls.predictions.bias for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param cls.predictions.transform.dense.weight for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param cls.predictions.transform.dense.bias for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param cls.predictions.transform.LayerNorm.weight for bf and torch are within rtol=0.06, atol=0.01? True\n",
      "Grad of param cls.predictions.transform.LayerNorm.bias for bf and torch are within rtol=0.06, atol=0.01? True\n"
     ]
    }
   ],
   "source": [
    "# Run the actual check\n",
    "check_bf_param_grads_allclose_torch(bf_model, torch_model, rtol=6e-2, atol=1e-2, print_output=True, print_stats=True, use_assert=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "b4cc9cc217af6b7e12b7da5c82d5884fde07a0e0f6b7f76767c2fbf53f076f9a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
