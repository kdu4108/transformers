{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/miniconda3/envs/jax-hf/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fde003d8470>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import brunoflow as bf\n",
    "from brunoflow.ad.utils import check_node_equals_tensor, check_node_allclose_tensor\n",
    "from jax import numpy as jnp\n",
    "import transformers\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForMaskedLM, \n",
    "    BertForMaskedLM, \n",
    "    BertTokenizer, \n",
    "    BertTokenizerFast, \n",
    "    BertEmbeddings,\n",
    "    BfBertEmbeddings,\n",
    "    BertConfig,\n",
    ")\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  7592,  1045,  2215,  2000,  4521,  2070,   103,  6240,  2651,\n",
      "          1012,  2009,  1005,  1055, 15060,   103,  2035,   999,   102],\n",
      "        [  101, 10930, 10930,  2054,  1005,  1055,  2039,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0]])\n"
     ]
    }
   ],
   "source": [
    "# Establish data\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "text = [\"hello I want to eat some [MASK] meat today. It's thanksgiving [MASK] all!\", \"yo yo what's up\"]\n",
    "\n",
    "# tokenize text and pass into model\n",
    "tokens = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-22 18:07:11.873060: E external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BfBertEmbeddings(\n",
      "  (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "  (position_embeddings): Embedding(512, 768)\n",
      "  (token_type_embeddings): Embedding(2, 768)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1)\n",
      ")\n",
      "BertEmbeddings(\n",
      "  (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "  (position_embeddings): Embedding(512, 768)\n",
      "  (token_type_embeddings): Embedding(2, 768)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/code/rycolab/transformers/src/transformers/models/bert/modeling_bf_bert.py:175: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in zeros is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  self.register_buffer(\"token_type_ids\", bf.Node(jnp.zeros(self.position_ids.shape, dtype=jnp.int64)), persistent=False) # todo is this 64 bit necessary?\n"
     ]
    }
   ],
   "source": [
    "# Create BfBertEmbeddings and BertEmbeddings\n",
    "config = BertConfig.from_pretrained(pretrained_model_name_or_path=\"/home/kevin/code/rycolab/brunoflow/models/bert/config.json\")\n",
    "bf_embs = BfBertEmbeddings(config)\n",
    "torch_embs = BertEmbeddings(config)\n",
    "print(bf_embs)\n",
    "print(torch_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save torch BertEmbeddings to file\n",
    "save_path = \"bertembeddings_torch.pt\"\n",
    "torch.save(torch_embs.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load torch BertEmbeddings into bf\n",
    "bf_embs.load_state_dict(torch.load(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BfBertEmbeddings(\n",
       "  (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "  (position_embeddings): Embedding(512, 768)\n",
       "  (token_type_embeddings): Embedding(2, 768)\n",
       "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bf_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(check_node_equals_tensor(bf_embs.word_embeddings.weight, torch_embs.word_embeddings.weight))\n",
    "assert(check_node_equals_tensor(bf_embs.position_embeddings.weight, torch_embs.position_embeddings.weight))\n",
    "assert(check_node_equals_tensor(bf_embs.token_type_embeddings.weight, torch_embs.token_type_embeddings.weight))\n",
    "assert(check_node_equals_tensor(bf_embs.LayerNorm.weight, torch_embs.LayerNorm.weight))\n",
    "# print(check_node_equals_tensor(bf_embs.dropout.weight, torch_embs.dropout.weight)) # this fails because dropout has no weights, I guess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_embeddings: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "position_embeddings: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-5.9605e-07, -5.9605e-07, -5.9605e-07,  ..., -5.9605e-07,\n",
      "         -5.9605e-07, -5.9605e-07],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]])\n",
      "token_type_embeddings: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] tensor([[-3.2783e-06, -3.2783e-06, -3.2783e-06,  ..., -3.2783e-06,\n",
      "         -3.2783e-06, -3.2783e-06],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]])\n",
      "LayerNorm: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] tensor([ 4.0124e+01, -3.8037e+01, -6.7387e+01, -9.5256e+00,  3.1269e+01,\n",
      "         1.7020e+02, -5.1295e+01, -8.1252e+01, -1.3855e+02, -1.1788e+02,\n",
      "        -1.4359e+02,  4.9899e+01,  9.1715e+01, -3.5440e+01, -1.3533e+02,\n",
      "        -1.1644e+02,  1.0323e+02, -9.5663e+01,  1.5107e+01,  1.1859e+02,\n",
      "         9.2065e+01,  1.8432e+02,  8.1927e+01,  1.5641e+00,  3.8741e+02,\n",
      "         1.6975e+02, -3.0634e+01, -2.5420e+02,  4.4214e+01,  1.0545e+01,\n",
      "        -1.3188e+02, -1.7288e+01, -1.0193e+02, -1.6321e+02, -9.2263e+01,\n",
      "         8.5989e+01, -1.3927e+02,  1.5807e+02, -1.2027e+02, -9.4039e+01,\n",
      "        -5.3009e+01,  4.5693e+01,  3.3256e+01,  1.6287e+02,  1.2310e+02,\n",
      "         2.4396e+02, -8.0933e+01,  3.7402e+01, -4.0194e+01,  1.4286e+01,\n",
      "        -5.3439e+01,  2.6971e+02,  3.9596e+01,  1.4640e+02, -3.5923e+01,\n",
      "         9.8284e+01, -2.2522e+02,  9.8784e+01,  8.7564e+01,  6.5536e+01,\n",
      "         2.0318e+02,  2.5692e+02,  1.5810e+02,  4.3011e+01,  1.2800e+02,\n",
      "         5.7197e+01,  1.0389e+02,  3.8180e+00,  1.4986e+02, -1.3916e+02,\n",
      "        -7.2804e+01, -8.3352e+01,  1.0204e+02,  1.7309e+02,  9.8296e+01,\n",
      "         9.4110e+01, -7.0824e+01, -8.6854e+00,  1.3537e+02,  3.0575e+01,\n",
      "        -3.7570e+00,  8.9300e+01,  1.6341e+02,  1.9978e+02,  1.5111e+02,\n",
      "         2.0996e+02,  1.0685e+01, -6.3728e+01,  1.1980e+02, -9.5661e+01,\n",
      "        -1.5422e+02, -9.4825e+01,  1.0611e+02,  4.0541e+01, -7.8644e+01,\n",
      "        -4.8365e+01, -2.2979e+01, -1.2488e+02,  4.5415e+01,  1.1046e+02,\n",
      "        -2.0452e+02,  2.1086e+02,  2.8531e+02,  3.1418e+02,  1.3015e+02,\n",
      "         2.8546e+01,  2.0062e+02,  2.6422e+02,  1.8356e+02, -1.0049e+02,\n",
      "         6.1768e+01,  1.9575e+02,  7.8777e+01,  8.0983e+00, -5.8903e+01,\n",
      "        -1.4230e+02, -1.0199e+02, -8.4176e+01, -8.6918e+01,  6.0603e+01,\n",
      "         4.8506e+01, -1.1319e+01, -1.0136e+02, -3.7475e+01, -1.6422e+02,\n",
      "         9.6797e+01, -4.1158e+01,  1.1824e+01, -4.7608e+01,  1.2761e+01,\n",
      "         1.8640e+02, -8.3442e+01, -1.9990e+02, -3.4827e+01, -7.0824e+01,\n",
      "         9.1521e+01, -1.4406e+02,  2.1341e+01,  2.1734e+02,  1.4931e+02,\n",
      "         5.9427e+00, -5.0608e+01, -1.0255e+02,  7.4665e+01,  1.3555e+02,\n",
      "         5.4471e+01,  2.1604e+02,  2.0955e+02, -8.2787e+01, -1.2746e+01,\n",
      "         4.4198e+01, -1.8579e+02, -3.0102e+01, -1.6498e+01, -2.5934e+02,\n",
      "        -1.2092e+02, -1.1360e+02,  2.4188e+01,  5.5974e+01,  2.0331e+02,\n",
      "        -2.5896e+02,  2.7807e+01, -5.2066e+01,  1.0298e+01, -7.8658e+01,\n",
      "        -5.6741e+01, -2.3419e+02,  2.5569e+02,  4.1916e+01, -1.0769e+02,\n",
      "        -2.4271e+02, -8.8399e+01,  9.6111e+01,  7.1890e+01, -1.7145e+02,\n",
      "         3.5065e+01,  7.6623e+01, -2.0860e+01,  1.4609e+02,  1.2291e+02,\n",
      "        -1.2476e+01,  1.0138e+02, -1.1536e+02,  1.2797e+02, -1.8032e+01,\n",
      "        -1.9010e+02, -2.3258e+02,  1.9206e+02, -1.1468e+02, -1.5499e+02,\n",
      "        -2.7486e+02, -1.0729e+02, -1.6229e+02, -2.9016e+02, -1.1656e+02,\n",
      "        -7.4545e+01,  1.3658e+02,  3.7086e-03, -1.1321e+02,  2.9059e+01,\n",
      "        -5.1018e+01,  5.9962e+01,  1.0014e+02,  6.2774e+00, -9.3222e+01,\n",
      "         1.2480e+02, -4.3595e+01, -3.4920e+01,  8.4108e+01,  7.5105e+01,\n",
      "        -6.0350e+01, -4.0437e+01,  1.6749e+02,  2.4028e+01, -7.0283e+00,\n",
      "         9.5277e+01, -1.1569e+02, -2.8581e+02,  9.2746e+01,  1.4473e+02,\n",
      "        -1.1467e+02, -2.2164e+02,  5.1633e+01,  1.3013e+01, -1.7401e+02,\n",
      "         2.3621e+01, -5.7670e+01,  8.2579e+01,  2.7022e+02, -8.6180e+01,\n",
      "        -1.7632e+02, -1.8157e+02, -6.5589e+01, -2.4357e+01,  7.5048e+01,\n",
      "        -1.4492e+01, -9.4478e+01,  9.3096e+01,  9.9931e+01, -1.9216e+02,\n",
      "         1.0421e+02, -1.5293e+02,  3.1755e+00,  8.6740e+01, -6.4501e+01,\n",
      "         1.0551e+02, -3.9986e+01, -6.5258e+01,  9.7735e+01,  1.3567e+01,\n",
      "        -1.1013e+02, -1.1288e+02,  1.0257e+02, -7.4182e+01,  8.4553e+01,\n",
      "        -7.4265e+01,  1.2285e+01,  1.2451e+02,  2.7652e+01, -1.4279e+02,\n",
      "        -1.6614e+02, -9.6205e+01,  1.8507e+01,  5.3032e+01, -4.8407e+01,\n",
      "        -3.1200e+02, -5.9979e+01, -1.7805e+02,  4.1911e+01,  1.7833e+02,\n",
      "         9.3778e+00, -5.5381e+01, -1.7109e+02,  2.6731e+02, -1.1101e+02,\n",
      "        -1.9311e+02, -9.1773e+01,  2.2065e+01,  1.6269e+02, -1.4250e+02,\n",
      "         1.4811e+02, -2.0549e+01,  1.4577e+02,  2.5896e+02, -7.9559e+01,\n",
      "         1.3884e+02,  1.5288e+02,  1.5699e+02, -1.2140e+02,  1.2344e+02,\n",
      "        -4.1768e+01, -1.5547e+02, -2.9451e+02,  3.0680e+01,  3.3727e+01,\n",
      "        -1.0408e+02, -1.0119e+02,  5.1517e+01,  1.2624e+02,  8.1523e+01,\n",
      "         5.2206e+01,  7.5808e+01,  7.4010e+01, -2.5002e+02,  4.5760e+01,\n",
      "         1.3676e+02, -5.0882e+01, -6.8379e+01, -1.5476e+02,  1.0766e+01,\n",
      "        -1.3921e+02, -1.2549e+02, -3.9513e+01,  1.7793e+00,  4.6581e+01,\n",
      "         1.2853e+02,  1.9003e+02,  2.0242e+02, -2.4863e+02, -8.5916e+01,\n",
      "        -1.1450e+02, -5.8938e+01, -1.8779e+00,  8.4564e+01,  2.0899e+01,\n",
      "        -2.2450e+01, -9.9139e+01, -4.4694e+01,  2.2363e+01,  6.7233e+01,\n",
      "        -3.0264e+01,  8.7929e+01, -1.3660e+02, -1.5221e+02, -2.2902e+02,\n",
      "         5.6369e+01, -9.3353e+01,  1.5024e+02,  2.1037e+02, -2.4313e+00,\n",
      "         4.3177e+01, -1.7491e+02, -1.4526e+02, -3.5178e+01, -3.3094e+00,\n",
      "         2.3293e+02, -1.1422e+02,  2.1254e+02,  1.8531e+01, -1.1594e+02,\n",
      "         2.4289e+00, -2.2680e+02,  1.8248e+01,  7.6933e+01, -3.5710e+01,\n",
      "        -9.6350e+01, -1.1938e+02,  6.4799e+01, -6.0481e+01,  5.7839e+01,\n",
      "        -2.9632e+01, -4.4681e+01,  1.2666e+01,  1.4806e+02, -4.6758e+01,\n",
      "         1.1836e+02, -7.8855e+01,  1.7951e+01, -1.9641e+02,  1.4824e+02,\n",
      "        -8.6513e+01,  1.5956e+02, -1.1742e+02, -7.0574e+01, -6.5196e+01,\n",
      "        -1.3231e+02, -6.3116e+01,  1.0246e+01, -2.1656e+02,  2.4620e+00,\n",
      "         4.9925e+01,  1.1089e+02,  5.2376e+01, -4.5330e+01,  2.8485e+02,\n",
      "        -1.7398e+02,  9.5394e+01,  9.4942e+00, -9.1911e+00, -2.6560e+00,\n",
      "        -9.2233e+01,  1.6119e+02,  3.0215e-01, -1.0920e+01,  8.8393e+01,\n",
      "        -4.7220e+01,  7.2838e+01,  1.1594e+02,  7.4616e+01,  6.5780e+00,\n",
      "        -1.2542e+02, -1.5241e+02,  2.5284e+02,  2.8338e+02, -7.9641e+01,\n",
      "         3.3492e+02,  2.0976e+01,  3.6908e+01,  3.9383e+01,  8.8251e+01,\n",
      "        -2.9149e+01, -1.2416e+02,  1.0765e+02, -6.6686e+01,  1.2528e+02,\n",
      "         1.2764e+02, -8.3857e+01, -1.0225e+02, -1.2300e+02,  3.3231e+01,\n",
      "        -5.9828e+01,  8.4015e+01,  2.1192e+02,  2.3970e+02,  1.5065e+02,\n",
      "         1.2344e+02,  2.3703e+02, -1.5998e+02,  9.9283e+01,  7.7343e+00,\n",
      "        -6.0909e+01,  1.7511e+02,  8.7087e+01,  1.3104e+02,  7.5374e+01,\n",
      "        -4.2097e+01,  8.1609e+01,  2.9769e+02,  2.0619e+02, -1.0920e+02,\n",
      "         1.1174e+02, -1.0818e+01, -2.2309e+02, -1.7511e+01,  2.0157e+01,\n",
      "         2.5475e+02, -3.1589e+01, -1.6387e+02, -1.5962e+02, -6.1915e+01,\n",
      "        -7.6649e+01, -4.5106e+01, -1.7971e+02,  1.2977e+02, -8.3024e+01,\n",
      "        -2.1499e+01, -1.3995e+02,  2.6910e+02,  9.8546e+01, -1.8026e+02,\n",
      "        -2.2389e+02,  1.7097e+02, -1.1660e+02, -1.1957e+02,  6.3694e+01,\n",
      "         1.7716e+02,  7.9048e+01, -1.2315e+01,  2.7744e+01, -1.1658e+01,\n",
      "         1.4671e+02,  4.8580e+01,  3.4927e+01,  5.8687e+01, -1.1075e+02,\n",
      "        -1.0188e+02, -5.2626e+01, -4.6132e+01,  9.1033e+01, -1.2197e+02,\n",
      "         1.0240e+02, -2.5793e+00, -2.2393e+01, -2.2116e+01,  9.5212e+00,\n",
      "         1.5309e+00,  1.9965e+02, -1.2467e+02, -5.1216e+01, -1.4472e+02,\n",
      "         3.4869e+01,  5.9563e+01,  8.4614e+01, -1.3554e+01,  6.0355e+01,\n",
      "         2.2608e+02, -1.4019e+02,  2.2572e+00, -1.9183e+02,  1.5187e+02,\n",
      "         2.5824e+02, -1.1830e+02,  2.0662e+01,  1.5993e+01, -6.7816e+01,\n",
      "        -1.7061e+02, -1.9828e+02, -9.1505e+01, -1.3589e+02, -5.1697e+01,\n",
      "         1.0932e+02, -3.7313e+01, -1.0258e+02,  3.0953e+02,  1.5519e+01,\n",
      "        -9.0872e+01,  7.4640e+01,  1.7618e+02, -9.8442e+01,  1.5772e+02,\n",
      "         4.4303e+01,  1.0066e+02, -1.3678e+01,  8.9041e+01, -1.4499e+02,\n",
      "         2.7065e+02,  3.8721e+01,  8.5633e+01,  1.6489e+01,  7.3954e+01,\n",
      "         1.6746e+02, -4.5782e+01,  2.8760e+01, -1.7775e+02,  7.2911e-01,\n",
      "        -1.8706e+02, -2.2756e+02, -8.0494e+01,  7.4619e+01, -2.3673e+02,\n",
      "         7.2034e+01, -2.7394e+02, -6.9896e+01,  5.4210e+01,  3.8644e+01,\n",
      "        -1.1395e+02, -5.5466e+00, -7.2946e+00,  1.1446e+02, -2.4440e+02,\n",
      "         2.6259e+01, -9.4329e+01,  9.0384e+01, -1.1501e+01,  3.3740e+01,\n",
      "         1.5863e+00,  1.0486e+01, -2.1848e+01,  1.8780e+01, -8.3981e+01,\n",
      "         1.8102e+02,  1.6089e+02,  2.1117e+01, -2.2372e+02, -5.7563e+01,\n",
      "         2.7025e+01, -1.3450e+02,  3.2174e+01,  7.1461e+01, -1.5615e+02,\n",
      "        -5.6820e+01, -2.1184e+01,  5.4809e+01,  1.2661e+01,  4.3297e+01,\n",
      "        -2.9773e+01,  1.1181e+02,  1.2915e+02, -7.0631e+00, -1.1033e+02,\n",
      "         4.2723e+01, -8.8435e+01, -2.1258e+02,  6.0342e+01,  2.2907e+02,\n",
      "         2.7846e+00,  2.1919e+01, -6.7228e+01, -1.2859e+01,  5.4916e+01,\n",
      "         1.9634e+02,  1.7003e+02,  9.4449e+01,  1.4705e+02, -1.2221e+02,\n",
      "        -5.1115e+01, -2.2424e+02, -1.7357e+01,  1.4189e+01, -1.8334e+02,\n",
      "         1.7179e+02, -1.1551e+02,  1.5954e+01, -1.2804e+02, -9.8631e+01,\n",
      "        -1.3597e+02, -1.5996e+02, -5.2225e+01, -1.1147e+02,  1.6197e+02,\n",
      "        -8.9947e+01,  2.0298e+01, -3.5617e+01,  1.6902e+02,  1.9276e+02,\n",
      "         8.7572e+01, -1.0481e+01, -6.1990e+01,  3.0208e+01,  8.5564e+01,\n",
      "        -1.7023e+02,  9.7926e+01, -1.4002e+02,  1.6756e+02,  8.9485e+01,\n",
      "         6.1446e+01, -1.8854e+02, -1.8120e+02, -1.0851e+02,  8.6905e+01,\n",
      "         3.6771e+02, -1.6536e+02,  2.8613e-01,  7.5546e+01, -1.6127e+02,\n",
      "        -1.3033e+02, -1.4385e+02, -3.0190e+00,  1.0913e+02,  9.6440e+01,\n",
      "        -1.3011e+00, -1.5569e+01,  5.1472e+01, -4.4428e+01, -5.7261e+01,\n",
      "        -1.4738e+02,  2.8364e+01,  2.6843e+01,  8.7992e+01, -2.2862e+01,\n",
      "        -3.4715e+01,  9.0739e+01,  1.6642e+02, -2.0110e+01,  7.2018e+01,\n",
      "        -2.3433e+02, -1.9013e+02, -1.6391e+02,  2.1166e+02,  2.4186e+02,\n",
      "        -6.4917e+01, -1.1310e+02, -6.1384e+01, -9.5724e+01, -7.9644e+00,\n",
      "        -2.1604e+02, -6.3623e+01,  4.1656e+01, -7.5243e+01,  5.9785e+01,\n",
      "         2.6280e+02, -3.0064e+02,  2.4872e+02,  6.3441e+01, -1.6552e+01,\n",
      "        -2.2780e+02,  8.8778e+01,  7.8530e+00, -1.1109e+02, -8.2341e+01,\n",
      "         1.1264e+02,  1.3370e+01, -3.7350e+00,  8.8627e+01,  1.7721e+01,\n",
      "        -1.8391e+02,  3.7858e+00,  7.6273e+01, -3.0839e+01, -7.5049e+01,\n",
      "         7.4227e+01, -3.2242e+01, -1.6893e+01,  2.0300e+02,  1.3354e+02,\n",
      "         3.1398e+01, -4.2067e+00, -9.5823e+01, -1.0397e+02,  1.7015e+01,\n",
      "        -3.8033e+01, -1.5071e+02, -2.2849e+02, -1.4696e+02, -1.5314e+01,\n",
      "        -1.5881e+02,  5.4883e+00,  1.1343e+02,  5.0405e+01,  6.6551e+01,\n",
      "        -2.0779e+02,  4.7673e+01, -2.3570e+02, -6.1100e+00,  4.8837e+01,\n",
      "         6.4363e+01, -4.1085e+01, -2.2371e+02, -1.6398e+02,  1.1502e+02,\n",
      "        -2.3972e-01, -1.0037e+02,  1.6499e+02,  6.7722e+01,  6.1671e+01,\n",
      "         1.3886e+02, -3.2654e+00,  4.5373e+01,  1.2073e+02, -6.4962e+01,\n",
      "        -6.7787e+01,  3.1084e+00,  1.0724e+02, -4.9999e+01, -8.6307e+01,\n",
      "        -6.8304e+00,  1.0889e+02,  3.5155e+01, -1.0942e+02, -1.3994e+02,\n",
      "        -5.0394e+01, -1.1871e+02,  5.1167e+01, -1.0757e+02,  2.0268e+01,\n",
      "         1.5999e+02, -2.1749e+02, -9.3996e+01,  2.3493e+01, -1.2438e+02,\n",
      "         5.3414e+01,  7.2636e+01, -1.5856e+02,  3.1083e+01, -1.9071e+01,\n",
      "         1.3989e+01,  8.0423e+01,  4.8935e+01,  7.9978e+01, -2.9470e+01,\n",
      "        -2.7940e+01, -6.0730e+01, -5.8485e+01,  7.9213e+01, -1.0957e+02,\n",
      "         1.1205e+02, -1.5921e+02,  1.7537e+02])\n"
     ]
    }
   ],
   "source": [
    "# Compare output of forward pass of BfBertEmbeddings and BertEmbeddings on the text - they're equal!\n",
    "jax_input_ids = jnp.array(input_ids.numpy(), dtype=int)\n",
    "torch_embs.train(False)\n",
    "out_bf = bf_embs(input_ids=jax_input_ids)\n",
    "out_torch = torch_embs(input_ids=input_ids)\n",
    "# print(out_bf.val)\n",
    "# print(out_torch)\n",
    "assert(check_node_allclose_tensor(out_bf, out_torch))\n",
    "\n",
    "# Compare grad after backward pass\n",
    "torch_embs.train(True)\n",
    "out_torch.backward(gradient=torch.ones_like(out_torch))\n",
    "# out_bf.backprop()\n",
    "\n",
    "print(\"word_embeddings:\", bf_embs.word_embeddings.weight.grad, torch_embs.word_embeddings.weight.grad)\n",
    "print(\"position_embeddings:\", bf_embs.position_embeddings.weight.grad, torch_embs.position_embeddings.weight.grad)\n",
    "print(\"token_type_embeddings:\", bf_embs.token_type_embeddings.weight.grad, torch_embs.token_type_embeddings.weight.grad)\n",
    "print(\"LayerNorm:\", bf_embs.LayerNorm.weight.grad, torch_embs.LayerNorm.weight.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "out_bf.backprop(values_to_compute=(\"grad\",))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('jax-hf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b4cc9cc217af6b7e12b7da5c82d5884fde07a0e0f6b7f76767c2fbf53f076f9a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
