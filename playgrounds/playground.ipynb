{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/miniconda3/envs/jax-hf/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from jax import numpy as jnp\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForMaskedLM, \n",
    "    BertForMaskedLM, \n",
    "    BertTokenizer, \n",
    "    BertTokenizerFast, \n",
    "    BertEmbeddings,\n",
    "    BfBertEmbeddings,\n",
    "    BertConfig,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  7592,  1045,  2215,  2000,  4521,  2070,   103,  6240,  2651,\n",
      "          1012,  2009,  1005,  1055, 15060,   103,  2035,   999,   102],\n",
      "        [  101, 10930, 10930,  2054,  1005,  1055,  2039,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0]])\n"
     ]
    }
   ],
   "source": [
    "# Establish data\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "text = [\"hello I want to eat some [MASK] meat today. It's thanksgiving [MASK] all!\", \"yo yo what's up\"]\n",
    "\n",
    "# tokenize text and pass into model\n",
    "tokens = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/code/rycolab/transformers/src/transformers/models/bert/modeling_bf_bert.py:174: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in zeros is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  self.token_type_ids = jnp.zeros(self.position_ids.shape, dtype=jnp.int64) # todo is this 64 bit necessary?\n"
     ]
    }
   ],
   "source": [
    "# Create BfBertEmbeddings and BertEmbeddings\n",
    "config = BertConfig.from_pretrained(pretrained_model_name_or_path=\"/home/kevin/code/rycolab/brunoflow/models/bert/config.json\")\n",
    "bf_embs = BfBertEmbeddings(config)\n",
    "torch_embs = BertEmbeddings(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.34472668  0.5307575  -0.6256574  ... -0.74307156 -0.7948409\n",
      "    0.410834  ]\n",
      "  [-0.12443667 -0.8550836  -0.33784032 ... -0.03916439 -1.4545143\n",
      "    0.26670295]\n",
      "  [-2.1477165  -0.90835226 -1.105538   ...  0.75945294 -1.8616817\n",
      "   -0.16157249]\n",
      "  ...\n",
      "  [-1.8152208   0.4767846  -0.3528018  ... -0.03726343 -2.2972047\n",
      "    0.14477256]\n",
      "  [-0.9735971  -2.01189    -0.08289954 ... -0.51852256 -1.0732709\n",
      "    0.9564046 ]\n",
      "  [-1.0029035  -1.2219137   0.8737193  ... -0.09658846 -1.1882741\n",
      "    0.27896053]]\n",
      "\n",
      " [[-0.34472668  0.5307575  -0.6256574  ... -0.74307156 -0.7948409\n",
      "    0.410834  ]\n",
      "  [-0.21959372 -1.3216001  -0.11712483 ... -0.14018822 -1.1034871\n",
      "    0.9674713 ]\n",
      "  [-2.029127   -1.2888647  -1.9155287  ...  0.5731998  -0.8514466\n",
      "    0.7274356 ]\n",
      "  ...\n",
      "  [-1.2329972   0.89194095 -0.20066781 ... -0.42031294 -0.9897826\n",
      "   -0.20450695]\n",
      "  [-2.481228   -2.043665   -0.8651028  ... -0.5931333  -0.5373075\n",
      "   -0.54956347]\n",
      "  [-1.3868319  -0.63329226 -0.24705154 ... -0.3211176  -0.5438302\n",
      "   -0.5995097 ]]]\n",
      "tensor([[[ 0.6118,  1.6921, -0.1921,  ..., -0.6922, -0.0046, -1.6994],\n",
      "         [ 1.1421,  0.6153, -0.6540,  ...,  2.9885, -0.1980, -1.9624],\n",
      "         [-2.8703,  1.7373, -0.6771,  ..., -0.3818,  0.3359, -0.8325],\n",
      "         ...,\n",
      "         [-0.7348,  0.1532, -0.0000,  ..., -0.4122, -0.1334, -0.1504],\n",
      "         [-0.7405,  1.0252, -2.1494,  ...,  0.7749, -0.0000, -0.9099],\n",
      "         [-0.0000,  0.3138,  1.7634,  ...,  0.6995, -0.6890,  0.0917]],\n",
      "\n",
      "        [[ 0.6118,  1.6921, -0.1921,  ..., -0.6922, -0.0000, -1.6994],\n",
      "         [-0.0145, -0.0000, -0.0000,  ...,  0.6692, -0.0480, -2.2899],\n",
      "         [-1.0749,  1.3905,  0.4402,  ..., -1.0106, -0.5380, -1.6049],\n",
      "         ...,\n",
      "         [-0.7578,  0.6073, -1.3931,  ..., -1.3306, -0.0727, -0.4018],\n",
      "         [-0.4626,  0.8649, -1.8716,  ...,  0.0000, -1.0551, -0.9168],\n",
      "         [-1.6637,  0.0725,  1.2033,  ...,  0.8253,  0.0543,  0.0000]]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Compare output of BfBertEmbeddings and BertEmbeddings on the text\n",
    "jax_input_ids = jnp.array(input_ids.numpy(), dtype=int)\n",
    "print(bf_embs(input_ids=jax_input_ids).val)\n",
    "print(torch_embs(input_ids=input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30522, 768, padding_idx=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "be.position_ids\n",
    "be.word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('bert',\n",
       "              BertModel(\n",
       "                (embeddings): BertEmbeddings(\n",
       "                  (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "                  (position_embeddings): Embedding(512, 768)\n",
       "                  (token_type_embeddings): Embedding(2, 768)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (encoder): BertEncoder(\n",
       "                  (layer): ModuleList(\n",
       "                    (0): BertLayer(\n",
       "                      (attention): BertAttention(\n",
       "                        (self): BertSelfAttention(\n",
       "                          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                          (dropout): Dropout(p=0.1, inplace=False)\n",
       "                        )\n",
       "                        (output): BertSelfOutput(\n",
       "                          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                          (dropout): Dropout(p=0.1, inplace=False)\n",
       "                        )\n",
       "                      )\n",
       "                      (intermediate): BertIntermediate(\n",
       "                        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                        (intermediate_act_fn): GELUActivation()\n",
       "                      )\n",
       "                      (output): BertOutput(\n",
       "                        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                        (dropout): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                    (1): BertLayer(\n",
       "                      (attention): BertAttention(\n",
       "                        (self): BertSelfAttention(\n",
       "                          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                          (dropout): Dropout(p=0.1, inplace=False)\n",
       "                        )\n",
       "                        (output): BertSelfOutput(\n",
       "                          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                          (dropout): Dropout(p=0.1, inplace=False)\n",
       "                        )\n",
       "                      )\n",
       "                      (intermediate): BertIntermediate(\n",
       "                        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                        (intermediate_act_fn): GELUActivation()\n",
       "                      )\n",
       "                      (output): BertOutput(\n",
       "                        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                        (dropout): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                    (2): BertLayer(\n",
       "                      (attention): BertAttention(\n",
       "                        (self): BertSelfAttention(\n",
       "                          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                          (dropout): Dropout(p=0.1, inplace=False)\n",
       "                        )\n",
       "                        (output): BertSelfOutput(\n",
       "                          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                          (dropout): Dropout(p=0.1, inplace=False)\n",
       "                        )\n",
       "                      )\n",
       "                      (intermediate): BertIntermediate(\n",
       "                        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                        (intermediate_act_fn): GELUActivation()\n",
       "                      )\n",
       "                      (output): BertOutput(\n",
       "                        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                        (dropout): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                    (3): BertLayer(\n",
       "                      (attention): BertAttention(\n",
       "                        (self): BertSelfAttention(\n",
       "                          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                          (dropout): Dropout(p=0.1, inplace=False)\n",
       "                        )\n",
       "                        (output): BertSelfOutput(\n",
       "                          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                          (dropout): Dropout(p=0.1, inplace=False)\n",
       "                        )\n",
       "                      )\n",
       "                      (intermediate): BertIntermediate(\n",
       "                        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                        (intermediate_act_fn): GELUActivation()\n",
       "                      )\n",
       "                      (output): BertOutput(\n",
       "                        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                        (dropout): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                    (4): BertLayer(\n",
       "                      (attention): BertAttention(\n",
       "                        (self): BertSelfAttention(\n",
       "                          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                          (dropout): Dropout(p=0.1, inplace=False)\n",
       "                        )\n",
       "                        (output): BertSelfOutput(\n",
       "                          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                          (dropout): Dropout(p=0.1, inplace=False)\n",
       "                        )\n",
       "                      )\n",
       "                      (intermediate): BertIntermediate(\n",
       "                        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                        (intermediate_act_fn): GELUActivation()\n",
       "                      )\n",
       "                      (output): BertOutput(\n",
       "                        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                        (dropout): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                    (5): BertLayer(\n",
       "                      (attention): BertAttention(\n",
       "                        (self): BertSelfAttention(\n",
       "                          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                          (dropout): Dropout(p=0.1, inplace=False)\n",
       "                        )\n",
       "                        (output): BertSelfOutput(\n",
       "                          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                          (dropout): Dropout(p=0.1, inplace=False)\n",
       "                        )\n",
       "                      )\n",
       "                      (intermediate): BertIntermediate(\n",
       "                        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                        (intermediate_act_fn): GELUActivation()\n",
       "                      )\n",
       "                      (output): BertOutput(\n",
       "                        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                        (dropout): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                    (6): BertLayer(\n",
       "                      (attention): BertAttention(\n",
       "                        (self): BertSelfAttention(\n",
       "                          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                          (dropout): Dropout(p=0.1, inplace=False)\n",
       "                        )\n",
       "                        (output): BertSelfOutput(\n",
       "                          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                          (dropout): Dropout(p=0.1, inplace=False)\n",
       "                        )\n",
       "                      )\n",
       "                      (intermediate): BertIntermediate(\n",
       "                        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                        (intermediate_act_fn): GELUActivation()\n",
       "                      )\n",
       "                      (output): BertOutput(\n",
       "                        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                        (dropout): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                    (7): BertLayer(\n",
       "                      (attention): BertAttention(\n",
       "                        (self): BertSelfAttention(\n",
       "                          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                          (dropout): Dropout(p=0.1, inplace=False)\n",
       "                        )\n",
       "                        (output): BertSelfOutput(\n",
       "                          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                          (dropout): Dropout(p=0.1, inplace=False)\n",
       "                        )\n",
       "                      )\n",
       "                      (intermediate): BertIntermediate(\n",
       "                        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                        (intermediate_act_fn): GELUActivation()\n",
       "                      )\n",
       "                      (output): BertOutput(\n",
       "                        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                        (dropout): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                    (8): BertLayer(\n",
       "                      (attention): BertAttention(\n",
       "                        (self): BertSelfAttention(\n",
       "                          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                          (dropout): Dropout(p=0.1, inplace=False)\n",
       "                        )\n",
       "                        (output): BertSelfOutput(\n",
       "                          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                          (dropout): Dropout(p=0.1, inplace=False)\n",
       "                        )\n",
       "                      )\n",
       "                      (intermediate): BertIntermediate(\n",
       "                        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                        (intermediate_act_fn): GELUActivation()\n",
       "                      )\n",
       "                      (output): BertOutput(\n",
       "                        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                        (dropout): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                    (9): BertLayer(\n",
       "                      (attention): BertAttention(\n",
       "                        (self): BertSelfAttention(\n",
       "                          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                          (dropout): Dropout(p=0.1, inplace=False)\n",
       "                        )\n",
       "                        (output): BertSelfOutput(\n",
       "                          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                          (dropout): Dropout(p=0.1, inplace=False)\n",
       "                        )\n",
       "                      )\n",
       "                      (intermediate): BertIntermediate(\n",
       "                        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                        (intermediate_act_fn): GELUActivation()\n",
       "                      )\n",
       "                      (output): BertOutput(\n",
       "                        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                        (dropout): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                    (10): BertLayer(\n",
       "                      (attention): BertAttention(\n",
       "                        (self): BertSelfAttention(\n",
       "                          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                          (dropout): Dropout(p=0.1, inplace=False)\n",
       "                        )\n",
       "                        (output): BertSelfOutput(\n",
       "                          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                          (dropout): Dropout(p=0.1, inplace=False)\n",
       "                        )\n",
       "                      )\n",
       "                      (intermediate): BertIntermediate(\n",
       "                        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                        (intermediate_act_fn): GELUActivation()\n",
       "                      )\n",
       "                      (output): BertOutput(\n",
       "                        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                        (dropout): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                    (11): BertLayer(\n",
       "                      (attention): BertAttention(\n",
       "                        (self): BertSelfAttention(\n",
       "                          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                          (dropout): Dropout(p=0.1, inplace=False)\n",
       "                        )\n",
       "                        (output): BertSelfOutput(\n",
       "                          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                          (dropout): Dropout(p=0.1, inplace=False)\n",
       "                        )\n",
       "                      )\n",
       "                      (intermediate): BertIntermediate(\n",
       "                        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                        (intermediate_act_fn): GELUActivation()\n",
       "                      )\n",
       "                      (output): BertOutput(\n",
       "                        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                        (dropout): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )),\n",
       "             ('cls',\n",
       "              BertOnlyMLMHead(\n",
       "                (predictions): BertLMPredictionHead(\n",
       "                  (transform): BertPredictionHeadTransform(\n",
       "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (transform_act_fn): GELUActivation()\n",
       "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  )\n",
       "                  (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "                )\n",
       "              ))])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = [\"hello woo isn't this so exciting?! I wanted to eat some chicken\"]\n",
    "tokens = tokenizer(s)\n",
    "input_ids = tokens[\"input_ids\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenizer\u001b[39m.\u001b[39mtokenize([s])\n",
      "File \u001b[0;32m~/miniconda3/envs/jax/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py:320\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.tokenize\u001b[0;34m(self, text, pair, add_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize\u001b[39m(\u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m, pair: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, add_special_tokens: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[0;32m--> 320\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode_plus(text\u001b[39m=\u001b[39;49mtext, text_pair\u001b[39m=\u001b[39;49mpair, add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\u001b[39m.\u001b[39mtokens()\n",
      "File \u001b[0;32m~/miniconda3/envs/jax/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2667\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2657\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2658\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2659\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[1;32m   2660\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2664\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2665\u001b[0m )\n\u001b[0;32m-> 2667\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_encode_plus(\n\u001b[1;32m   2668\u001b[0m     text\u001b[39m=\u001b[39;49mtext,\n\u001b[1;32m   2669\u001b[0m     text_pair\u001b[39m=\u001b[39;49mtext_pair,\n\u001b[1;32m   2670\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   2671\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m   2672\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m   2673\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   2674\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   2675\u001b[0m     is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   2676\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   2677\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   2678\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   2679\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   2680\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   2681\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   2682\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   2683\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   2684\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   2685\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2686\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/jax/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py:502\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_encode_plus\u001b[39m(\n\u001b[1;32m    480\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    481\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    498\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    499\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BatchEncoding:\n\u001b[1;32m    501\u001b[0m     batched_input \u001b[39m=\u001b[39m [(text, text_pair)] \u001b[39mif\u001b[39;00m text_pair \u001b[39melse\u001b[39;00m [text]\n\u001b[0;32m--> 502\u001b[0m     batched_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_encode_plus(\n\u001b[1;32m    503\u001b[0m         batched_input,\n\u001b[1;32m    504\u001b[0m         is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m    505\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m    506\u001b[0m         padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m    507\u001b[0m         truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m    508\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m    509\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m    510\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m    511\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m    512\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m    513\u001b[0m         return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m    514\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m    515\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m    516\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m    517\u001b[0m         return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m    518\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    519\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[1;32m    522\u001b[0m     \u001b[39m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[1;32m    523\u001b[0m     \u001b[39m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[1;32m    524\u001b[0m     \u001b[39mif\u001b[39;00m return_tensors \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m return_overflowing_tokens:\n",
      "File \u001b[0;32m~/miniconda3/envs/jax/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py:429\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[39m# Set the truncation and padding strategy and restore the initial configuration\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_truncation_and_padding(\n\u001b[1;32m    422\u001b[0m     padding_strategy\u001b[39m=\u001b[39mpadding_strategy,\n\u001b[1;32m    423\u001b[0m     truncation_strategy\u001b[39m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    426\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m    427\u001b[0m )\n\u001b[0;32m--> 429\u001b[0m encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tokenizer\u001b[39m.\u001b[39;49mencode_batch(\n\u001b[1;32m    430\u001b[0m     batch_text_or_text_pairs,\n\u001b[1;32m    431\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m    432\u001b[0m     is_pretokenized\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m    433\u001b[0m )\n\u001b[1;32m    435\u001b[0m \u001b[39m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[39m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[39m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[39m#                       List[EncodingFast]\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \u001b[39m#                    ]\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \u001b[39m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[1;32m    441\u001b[0m tokens_and_encodings \u001b[39m=\u001b[39m [\n\u001b[1;32m    442\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_convert_encoding(\n\u001b[1;32m    443\u001b[0m         encoding\u001b[39m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    452\u001b[0m     \u001b[39mfor\u001b[39;00m encoding \u001b[39min\u001b[39;00m encodings\n\u001b[1;32m    453\u001b[0m ]\n",
      "\u001b[0;31mTypeError\u001b[0m: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]"
     ]
    }
   ],
   "source": [
    "tokenizer.tokenize([s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'##pone': 29513,\n",
       " 'sen': 12411,\n",
       " 'disgrace': 29591,\n",
       " 'cannon': 8854,\n",
       " 'dal': 17488,\n",
       " 'conclusions': 15306,\n",
       " 'expense': 10961,\n",
       " '273': 25371,\n",
       " 'temps': 29023,\n",
       " 'ල': 1406,\n",
       " 'smooth': 5744,\n",
       " '##ginal': 24965,\n",
       " 'cm': 4642,\n",
       " 'transylvania': 20816,\n",
       " '##mina': 22311,\n",
       " 'comb': 22863,\n",
       " 'finance': 5446,\n",
       " '##dson': 25112,\n",
       " 'tatum': 25913,\n",
       " 'canons': 21975,\n",
       " '镇': 1966,\n",
       " 'ʔ': 1139,\n",
       " 'anatomy': 13336,\n",
       " 'abstracts': 29474,\n",
       " '[unused350]': 355,\n",
       " 'italian': 3059,\n",
       " '##bad': 9024,\n",
       " 'appointments': 14651,\n",
       " '##chfield': 22693,\n",
       " 'given': 2445,\n",
       " 'carolina': 3792,\n",
       " 'lacey': 16355,\n",
       " 'frenzy': 21517,\n",
       " 'jana': 23341,\n",
       " 'geometridae': 24687,\n",
       " 'letterman': 26593,\n",
       " '家': 1825,\n",
       " 'forged': 16158,\n",
       " '##∂': 30120,\n",
       " 'violently': 14196,\n",
       " 'specified': 9675,\n",
       " 'harmful': 17631,\n",
       " 'isolate': 27152,\n",
       " '241': 22343,\n",
       " '##mith': 19864,\n",
       " 'officers': 3738,\n",
       " '##lan': 5802,\n",
       " 'km': 2463,\n",
       " '##iating': 15370,\n",
       " '##yman': 17906,\n",
       " 'campaign': 3049,\n",
       " 'rapids': 12775,\n",
       " 'seekers': 24071,\n",
       " '##anta': 26802,\n",
       " 'according': 2429,\n",
       " 'compliance': 12646,\n",
       " '##child': 19339,\n",
       " 'tank': 4951,\n",
       " '##ク': 30228,\n",
       " '##sto': 16033,\n",
       " 'traveled': 6158,\n",
       " 'contemporaries': 16682,\n",
       " '1979': 3245,\n",
       " '##ence': 10127,\n",
       " '##tase': 18260,\n",
       " 'broad': 5041,\n",
       " 'via': 3081,\n",
       " 'dawned': 27702,\n",
       " 'choice': 3601,\n",
       " 'tier': 7563,\n",
       " '##lston': 21540,\n",
       " 'ito': 23333,\n",
       " '##hila': 26415,\n",
       " '宣': 1823,\n",
       " 'song': 2299,\n",
       " 'realization': 12393,\n",
       " 'њ': 1216,\n",
       " 'commended': 22429,\n",
       " '##rogen': 22991,\n",
       " '##words': 22104,\n",
       " 'manga': 8952,\n",
       " 'lonely': 9479,\n",
       " '##rbin': 27366,\n",
       " 'anthony': 4938,\n",
       " '##yla': 23943,\n",
       " 'linen': 17517,\n",
       " 'retreat': 7822,\n",
       " '##making': 12614,\n",
       " 'train': 3345,\n",
       " 'several': 2195,\n",
       " 'statues': 11342,\n",
       " 'maddy': 20789,\n",
       " 'misconduct': 23337,\n",
       " 'cad': 28353,\n",
       " 'instituted': 14948,\n",
       " 'booking': 21725,\n",
       " 'outraged': 23558,\n",
       " '##ais': 15593,\n",
       " 'killed': 2730,\n",
       " 'acronym': 20137,\n",
       " 'vibe': 21209,\n",
       " 'straighten': 28568,\n",
       " 'empire': 3400,\n",
       " 'regal': 21279,\n",
       " 'inconsistent': 20316,\n",
       " 'assured': 8916,\n",
       " 'caledonia': 19305,\n",
       " 'sobbing': 20040,\n",
       " '##les': 4244,\n",
       " '[unused993]': 998,\n",
       " '##vern': 23062,\n",
       " 'shelley': 15828,\n",
       " '##orum': 20527,\n",
       " '1868': 7582,\n",
       " 'jamaican': 17851,\n",
       " 'madeira': 27309,\n",
       " 'edith': 13257,\n",
       " '##ffie': 29055,\n",
       " '##ily': 6588,\n",
       " '##rika': 23778,\n",
       " 'backup': 10200,\n",
       " 'blade': 6085,\n",
       " 'pi': 14255,\n",
       " 'martian': 20795,\n",
       " 'bundles': 26825,\n",
       " 'regulators': 25644,\n",
       " 'afb': 16909,\n",
       " '167': 16785,\n",
       " '[unused383]': 388,\n",
       " 'laurel': 11893,\n",
       " 'basil': 14732,\n",
       " 'retro': 22307,\n",
       " 'realizes': 10919,\n",
       " 'aimee': 23551,\n",
       " 'syndication': 26973,\n",
       " 'spacecraft': 12076,\n",
       " 'mysteriously': 29239,\n",
       " '##ique': 7413,\n",
       " 'servicemen': 26714,\n",
       " '##vi': 5737,\n",
       " 'enraged': 18835,\n",
       " 'ahmedabad': 27249,\n",
       " 'directed': 2856,\n",
       " 'identify': 6709,\n",
       " '##yard': 14132,\n",
       " '34th': 20460,\n",
       " 'this': 2023,\n",
       " 'downloaded': 22817,\n",
       " 'phonetic': 26664,\n",
       " 'wesley': 11482,\n",
       " 'tyson': 19356,\n",
       " 'salty': 23592,\n",
       " 'symphonies': 29355,\n",
       " 'eventual': 9523,\n",
       " 'foreigner': 29524,\n",
       " 'rugby': 4043,\n",
       " '³': 1083,\n",
       " 'م': 1295,\n",
       " '##tten': 25970,\n",
       " 'thatcher': 21127,\n",
       " '##ab': 7875,\n",
       " 'bose': 21299,\n",
       " 'courageous': 26103,\n",
       " '1830': 9500,\n",
       " 'examine': 11628,\n",
       " 'baseball': 3598,\n",
       " 'laura': 6874,\n",
       " 'coding': 16861,\n",
       " 'peas': 26072,\n",
       " 'constructive': 26157,\n",
       " '##阝': 30496,\n",
       " 'dodged': 28339,\n",
       " '[unused505]': 510,\n",
       " '[unused507]': 512,\n",
       " '##ほ': 30202,\n",
       " 'taxpayers': 26457,\n",
       " 'cisco': 26408,\n",
       " 'favoured': 16822,\n",
       " 'slick': 13554,\n",
       " '##llen': 12179,\n",
       " 'lifestyle': 9580,\n",
       " 'winds': 7266,\n",
       " 'mocked': 24195,\n",
       " 'suite': 7621,\n",
       " 'stacked': 16934,\n",
       " 'mach': 24532,\n",
       " 'pavement': 14271,\n",
       " 'fungus': 16622,\n",
       " '⊕': 1612,\n",
       " 'adventist': 25696,\n",
       " 'robertson': 9923,\n",
       " 'verge': 16079,\n",
       " '##yles': 26274,\n",
       " '[unused410]': 415,\n",
       " 'bei': 21388,\n",
       " '[unused572]': 577,\n",
       " 'peruvian': 15432,\n",
       " 'bicycles': 21066,\n",
       " 'queen': 3035,\n",
       " '##nare': 26148,\n",
       " '##inski': 19880,\n",
       " 'freshman': 10452,\n",
       " '##δ': 29722,\n",
       " 'notation': 14869,\n",
       " 'nair': 22525,\n",
       " 'tasked': 13487,\n",
       " 'fredrik': 25454,\n",
       " '51': 4868,\n",
       " '##off': 7245,\n",
       " 'ʀ': 1127,\n",
       " 'jones': 3557,\n",
       " 'brushes': 22569,\n",
       " '##ved': 7178,\n",
       " '##hani': 23573,\n",
       " '##beau': 26401,\n",
       " 'beams': 13110,\n",
       " 'himalayas': 26779,\n",
       " 'contenders': 27236,\n",
       " 'implication': 25323,\n",
       " '1840s': 19308,\n",
       " 'ledger': 27106,\n",
       " '##hole': 11484,\n",
       " '陽': 1973,\n",
       " 'skilled': 10571,\n",
       " 'cody': 13326,\n",
       " 'sony': 8412,\n",
       " 'lal': 21348,\n",
       " 'ministers': 7767,\n",
       " 'abu': 8273,\n",
       " '[unused338]': 343,\n",
       " 'barge': 19398,\n",
       " 'barley': 21569,\n",
       " 'rhythmic': 14797,\n",
       " '##tain': 18249,\n",
       " 'clarkson': 18648,\n",
       " '1000': 6694,\n",
       " 'oswald': 17411,\n",
       " 'factual': 25854,\n",
       " 'comprehend': 22346,\n",
       " '##ditional': 27064,\n",
       " 'jean': 3744,\n",
       " 'ц': 1201,\n",
       " '##trom': 13887,\n",
       " 'shower': 6457,\n",
       " '[unused646]': 651,\n",
       " 'tonnes': 11000,\n",
       " 'compressor': 29329,\n",
       " 'rai': 15547,\n",
       " '##lik': 18393,\n",
       " 'dynasties': 23014,\n",
       " 'fen': 21713,\n",
       " 'albion': 13392,\n",
       " '##national': 25434,\n",
       " '##elial': 24587,\n",
       " 'grown': 4961,\n",
       " 'joel': 8963,\n",
       " 'khalifa': 27925,\n",
       " 'adaptation': 6789,\n",
       " '##lies': 11983,\n",
       " 'δ': 1158,\n",
       " 'pickering': 27078,\n",
       " 'clerk': 7805,\n",
       " '##ulously': 21227,\n",
       " 'available': 2800,\n",
       " '[unused329]': 334,\n",
       " 'minutes': 2781,\n",
       " 'florian': 29517,\n",
       " 'wayne': 6159,\n",
       " 'humiliated': 26608,\n",
       " '##icum': 22167,\n",
       " 'blinked': 7948,\n",
       " '##nist': 26942,\n",
       " 'at': 2012,\n",
       " 'peaked': 6601,\n",
       " 'karma': 19902,\n",
       " 'rand': 14566,\n",
       " '[unused862]': 867,\n",
       " 'lass': 27333,\n",
       " 'endangered': 10193,\n",
       " 'gradually': 6360,\n",
       " 'investment': 5211,\n",
       " '##ice': 6610,\n",
       " 'buses': 7793,\n",
       " '1792': 13414,\n",
       " 'expenses': 11727,\n",
       " 'barrie': 24953,\n",
       " 'husky': 18758,\n",
       " '[unused763]': 768,\n",
       " 'flutter': 23638,\n",
       " '##idia': 29342,\n",
       " 'subordinate': 15144,\n",
       " '##rmin': 27512,\n",
       " 'libretto': 17621,\n",
       " '[unused306]': 311,\n",
       " 'stairs': 5108,\n",
       " 'bethel': 19375,\n",
       " 'showed': 3662,\n",
       " 'yong': 18999,\n",
       " '##phon': 20846,\n",
       " 'entering': 5738,\n",
       " 'fencing': 15962,\n",
       " 'bolted': 18088,\n",
       " 'pride': 6620,\n",
       " 'appoint': 16823,\n",
       " '##hine': 14014,\n",
       " 'radical': 7490,\n",
       " 'marsden': 29558,\n",
       " '分': 1775,\n",
       " '[unused70]': 71,\n",
       " 'monitor': 8080,\n",
       " 'commenting': 15591,\n",
       " 'trustees': 9360,\n",
       " 'continues': 4247,\n",
       " '1967': 3476,\n",
       " 'restore': 9239,\n",
       " 'doing': 2725,\n",
       " 'پ': 1302,\n",
       " 'educate': 16957,\n",
       " 'match': 2674,\n",
       " 'exhibit': 8327,\n",
       " 'panties': 12095,\n",
       " 'groove': 14100,\n",
       " '##rut': 22134,\n",
       " '⁻': 1545,\n",
       " 'myrtle': 21381,\n",
       " 'stresses': 23253,\n",
       " 'clinics': 17865,\n",
       " 'cortes': 22242,\n",
       " '⋅': 1614,\n",
       " 'colbert': 23928,\n",
       " 'item': 8875,\n",
       " 'demos': 18267,\n",
       " 'polished': 12853,\n",
       " 'mounted': 5614,\n",
       " '##zu': 9759,\n",
       " '##optera': 25324,\n",
       " 'whole': 2878,\n",
       " 'coordinated': 14206,\n",
       " '##lone': 27165,\n",
       " 'crashed': 8007,\n",
       " 'ethiopian': 15101,\n",
       " 'overwhelming': 10827,\n",
       " 'methodist': 8938,\n",
       " 'grimaced': 19014,\n",
       " 'plato': 18858,\n",
       " 'ـ': 1290,\n",
       " '##目': 30444,\n",
       " 'parrot': 22530,\n",
       " 'baldwin': 10970,\n",
       " 'transmissions': 21670,\n",
       " '1860s': 15914,\n",
       " 'outbreak': 8293,\n",
       " 'recreational': 10517,\n",
       " 'crowded': 10789,\n",
       " 'percentage': 7017,\n",
       " '##omp': 25377,\n",
       " 'preserve': 7969,\n",
       " '##sboro': 25623,\n",
       " 'railways': 7111,\n",
       " 'nazis': 13157,\n",
       " 'greenland': 16128,\n",
       " 'invited': 4778,\n",
       " '98': 5818,\n",
       " 'enjoyed': 5632,\n",
       " 'households': 3911,\n",
       " 'faye': 19243,\n",
       " 'martial': 7761,\n",
       " 'buying': 9343,\n",
       " 'canadians': 16485,\n",
       " 'killing': 4288,\n",
       " 'ny': 6396,\n",
       " 'ை': 1399,\n",
       " 'chorale': 26561,\n",
       " '[unused313]': 318,\n",
       " 'potomac': 18854,\n",
       " '##し': 30183,\n",
       " '##arney': 25831,\n",
       " 'greeks': 13176,\n",
       " 'duplicate': 24473,\n",
       " '##iro': 9711,\n",
       " 'capable': 5214,\n",
       " 'banjo': 16698,\n",
       " '##ga': 3654,\n",
       " '##ᵐ': 30038,\n",
       " 'myles': 27056,\n",
       " '##mir': 14503,\n",
       " 'elves': 16980,\n",
       " 'tanya': 19956,\n",
       " '##cens': 19023,\n",
       " 'firth': 25138,\n",
       " 'cds': 14340,\n",
       " 'area': 2181,\n",
       " 'resides': 11665,\n",
       " 'lowering': 13845,\n",
       " '##ষ': 29911,\n",
       " '##raj': 14220,\n",
       " '##night': 15864,\n",
       " 'producing': 5155,\n",
       " 'encouraged': 6628,\n",
       " 'burger': 15890,\n",
       " 'hundred': 3634,\n",
       " 'fantastic': 10392,\n",
       " 'teams': 2780,\n",
       " 'pray': 11839,\n",
       " 'let': 2292,\n",
       " 'aunt': 5916,\n",
       " 'wards': 11682,\n",
       " 'dh': 28144,\n",
       " 'stuart': 6990,\n",
       " 'conducting': 9283,\n",
       " 'swiss': 5364,\n",
       " 'fa': 6904,\n",
       " 'newman': 10625,\n",
       " '119': 13285,\n",
       " 'allowance': 21447,\n",
       " '[unused967]': 972,\n",
       " 'erwin': 22209,\n",
       " 'djs': 23837,\n",
       " '[unused827]': 832,\n",
       " '##logy': 6483,\n",
       " 'filly': 22062,\n",
       " 'nominated': 4222,\n",
       " '##sam': 21559,\n",
       " 'lauded': 26507,\n",
       " '##bine': 16765,\n",
       " 'lazarus': 23623,\n",
       " 'are': 2024,\n",
       " 'sutra': 26567,\n",
       " 'sandstone': 11694,\n",
       " 'medalist': 12968,\n",
       " '[unused108]': 113,\n",
       " '##ction': 7542,\n",
       " 'loren': 28779,\n",
       " 'undisclosed': 18206,\n",
       " 'gerald': 9659,\n",
       " 'squirrels': 29384,\n",
       " '##ign': 23773,\n",
       " 'colombian': 13598,\n",
       " 'enlightenment': 16724,\n",
       " 'couple': 3232,\n",
       " 'armour': 12371,\n",
       " 'pegasus': 26606,\n",
       " '##gh': 5603,\n",
       " 'ruby': 10090,\n",
       " '[unused600]': 605,\n",
       " 'provocative': 26422,\n",
       " '##nier': 14862,\n",
       " 'preserving': 15224,\n",
       " 'worm': 15485,\n",
       " 'chained': 22075,\n",
       " 'chew': 21271,\n",
       " '49': 4749,\n",
       " 'cooperation': 6792,\n",
       " 'confidential': 18777,\n",
       " 'rs': 12667,\n",
       " 'labeled': 12599,\n",
       " 'confinement': 21551,\n",
       " '[unused180]': 185,\n",
       " '1086': 28196,\n",
       " 'demonic': 23170,\n",
       " 'cry': 5390,\n",
       " 'weighs': 21094,\n",
       " 'handful': 9210,\n",
       " 'dodge': 11898,\n",
       " 'persecution': 14522,\n",
       " '##lves': 20899,\n",
       " 'miners': 11257,\n",
       " '##pore': 26691,\n",
       " 'к': 1189,\n",
       " 'townsville': 27492,\n",
       " 'ecumenical': 23540,\n",
       " '##31': 21486,\n",
       " 'dwell': 23120,\n",
       " 'traditionally': 6964,\n",
       " 'transferred': 4015,\n",
       " 'anyone': 3087,\n",
       " 'controller': 11486,\n",
       " 'incidence': 18949,\n",
       " 'wet': 4954,\n",
       " 'peach': 18237,\n",
       " 'sherry': 22268,\n",
       " 'lexie': 24123,\n",
       " 'motto': 12652,\n",
       " 'raju': 25098,\n",
       " 'distant': 6802,\n",
       " 'temperament': 26270,\n",
       " 'ebony': 27680,\n",
       " 'accelerated': 14613,\n",
       " '##lum': 12942,\n",
       " 'cited': 6563,\n",
       " 'rap': 9680,\n",
       " 'arabian': 13771,\n",
       " 'sourced': 23184,\n",
       " 'marred': 24563,\n",
       " 'vast': 6565,\n",
       " 'dylan': 7758,\n",
       " 'women': 2308,\n",
       " 'butte': 25024,\n",
       " '[unused418]': 423,\n",
       " 'blond': 8855,\n",
       " 'chemistry': 6370,\n",
       " 'posed': 13686,\n",
       " 'turf': 14585,\n",
       " 'linger': 26577,\n",
       " 'fourier': 26899,\n",
       " '##iser': 17288,\n",
       " 'æ': 1097,\n",
       " 'flaws': 21407,\n",
       " 'int': 20014,\n",
       " 'pipe': 8667,\n",
       " 'minerva': 27383,\n",
       " 'emotional': 6832,\n",
       " 'fires': 8769,\n",
       " '##cision': 28472,\n",
       " 'donnie': 28486,\n",
       " 'peek': 19043,\n",
       " 'dislike': 18959,\n",
       " '##ango': 23422,\n",
       " 'rug': 20452,\n",
       " 'tone': 4309,\n",
       " '##ʻi': 26444,\n",
       " 'first': 2034,\n",
       " 'fleetwood': 23866,\n",
       " '[unused214]': 219,\n",
       " 'taft': 19911,\n",
       " 'rex': 10151,\n",
       " 'calabria': 29430,\n",
       " 'annum': 28907,\n",
       " 'jockey': 13989,\n",
       " 'stepmother': 26959,\n",
       " '##oured': 16777,\n",
       " 'web': 4773,\n",
       " 'jay': 6108,\n",
       " '##rran': 28327,\n",
       " 'swollen': 13408,\n",
       " '##mute': 26746,\n",
       " '##etched': 29574,\n",
       " 'ely': 20779,\n",
       " '267': 25491,\n",
       " 'lifelong': 13506,\n",
       " 'baja': 19497,\n",
       " '秋': 1929,\n",
       " 'ea': 19413,\n",
       " '##10': 10790,\n",
       " 'psychiatry': 18420,\n",
       " '##ogan': 21131,\n",
       " 'un': 4895,\n",
       " 'novice': 23131,\n",
       " 'hurdle': 27630,\n",
       " '##lal': 13837,\n",
       " 'documents': 5491,\n",
       " '88': 6070,\n",
       " '##comb': 18274,\n",
       " 'ং': 1346,\n",
       " '##uses': 25581,\n",
       " 'fence': 8638,\n",
       " 'quoting': 27394,\n",
       " 'workshops': 9656,\n",
       " 'encounter': 8087,\n",
       " 'grades': 7022,\n",
       " 'guerre': 24613,\n",
       " 'libby': 19533,\n",
       " '36': 4029,\n",
       " 'mix': 4666,\n",
       " 'uncanny': 28953,\n",
       " 'neural': 15756,\n",
       " '##jm': 24703,\n",
       " 'prasad': 17476,\n",
       " 'tackle': 11147,\n",
       " 'landscapes': 12793,\n",
       " 'publishers': 8544,\n",
       " 'curated': 17940,\n",
       " '[unused701]': 706,\n",
       " 'pains': 20398,\n",
       " '[unused225]': 230,\n",
       " 'winkler': 20472,\n",
       " 'electric': 3751,\n",
       " '##boarding': 21172,\n",
       " '##graphs': 27341,\n",
       " '##his': 24158,\n",
       " '##ii': 6137,\n",
       " '##pment': 24073,\n",
       " 'soared': 29127,\n",
       " 'greenwich': 13861,\n",
       " '##ege': 24746,\n",
       " '##ж': 29743,\n",
       " 'emeritus': 12372,\n",
       " '[unused829]': 834,\n",
       " 'due': 2349,\n",
       " 'softball': 12585,\n",
       " 'ked': 16135,\n",
       " 'initiation': 17890,\n",
       " '##gang': 24930,\n",
       " '##心': 30375,\n",
       " '##haw': 14238,\n",
       " 'prairie': 10996,\n",
       " 'fulfilled': 16829,\n",
       " 'ე': 1442,\n",
       " '##tori': 29469,\n",
       " 'tighten': 21245,\n",
       " 'gearbox': 22227,\n",
       " 'tolkien': 23602,\n",
       " 'it': 2009,\n",
       " 'cale': 21854,\n",
       " '356': 27509,\n",
       " 'threatening': 8701,\n",
       " 'operational': 6515,\n",
       " 'ulrich': 19619,\n",
       " 'canopy': 14582,\n",
       " 'terrible': 6659,\n",
       " 'mccall': 25790,\n",
       " 'mere': 8210,\n",
       " 'black': 2304,\n",
       " 'orthodoxy': 26582,\n",
       " 'margaret': 5545,\n",
       " '##abas': 27537,\n",
       " '##rti': 28228,\n",
       " 'draining': 19689,\n",
       " 'permitting': 24523,\n",
       " 'flies': 10029,\n",
       " 'woodland': 11051,\n",
       " 'rebounds': 11049,\n",
       " '##bic': 13592,\n",
       " 'bamboo': 15216,\n",
       " '##gan': 5289,\n",
       " 'input': 7953,\n",
       " 'worthless': 22692,\n",
       " 'anticipated': 11436,\n",
       " 'famed': 15607,\n",
       " 'python': 18750,\n",
       " '[unused256]': 261,\n",
       " '政': 1860,\n",
       " 'specialists': 15744,\n",
       " '[unused537]': 542,\n",
       " 'number': 2193,\n",
       " 'tai': 13843,\n",
       " 'pardon': 14933,\n",
       " 'dodd': 21258,\n",
       " '1707': 25029,\n",
       " 'wcw': 24215,\n",
       " '##tens': 25808,\n",
       " 'derry': 17455,\n",
       " 'stein': 14233,\n",
       " 'misleading': 22369,\n",
       " '[unused59]': 60,\n",
       " '##zation': 9276,\n",
       " 'yue': 27163,\n",
       " 'copies': 4809,\n",
       " 'nationalists': 17934,\n",
       " 'angle': 6466,\n",
       " 'favored': 12287,\n",
       " 'clone': 17598,\n",
       " 'wearing': 4147,\n",
       " 'mining': 5471,\n",
       " 'stalled': 20659,\n",
       " 'syriac': 24109,\n",
       " 'city': 2103,\n",
       " 'holt': 12621,\n",
       " '##ा': 29876,\n",
       " 'gov': 18079,\n",
       " 'ferns': 25715,\n",
       " 'choosing': 10549,\n",
       " 'pierced': 16276,\n",
       " '##arty': 23871,\n",
       " '1808': 13040,\n",
       " 'outskirts': 12730,\n",
       " '[unused550]': 555,\n",
       " 'morale': 19292,\n",
       " 'turkey': 4977,\n",
       " '##ann': 11639,\n",
       " 'lighter': 9442,\n",
       " '##dine': 10672,\n",
       " 'jacques': 7445,\n",
       " 'individuals': 3633,\n",
       " '##zal': 16739,\n",
       " 'permitted': 7936,\n",
       " 'rosario': 17496,\n",
       " 'squads': 20191,\n",
       " 'bland': 20857,\n",
       " 'arc': 8115,\n",
       " 'petra': 20953,\n",
       " 'agriculture': 5237,\n",
       " 'easily': 4089,\n",
       " 'jai': 17410,\n",
       " 'religion': 4676,\n",
       " '##ister': 12911,\n",
       " 'both': 2119,\n",
       " 'alloys': 28655,\n",
       " 'hq': 16260,\n",
       " 'style': 2806,\n",
       " '##izer': 17629,\n",
       " 'knowing': 4209,\n",
       " '##dgets': 28682,\n",
       " 'scotland': 3885,\n",
       " '##ula': 7068,\n",
       " 'labeling': 28847,\n",
       " '800': 5385,\n",
       " 'investigator': 14064,\n",
       " 'suitable': 7218,\n",
       " 'tale': 6925,\n",
       " '##belt': 21561,\n",
       " 'perfect': 3819,\n",
       " 'youthful': 22446,\n",
       " '[unused460]': 465,\n",
       " 'hulk': 16009,\n",
       " 'cheered': 25471,\n",
       " '##42': 20958,\n",
       " 'linear': 7399,\n",
       " '1884': 6988,\n",
       " '[unused491]': 496,\n",
       " '[unused758]': 763,\n",
       " '夫': 1813,\n",
       " '##雄': 30500,\n",
       " 'juliette': 24696,\n",
       " 'chartered': 12443,\n",
       " 'giants': 7230,\n",
       " '1b': 26314,\n",
       " 'numerous': 3365,\n",
       " '##さ': 30182,\n",
       " '1672': 27253,\n",
       " 'willie': 9893,\n",
       " 'encore': 19493,\n",
       " 'crowns': 24364,\n",
       " 'universe': 5304,\n",
       " 'azores': 28320,\n",
       " '[unused134]': 139,\n",
       " 'segment': 6903,\n",
       " 'suspicious': 10027,\n",
       " '[unused72]': 73,\n",
       " '##ido': 13820,\n",
       " 'chilling': 27017,\n",
       " '##edd': 22367,\n",
       " 'enrico': 21982,\n",
       " 'sydney': 3994,\n",
       " 'blanchard': 26870,\n",
       " 'mandates': 25979,\n",
       " '##vers': 14028,\n",
       " 'slavery': 8864,\n",
       " 'deco': 21933,\n",
       " 'poultry': 22468,\n",
       " 'anwar': 28372,\n",
       " 'bahadur': 21715,\n",
       " '1870': 6940,\n",
       " '##bruck': 28985,\n",
       " '##formed': 29021,\n",
       " 'generation': 4245,\n",
       " 'clusters': 12906,\n",
       " 'chi': 9610,\n",
       " 'digitally': 18397,\n",
       " 'cereal': 20943,\n",
       " 'sharp': 4629,\n",
       " 'goodness': 15003,\n",
       " '##bang': 25153,\n",
       " '##ʿ': 29714,\n",
       " '##erly': 12561,\n",
       " '##ncia': 22750,\n",
       " 'yesterday': 7483,\n",
       " 'lease': 10084,\n",
       " 'mutant': 15527,\n",
       " 'humidity': 18213,\n",
       " 'jake': 5180,\n",
       " 'metropolitan': 4956,\n",
       " 'speak': 3713,\n",
       " 'balls': 7395,\n",
       " 'nationwide': 9053,\n",
       " 'hume': 20368,\n",
       " '松': 1880,\n",
       " 'fivb': 28423,\n",
       " 'ghanaian': 27202,\n",
       " 'io': 22834,\n",
       " 'protestant': 8330,\n",
       " '[unused221]': 226,\n",
       " 'trophy': 5384,\n",
       " '健': 1768,\n",
       " '1978': 3301,\n",
       " 'creditors': 23112,\n",
       " '−': 1597,\n",
       " '##せ': 30185,\n",
       " 'espn': 10978,\n",
       " 'gough': 29124,\n",
       " 'tunes': 13281,\n",
       " 'indian': 2796,\n",
       " 'hummed': 26747,\n",
       " 'dart': 14957,\n",
       " 'ethnicity': 18240,\n",
       " 'montrose': 24990,\n",
       " 'questioning': 11242,\n",
       " 'worlds': 8484,\n",
       " 'dissent': 24116,\n",
       " 'experiments': 7885,\n",
       " 'independently': 9174,\n",
       " '##省': 30446,\n",
       " '##ound': 28819,\n",
       " 'always': 2467,\n",
       " 'intervention': 8830,\n",
       " '##ura': 4648,\n",
       " '##tical': 14656,\n",
       " '##typical': 27086,\n",
       " 'rounded': 8352,\n",
       " 'themed': 11773,\n",
       " 'butler': 7055,\n",
       " '##ired': 27559,\n",
       " 'distraction': 14836,\n",
       " 'sucks': 19237,\n",
       " '##itated': 15198,\n",
       " 'দ': 1364,\n",
       " 'worked': 2499,\n",
       " 'likewise': 10655,\n",
       " 'damien': 12587,\n",
       " 'smackdown': 22120,\n",
       " '##rud': 28121,\n",
       " 'harvard': 5765,\n",
       " 'ourselves': 9731,\n",
       " '##id': 3593,\n",
       " 'padma': 23731,\n",
       " 'philosophy': 4695,\n",
       " 'kitty': 14433,\n",
       " '##ho': 6806,\n",
       " 'earlier': 3041,\n",
       " 'audiences': 9501,\n",
       " 'indifference': 25920,\n",
       " 'reinforcements': 14214,\n",
       " 'creep': 19815,\n",
       " 'excerpt': 28142,\n",
       " 'guineas': 22385,\n",
       " 'י': 1250,\n",
       " 'disaster': 7071,\n",
       " 'orleans': 5979,\n",
       " 'richards': 9712,\n",
       " 'า': 1422,\n",
       " '##oping': 17686,\n",
       " 'cardinal': 7185,\n",
       " '[unused904]': 909,\n",
       " '##hd': 14945,\n",
       " 'links': 6971,\n",
       " 'rock': 2600,\n",
       " '##active': 19620,\n",
       " 'scoop': 23348,\n",
       " '##jing': 29518,\n",
       " 'koppen': 20139,\n",
       " '##islaus': 25678,\n",
       " 'dismissed': 7219,\n",
       " 'acute': 11325,\n",
       " 'gb': 16351,\n",
       " '##nded': 25848,\n",
       " '##ф': 29749,\n",
       " '[unused6]': 7,\n",
       " 'reasons': 4436,\n",
       " 'boundaries': 7372,\n",
       " '行': 1945,\n",
       " 'beats': 10299,\n",
       " '[unused559]': 564,\n",
       " '##rit': 14778,\n",
       " 'winger': 16072,\n",
       " 'seniors': 15995,\n",
       " 'philosophical': 9569,\n",
       " '##mmy': 18879,\n",
       " '##村': 30404,\n",
       " 'sporadic': 24590,\n",
       " 'yi': 12316,\n",
       " 'lam': 16983,\n",
       " 'convicts': 24948,\n",
       " 'from': 2013,\n",
       " 'cicero': 23080,\n",
       " 'remake': 12661,\n",
       " 'midwest': 13608,\n",
       " '179': 20311,\n",
       " '##cle': 14321,\n",
       " '##erence': 20935,\n",
       " 'listened': 7791,\n",
       " '##ぬ': 30195,\n",
       " 'ɾ': 1126,\n",
       " 'rank': 4635,\n",
       " 'scroll': 17186,\n",
       " 'bane': 25163,\n",
       " 'fray': 25975,\n",
       " '##cript': 23235,\n",
       " '##riation': 18769,\n",
       " 'yours': 6737,\n",
       " 'beaufort': 23622,\n",
       " '##ury': 13098,\n",
       " 'awaiting': 15497,\n",
       " 'taxonomy': 25274,\n",
       " 'deportation': 23702,\n",
       " 'cloudy': 24706,\n",
       " 'southwestern': 8772,\n",
       " '195': 17317,\n",
       " '##tu': 8525,\n",
       " 'country': 2406,\n",
       " '道': 1957,\n",
       " 'entertainer': 21751,\n",
       " 'reductions': 25006,\n",
       " 'reflected': 7686,\n",
       " '##llas': 25816,\n",
       " '##witz': 15362,\n",
       " 'gutierrez': 20836,\n",
       " 'gap': 6578,\n",
       " 'ambushed': 22168,\n",
       " 'jail': 7173,\n",
       " '木': 1875,\n",
       " 'followers': 8771,\n",
       " 'wexford': 23121,\n",
       " '1809': 12861,\n",
       " 'dish': 9841,\n",
       " 'klan': 26613,\n",
       " 'glide': 21096,\n",
       " '##π': 29731,\n",
       " 'manufacturers': 8712,\n",
       " '##gaon': 27073,\n",
       " 'matched': 10349,\n",
       " 'scarlett': 20862,\n",
       " 'brewery': 12161,\n",
       " 'naia': 29511,\n",
       " 'chiba': 27368,\n",
       " '##bill': 24457,\n",
       " '##missive': 27876,\n",
       " 'sinn': 26403,\n",
       " '##nz': 14191,\n",
       " 'pak': 22190,\n",
       " 'sample': 7099,\n",
       " 'suffers': 17567,\n",
       " 'shed': 8328,\n",
       " 'perhaps': 3383,\n",
       " 'temperatures': 7715,\n",
       " 'happens': 6433,\n",
       " 'vein': 12818,\n",
       " 'steiner': 21264,\n",
       " 'fugitive': 21329,\n",
       " 'lc': 29215,\n",
       " 'cabinets': 20053,\n",
       " '[unused473]': 478,\n",
       " 'accelerate': 23306,\n",
       " 'cones': 23825,\n",
       " 'cave': 5430,\n",
       " '##yar': 13380,\n",
       " 'kidd': 25358,\n",
       " 'herb': 12810,\n",
       " '中': 1746,\n",
       " 'শ': 1374,\n",
       " 'cramer': 29433,\n",
       " '##ola': 6030,\n",
       " 'beethoven': 15461,\n",
       " 'ami': 26445,\n",
       " 'apocalyptic': 27660,\n",
       " '##tz': 5753,\n",
       " '##ο': 29730,\n",
       " 'fanny': 17813,\n",
       " 'segments': 9214,\n",
       " '古': 1789,\n",
       " 'substitute': 7681,\n",
       " 'albans': 26311,\n",
       " '##rrick': 24999,\n",
       " '##uria': 27703,\n",
       " 'latter': 3732,\n",
       " 'malabar': 28785,\n",
       " 'functioning': 12285,\n",
       " 'provides': 3640,\n",
       " 'access': 3229,\n",
       " 'displacement': 13508,\n",
       " 'guilt': 8056,\n",
       " 'hopped': 17230,\n",
       " '[unused318]': 323,\n",
       " 'five': 2274,\n",
       " 'gameplay': 11247,\n",
       " 'replaced': 2999,\n",
       " 'influential': 6383,\n",
       " 'honey': 6861,\n",
       " 'plant': 3269,\n",
       " 'awaited': 19605,\n",
       " 'adhere': 25276,\n",
       " 'advertising': 6475,\n",
       " '##ines': 10586,\n",
       " 'ras': 20710,\n",
       " '##opa': 29477,\n",
       " 'implied': 13339,\n",
       " 'elizabeth': 3870,\n",
       " '##bat': 14479,\n",
       " '##ⱼ': 30157,\n",
       " 'phases': 12335,\n",
       " '##ennial': 22929,\n",
       " '##agh': 17988,\n",
       " '##tford': 26341,\n",
       " 'tavern': 13090,\n",
       " '手': 1858,\n",
       " 'stagecoach': 26025,\n",
       " 'ය': 1404,\n",
       " '##shing': 12227,\n",
       " 'someone': 2619,\n",
       " 'defected': 26330,\n",
       " '##lium': 21816,\n",
       " 'reviewer': 12027,\n",
       " 'tides': 22487,\n",
       " 'rudd': 25298,\n",
       " 'vocalists': 27478,\n",
       " '##lore': 20186,\n",
       " 'voting': 6830,\n",
       " '##worm': 22769,\n",
       " '##ipes': 28108,\n",
       " 'attested': 18470,\n",
       " 'schneider': 15159,\n",
       " ...}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'hello',\n",
       " 'woo',\n",
       " 'isn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'this',\n",
       " 'so',\n",
       " 'exciting',\n",
       " '?',\n",
       " '!',\n",
       " 'i',\n",
       " 'wanted',\n",
       " 'to',\n",
       " 'eat',\n",
       " 'some',\n",
       " 'chicken',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('jax-hf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b4cc9cc217af6b7e12b7da5c82d5884fde07a0e0f6b7f76767c2fbf53f076f9a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
